Chapter 10. Model Comparison and Hierarchical Modeling
================
A Solomon Kurz
2019-03-08

Model Comparison and Hierarchical Modeling
==========================================

> There are situations in which different models compete to describe the same set of data...
>
> ...Bayesian inference is reallocation of credibility over possibilities. In model comparison, the focal possibilities are the models, and Bayesian model comparison reallocates credibility across the models, given the data. In this chapter, we explore examples and methods of Bayesian inference about the relative credibilities of models. (pp. 265--266)

In the text, the emphasis is on the Bayes Factor paradigm. While we will discuss that, we will also present the alternatives available with information criteria and model averaging and attacking.

10.1. General formula and the Bayes factor
------------------------------------------

So far we have spoken of

-   the data, denoted by *D* or *y*,
-   the model parameters, generically denoted by *θ*,
-   the likelihood function, denoted by *p*(*D*|*θ*), and
-   the prior distribution, denoted by *p*(*θ*).

Now we add to that *m*, which is a model index with *m* = 1 standing for the first model, *m* = 2 standing for the second model, and so on. So when we have more than one model in play, we might refer to the likelihood as *p*<sub>*m*</sub>(*y*|*θ*<sub>*m*</sub>, *m*) and the prior as *p*<sub>*m*</sub>(*θ*<sub>*m*</sub>|*m*). It's also the case, then, that each model can be given a prior probability *p*(*m*).

"The Bayes factor (BF) is the ratio of the probabilities of the data in models 1 and 2" (p. 268).

$$\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}$$

> One convention for converting the magnitude of the BF to a discrete decision about the models is that there is "substantial" evidence for model *m* = 1 when the BF exceeds 3.0 and, equivalently, "substantial" evidence for model *m* = 2 when the BF is less than 1/3 ([Jeffreys, 1961](https://global.oup.com/academic/product/theory-of-probability-9780198503682?cc=us&lang=en&); [Kass & Raftery, 1995](http://xyala.cap.ed.ac.uk/teaching/tutorials/phylogenetics/Bayesian_Workshop/PDFs/Kass%20and%20Raftery%201995.pdf); [Wetzels et al., 2011](https://pdfs.semanticscholar.org/1874/4e6c84087ccc20bc0f6db28020bc48c81b4a.pdf)).

10.2. Example: Two factories of coins
-------------------------------------

Kruschke considered the coin bias of two factories, each described by the beta distribution. We can organize how to derive the *α* and *β* parameters from *ω* and *κ* with a tibble.

``` r
library(tidyverse)

d <-
  tibble(factory = 1:2,
         omega   = c(.25, .75),
         kappa   = 12) %>% 
  mutate(alpha =      omega  * (kappa - 2) + 1,
         beta  = (1 - omega) * (kappa - 2) + 1)

d %>% 
  knitr::kable()
```

|  factory|  omega|  kappa|  alpha|  beta|
|--------:|------:|------:|------:|-----:|
|        1|   0.25|     12|    3.5|   8.5|
|        2|   0.75|     12|    8.5|   3.5|

Thus given *ω*<sub>1</sub> = .25, *ω*<sub>2</sub> = .75 and *κ* = 12, we can describe the bias of the two coin factories as B<sub>1</sub>(3.5, 8.5) and B<sub>2</sub>(8.5, 3.5). With a little wrangling, we canuse our `d` tibble to make the densities of Figure 10.2.

``` r
length <- 101

d %>% 
  expand(nesting(factory, alpha, beta),
         theta = seq(from = 0, to = 1, length.out = length)) %>%
  mutate(label = str_c("factory ", factory)) %>% 
  
  ggplot(aes(x = theta, 
             ymin = 0, 
             ymax = dbeta(x = theta, shape1 = alpha, shape2 = beta))) +
  geom_ribbon(fill = "grey67") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(theta)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~label)
```

![](10_files/figure-markdown_github/unnamed-chunk-2-1.png)

We might recreate the top panel with `geom_col()`.

``` r
tibble(Model = c("1", "2"),
       y = 1) %>% 
  
  ggplot(aes(x = Model, y = y)) +
  geom_col(width = .75, fill = "grey50") +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle(expression(paste(italic(P)[italic(m)]))) +
  theme(panel.grid   = element_blank(),
        axis.ticks.x = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-3-1.png)

Consider the Bernoulli bar plots in the bottom panels of Figure 10.2. The heights of the bars are arbitrary and just intended to give a sense of the Bernoulli distribution. If we wanted the heights to correspond to the Beta distributions above them, we might do so like this.

``` r
tibble(factory = rep(str_c("factory ", 1:2), each  = 2),
       flip    = rep(c("tails", "heads"), times = 2) %>% 
         factor(., levels = c("tails", "heads")),
       prob    = c(.75, .25, .25, .75)) %>% 
  
  ggplot(aes(x = flip, y = prob)) +
  geom_col(width = .75, fill = "grey50") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(NULL) +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank()) +
  facet_wrap(~factory)
```

![](10_files/figure-markdown_github/unnamed-chunk-4-1.png)

But now

> suppose we flip the coin nine times and get six heads. Given those data, what are the posterior probabilities of the coin coming from the head-biased or tail-biased factories? We will pursue the answer three ways: via formal analysis, grid approximation, and MCMC. (p. 270)

### 10.2.1. Solution by formal analysis.

Here we rehearse if we have beta(*θ*, *a*, *b*) prior for *θ* of the Bernoulli likelihood function, then the analytic solution for the posterior is beta(*θ*|*z* + *a*, *N*–*z* + *b*). Within this paradigm, if you would like to compute *p*(*D*|*m*), don't use the following function. If suffers from [underflow](https://en.wikipedia.org/wiki/Arithmetic_underflow) with large values.

``` r
p_d <- function(z, n, a, b) { 
  beta(z + a, n - z + b) / beta(a, b) 
}
```

This version is more robust.

``` r
p_d <- function(z, n, a, b) { 
  exp(lbeta(z + a, n - z + b) - lbeta(a, b)) 
}
```

You'd use it like this to compute *p*(*D*|*m*<sub>1</sub>).

``` r
p_d(z = 6, n = 9, a = 3.5, b = 8.5)
```

    ## [1] 0.0004993439

So to compute our BF, $\\frac{p(D|m\_1)}{p(D|m\_2)}$, you might use the `p_d()` function like this.

``` r
p_d_1 <- p_d(z = 6, n = 9, a = 3.5, b = 8.5)
p_d_2 <- p_d(z = 6, n = 9, a = 8.5, b = 3.5)

p_d_1 / p_d_2
```

    ## [1] 0.2135266

And if we computed the BF the other way, it'd look like this.

``` r
p_d_2 / p_d_1
```

    ## [1] 4.683258

Since the BF itself is only $\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}$, we'd need to bring in the priors for the models themselves to get the posterior probabilities, which follows the form

$$\\frac{p(m = 1 | D)}{p(m = 2 | D)} = \\bigg (\\frac{p(D | m = 1)}{p(D | m = 2)} \\bigg ) \\bigg ( \\frac{p(m = 1)}{p(m = 2)} \\bigg)$$

If for both our models, *p*(*m*)=.5, then

``` r
(p_d_1 * .5) / (p_d_2 * .5)
```

    ## [1] 0.2135266

As Kruschke pointed out, because we’re working in the probability metric, the sum of *p*(*m* = 1|*D*) and *p*(*m* = 2|*D*) must be 1. By simple algebra then,

*p*(*m* = 2|*D*)=1 − *p*(*m* = 1|*D*)

Therefore, it's also the case that

$$\\frac{p(m = 1 | D)}{1 - p(m = 1 | D)} = 0.2135266$$

Thus, 0.2135266 is in an odds metric. If you want to convert odds to a probability, you follow the formula

$$\\text{odds} = \\frac{\\text{probability}}{\\text{probability}}$$

And with more algegraic manipulation, you can solve for the probability.

$$
\\begin{eqnarray}
\\text{odds} & = & \\frac{\\text{probability}}{1 - \\text{probability}} \\\\
\\text{odds} - \\text{odds} \\cdot \\text{probability} & = & \\text{probability} \\\\
\\text{odds} & = & \\text{probability} + \\text{odds} \\cdot \\text{probability} \\\\
\\text{odds} & = & \\text{probability} (1 + \\text{odds}) \\\\
\\frac{\\text{odds}}{1 + \\text{odds}} & = & \\text{probability}
\\end{eqnarray}
$$

Thus, the posterior probability for *m* = 1 is

$$p(m = 1 | D) = \\frac{0.2135266}{1 + 0.2135266}$$

In code:

``` r
odds <- (p_d_1 * .5) / (p_d_2 * .5)

odds / (1 + odds)
```

    ## [1] 0.1759554

Relative to *m* = 2, our posterior probability for *m* = 1 is about .18. And therefore, the posterior probability of *m* = 2 is 1 minus that.

``` r
1 - (odds / (1 + odds))
```

    ## [1] 0.8240446

Given the data, the two models and the prior assumption they were equally credible, we conclude *m* = 2 is .82 probable.

### 10.2.2. Solution by grid approximation.

We won't be able to make the wireframe plots on the left of Figure 10.3, but we can do some of the others. Here's the upper right panel.

``` r
tibble(omega = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(m_p = ifelse(omega %in% c(.25, .75), 15, 0)) %>% 
  
  ggplot(aes(x = omega, 
             ymin = 0,
             ymax = m_p)) +
  geom_ribbon(fill = "grey67", color = "grey67") +
  coord_flip(ylim = 0:25) +
  labs(subtitle = "Remember, the scale on the x is arbitrary.",
       x = expression(omega),
       y = expression(paste("Marginal p(", omega, ")"))) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-12-1.png)

Building on that, here's the upper middle panel of the "two \[prior\] dorsal fins" (p. 271).

``` r
d <-
  tibble(omega = seq(from = 0, to = 1, length.out = length)) %>% 
  expand(omega, 
         theta = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(prior = ifelse(omega == .25, dbeta(theta, 3.5, 8.5),
                          ifelse(omega == .75, dbeta(theta, 8.5, 3.5),
                                 0)))
d %>% 
  ggplot(aes(x = theta, y = omega, fill = prior)) +
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  coord_equal() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

![](10_files/figure-markdown_github/unnamed-chunk-13-1.png)

This time we'll separate *p*<sub>*m* = 1</sub>(*θ*) and *p*<sub>*m* = 2</sub>(*θ*) into the two short plots on the right of the next row down.

``` r
p1 <-
  d %>% 
  filter(omega == .75) %>% 
  
  ggplot(aes(x = theta, 
             ymin = 0, 
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, " = .75)"))) +
   theme(panel.grid = element_blank())

p2 <-
  d %>% 
  filter(omega == .25) %>% 
  
  ggplot(aes(x = theta, 
             ymin = 0, 
             ymax = prior)) +
  geom_ribbon(fill = "grey67") +
  labs(x = expression(theta),
       y = expression(paste("p(", theta, "|", omega, " = .25)"))) +
  theme(panel.grid = element_blank())

# we'll put them together with help from gridExtra
library(gridExtra)

grid.arrange(p1, p2)
```

![](10_files/figure-markdown_github/unnamed-chunk-14-1.png)

We can continue to build on those sensibilities for the middle panel of the same row. Here we're literally adding *p*<sub>*m* = 1</sub>(*θ*) to *p*<sub>*m* = 2</sub>(*θ*) and taking their average.

``` r
tibble(theta = seq(from = 0, to = 1, length.out = length)) %>% 
  mutate(d_75 = dbeta(x = theta, shape1 = 8.5, shape2 = 3.5),
         d_25 = dbeta(x = theta, shape1 = 3.5, shape2 = 8.5)) %>% 
  mutate(mean_prior = (d_75 + d_25) / 2) %>% 

  ggplot(aes(x = theta, 
             ymin = 0, 
             ymax = mean_prior)) +
  geom_ribbon(fill = "grey67") +
  coord_cartesian(ylim = 0:3) +
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, ")"))) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-15-1.png)

We need the Bernoulli likelihood function for the next step.

``` r
bernoulli_likelihood <- function(theta, data) {
  n <- length(data)
  z <- sum(data)
  return(theta^z * (1 - theta)^(n - sum(data)))
  }
```

Time to feed our data and the parameter space into `bernoulli_likelihood()`, which will allow us to make the 2-dimensional density plot at the heart of Figure 10.3.

``` r
n <- 9
z <- 6

trial_data <- rep(0:1, times = c(n - z, z))

d <-
  d %>% 
  mutate(likelihood = bernoulli_likelihood(theta = theta, 
                                           data  = trial_data))

d %>%
  ggplot(aes(x = theta, y = omega, fill = likelihood)) +
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  coord_equal() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

![](10_files/figure-markdown_github/unnamed-chunk-17-1.png)

Now we just need the marginal likelihood, *p*(*D*), to compute the posterior. Our first depiction will be the middle panel of the second row from the bottom--the panel with the uneven dolphin fins.

``` r
d <-
  d %>% 
  mutate(marginal_likelihood = sum(prior * likelihood)) %>% 
  mutate(posterior = (prior * likelihood) / marginal_likelihood) 

d %>% 
  ggplot(aes(x = theta, y = omega, fill = posterior)) +
  geom_raster(interpolate = T) +
  scale_fill_viridis_c(option = "A") +
  labs(x = expression(theta),
       y = expression(omega)) +
  coord_equal() +
  theme(panel.grid = element_blank(),
        legend.position = "none")
```

![](10_files/figure-markdown_github/unnamed-chunk-18-1.png)

Here, then, is a way to get the panel in on the right of the second row from the bottom.

``` r
d %>% 
  mutate(marginal = (posterior / max(posterior)) * 25) %>% 

  ggplot(aes(x = omega,
             ymin = 0,
             ymax = marginal)) +
  geom_ribbon(fill = "grey67", color = "grey67") +
  coord_flip(ylim = 0:25) +
  labs(subtitle = "Remember, the scale on the x is arbitrary.",
       x = expression(omega),
       y = expression(paste("Marginal p(", omega, "|D)"))) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-19-1.png)

To make the middle bottom panel of Figure 10.3, we have to average the posterior values of *θ* over the grid of *ω* values. That is, we have to marginalize.

``` r
 d %>%
  group_by(theta) %>% 
  summarise(marginal_theta = mean(posterior)) %>% 
  
  ggplot(aes(x = theta, 
             ymin = 0,
             ymax = marginal_theta)) +
  geom_ribbon(fill = "grey67") + 
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, "|D)"))) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-20-1.png)

For the lower right panel of Figure 10.3, we'll filter to our two focal values of *ω* and then facet by them.

``` r
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(omega = str_c("omega == ", omega)) %>%

  ggplot(aes(x = theta, 
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") + 
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, "|", omega, ")"))) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~omega, ncol = 1, scales = "free", labeller = label_parsed)
```

![](10_files/figure-markdown_github/unnamed-chunk-21-1.png)

Do note the different scales on the *y*. Here's what they'd look like on the same scale.

``` r
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  mutate(omega = str_c("omega == ", omega)) %>%

  ggplot(aes(x = theta, 
             ymin = 0,
             ymax = posterior)) +
  geom_ribbon(fill = "grey67") + 
  labs(x = expression(theta),
       y = expression(paste("Marginal p(", theta, "|", omega, ")"))) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~omega, ncol = 1, labeller = label_parsed)
```

![](10_files/figure-markdown_github/unnamed-chunk-22-1.png)

Hopefully that helps build the intuition of what Kruschke meant when he wrote "*visual inspection suggests that the ratio of the heights is about 5 to 1, which matches the Bayes factor of 4.68 that we computed exactly in the previous section*" (p. 273, *emphasis* in the original).

Using the grid, you might compute that BF like this:

``` r
d %>% 
  filter(omega %in% c(.25, .75)) %>% 
  group_by(omega) %>% 
  summarise(sum_posterior = sum(posterior)) %>% 
  mutate(model = c("model_2", "model_1")) %>% 
  select(-omega) %>% 
  spread(key = model, value = sum_posterior) %>% 
  summarise(BF = model_1 / model_2)
```

    ## # A tibble: 1 x 1
    ##      BF
    ##   <dbl>
    ## 1  4.68

10.3. Solution by MCMC
----------------------

Kruschke started with: "For large, complex models, we cannot derive *p*(*D*|*m*) analytically or with grid approximation, and therefore we will approximate the posterior probabilities using MCMC methods" (p. 274). He's not kidding. Welcome to modern Bayes.

### 10.3.1. Nonhierarchical MCMC computation of each model’s marginal likelihood.

Before you get excited, Kruschke warned: "For complex models, this method might not be tractable. \[But\] for the simple application here, however, the method works well, as demonstrated in the next section" (p. 277).

#### 10.3.1.1. Implementation with ~~JAGS~~ brms.

Load brms.

``` r
library(brms)
```

Let's save the `trial_data` as a tibble.

``` r
trial_data <- 
  tibble(y = trial_data)
```

Let's learn a new brms skill. When you want to enter variables into the parameters defining priors in `brms::brm()`, you need to specify them using the `stanvar()` function. Since we want to do this for two variables, we’ll use `stanvar()` twice and save the results as an object, conveniently named `stanvars`.

``` r
omega <- .75
kappa <- 12

stanvars <-
  stanvar(     omega  * (kappa - 2) + 1, name = "my_alpha") +
  stanvar((1 - omega) * (kappa - 2) + 1, name = "my_beta")
```

Now we have our `stanvars` object, we are ready to fit the first model (i.e., the model for which *ω* = .75).

``` r
fit1 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(my_alpha, my_beta), class = Intercept),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      stanvars = stanvars,
      control = list(adapt_delta = .999),
      # This will let us use `prior_samples()` later on
      sample_prior = "yes")
```

Note how we fed our `stanvars` object into the `stanvars` function.

Anyway, let's inspect the chains.

``` r
plot(fit1)
```

![](10_files/figure-markdown_github/unnamed-chunk-27-1.png)

We'll glance at the model summary, too.

``` r
print(fit1)
```

    ##  Family: bernoulli 
    ##   Links: mu = identity 
    ## Formula: y ~ 1 
    ##    Data: trial_data (Number of observations: 9) 
    ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 1;
    ##          total post-warmup samples = 40000
    ## 
    ## Population-Level Effects: 
    ##           Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat
    ## Intercept     0.69      0.10     0.48     0.86       8008 1.00
    ## 
    ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample 
    ## is a crude measure of effective sample size, and Rhat is the potential 
    ## scale reduction factor on split chains (at convergence, Rhat = 1).

Next we'll follow Kruschke and extract the posterior samples, saving them as `theta`.

``` r
theta <- posterior_samples(fit1)

head(theta)
```

    ##   b_Intercept      lp__
    ## 1   0.7263424 -4.691665
    ## 2   0.7626307 -4.815941
    ## 3   0.7222605 -4.686314
    ## 4   0.8012281 -5.125448
    ## 5   0.7373272 -4.714354
    ## 6   0.6857306 -4.707360

The `fixef()` function will return the posterior summaries for the model intercept (i.e., *θ*). We can then index and save the desired summaries.

``` r
fixef(fit1)
```

    ##           Estimate Est.Error      Q2.5    Q97.5
    ## Intercept 0.691113  0.098457 0.4820692 0.863507

``` r
(mean_theta <- fixef(fit1)[1])
```

    ## [1] 0.691113

``` r
(sd_theta   <- fixef(fit1)[2])
```

    ## [1] 0.098457

Now we'll convert them to the *α* and *β* parameters, `a_post` and `b_post`, respectively.

``` r
a_post <-      mean_theta  * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
b_post <- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
```

Recall we've already defined:

``` r
n     <- 9
z     <- 6
omega <- .75
kappa <- 12
```

Thus we'll use them to compute $\\frac{1}{p(D)}$. Here we'll express Kruschke's `oneOverPD` as a function, `one_over_pd()`.

``` r
one_over_pd <- function(theta) {
  mean(dbeta(theta, a_post, b_post ) / 
         (theta^z * (1 - theta)^(n - z) * 
            dbeta(theta, omega * (kappa - 2) + 1, (1 - omega) * (kappa - 2) + 1 )))
}
```

We're ready to use `one_over_pd()` to help compute *p*(*D*).

``` r
theta %>% 
  summarise(pd = 1 / one_over_pd(theta = b_Intercept))
```

    ##            pd
    ## 1 0.002338466

That matches up nicely with Kruschke's value! Let's rinse, wash, and repeat for *ω* = .25. First, we'll need to redefine `omega` and our `stanvars`.

``` r
omega <- .25

stanvars <-
  stanvar(     omega  * (kappa - 2) + 1, name = "my_alpha") +
  stanvar((1 - omega) * (kappa - 2) + 1, name = "my_beta")
```

Fit the model.

``` r
fit2 <-
  brm(data = trial_data, 
      family = bernoulli(link = identity),
      y ~ 1,
      prior(beta(my_alpha, my_beta), class = Intercept),
      iter = 11000, warmup = 1000, chains = 4, cores = 4,
      seed = 10,
      stanvars = stanvars,
      control = list(adapt_delta = .999))
```

We'll do the rest in bulk.

``` r
theta <- posterior_samples(fit2)

mean_theta <- fixef(fit2)[1]
sd_theta   <- fixef(fit2)[2]

a_post <-      mean_theta  * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)
b_post <- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1)

theta %>% 
  summarise(pd = 1 / one_over_pd(theta = b_Intercept))
```

    ##             pd
    ## 1 0.0004992476

Boom!

### 10.3.2. Hierarchical MCMC computation of relative model probability.

I'm not aware of a way to specify a model "in which the top-level parameter is the index across models" in brms (p. 278). If you know of a way, [share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues).

However, we do have options. We can compare and weight models using information criteria, about which you can learn more [here](https://youtu.be/t0pRuy1_190?t=978). In brms, the LOO and WAIC are two primary information criteria available. You can compute them for a given model with the `loo()` and `waic()` functions, respectively. Here we use `loo()` and save the output as objects.

``` r
l_fit1 <- loo(fit1)
l_fit2 <- loo(fit2)
```

Here's the basic LOO summary for fit1.

``` r
print(l_fit1)
```

    ## 
    ## Computed from 40000 by 9 log-likelihood matrix
    ## 
    ##          Estimate  SE
    ## elpd_loo     -6.2 1.3
    ## p_loo         0.5 0.1
    ## looic        12.5 2.7
    ## ------
    ## Monte Carlo SE of elpd_loo is 0.0.
    ## 
    ## All Pareto k estimates are good (k < 0.5).
    ## See help('pareto-k-diagnostic') for details.

You get a wealth of output, more of which can be seen with `str(l_fit1)`. First, notice the message "All Pareto k estimates are good (k &lt; 0.5)." Pareto *k* values can be [used for diagnostics](https://cran.r-project.org/web/packages/loo/vignettes/loo2-example.html#plotting-pareto-k-diagnostics). Each case in the data gets its own *k* value and we like it when those *k*s are low. The makers of the [loo package](https://github.com/stan-dev/loo) get worried when *k* values exceed 0.7 and, as a result, `loo()` returns a warning message when they do. Happily, we have no such warning messages in this example.

In the main section, we get estimates for the expected log predictive density (`elpd_loo`), the estimated effective number of parameters (`p_loo`), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; `looic`). Each come with a standard error (i.e., `SE`). Like other information criteria, the LOO values aren't of interest in and of themselves. However, the estimate of one model's LOO relative to that of another is of great interest. We generally prefer models with lower information criteria. With the `compare_ic()` function, we can compute a formal difference score between multiple loo objects.

``` r
compare_ic(l_fit1, l_fit2)
```

    ##             LOOIC   SE
    ## fit1        12.46 2.70
    ## fit2        14.12 0.64
    ## fit1 - fit2 -1.66 3.34

Each difference score also comes with a standard error. In this case, `fit1` has the lower estimates, but the standard error of their difference score is large relative to the size of their difference. So the LOO difference score puts them on similar footing. You can do a similar analysis with the WAIC estimates.

In addition to difference-score comparisons, you can also use the LOO or WAIC for AIC-type model weighting. In brms, you do this with the `model_weights()` function.

``` r
(mw <- model_weights(fit1, fit2))
```

    ##    fit1    fit2 
    ## 0.83023 0.16977

I don't know that I'd call these weights probabilities, but they do sum to one. In this case, the analysis suggests we put about five times more weight to `fit1` relative to `fit2`.

``` r
mw[1] / mw[2]
```

    ##     fit1 
    ## 4.890321

With `brms::model_weights()`, we have a variety of weighting schemes avaliable to us. Since we didn't specify any in the `weights` argument, we used the default `"loo2"`, which is--perhaps confusingly given the name--the stacking method according to the [paper by Yao, Vehtari, Simpson, and Gelman](http://www.stat.columbia.edu/~gelman/research/published/stacking_paper_discussion_rejoinder.pdf). Vehtari has [written about the paper](https://statmodeling.stat.columbia.edu/2017/04/11/stacking-pseudo-bma-and-aic-weights/) on Gelman's blog, too. But anyway, the point is that different weighting schemes might not produce the same results. For example, here's the result from weighting using the WAIC.

``` r
model_weights(fit1, fit2, weights = "waic")
```

    ##      fit1      fit2 
    ## 0.6967995 0.3032005

Similar, for sure. But not the same. The stacking method via the brms default `weights = "loo2"` is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper).

For more on stacking and other weighting schemes, see Vehtari and Gabry’s vignette [*Bayesian Stacking and Pseudo-BMA weights using the loo package*](https://cran.r-project.org/web/packages/loo/vignettes/loo2-weights.html) or Vehtari's [modelselection\_tutorial GitHub repository](https://github.com/avehtari/modelselection_tutorial). But don't worry. We will have more opportunities to practice with information criteria, model weights, and such later in this project.

#### 10.3.2.1. ~~Using~~ \[No need to use\] pseudo-priors to reduce autocorrelation.

Since we didn't use Kruschke's method from the last subsection, we don't have the same worry about autocorrelation. For example, here are the autocorrelation plots for `fit1`.

``` r
library(bayesplot)

mcmc_acf(posterior_samples(fit1, add_chain = T), 
         pars = "b_Intercept",
         lags = 35)
```

![](10_files/figure-markdown_github/unnamed-chunk-41-1.png)

Our autocorrelations were a little high for HMC, but nowhere near pathological. The results for `fit2` were similar. As you might imagine from the moderate autocorrelations, the *N*<sub>*e**f**f*</sub>/*N* ratio for `b_Intercept` wasn't great.

``` r
neff_ratio(fit1)[1] %>% 
  mcmc_neff() +
  yaxis_text(hjust = 0)
```

![](10_files/figure-markdown_github/unnamed-chunk-42-1.png)

But we specified a lot of post-warmup iterations, so we're still in good shape. Plus, the $\\hat{R}$ was fine.

``` r
rhat(fit1)[1]
```

    ## b_Intercept 
    ##    1.000613

### 10.3.3. Models with different "noise" distributions in ~~JAGS~~ brms.

Kruschke tells us

> probability distribution\[s are\] sometimes \[called "noise"\] distribution\[s\] because \[they describe\] the random variability of the data values around the underlying trend. In more general applications, different models can have different noise distributions. For example, one model might describe the data as log-normal distributed, while another model might describe the data as gamma distributed. (p. 288)

If there are more than one plausible noise distributions for our data, we might want to compare the models. Kruschke then gave us a general trick in the form of this JAGS code:

``` r
data {
  C <- 10000 # JAGS does not warn if too small!
  for (i in 1:N) {
    ones[i] <- 1 }
} model {
  for (i in 1:N) {
    spy1[i] <- pdf1(y[i], parameters1) / C # where pdf1 is a formula
    spy2[i] <- pdf2(y[i], parameters2) / C # where pdf2 is a formula
    spy[i]  <- equals(m,1) * spy1[i] + equals(m, 2) * spy2[i]
    ones[i] ~ dbern(spy[i])
  }
  parameters1 ~ dprior1...
  parameters2 ~ dprior2...
  m ~ dcat(mPriorProb[])
  mPriorProb[1] <- .5
  mPriorProb[2] <- .5
}
```

I'm not aware that we can do this within the Stan/brms framework. If I'm in error and you know how, [please share your code](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). However, we do have options. In anticipation of Chapter 16, let's consider Gaussian-like data with thick tails. We might generate some like this:

``` r
# how many draws would you like?
n <- 1e3

set.seed(10)
(d <- tibble(y = rt(n, df = 7)))
```

    ## # A tibble: 1,000 x 1
    ##          y
    ##      <dbl>
    ##  1  0.0214
    ##  2 -0.987 
    ##  3  0.646 
    ##  4 -0.237 
    ##  5  0.977 
    ##  6 -0.200 
    ##  7  0.781 
    ##  8 -1.09  
    ##  9  1.83  
    ## 10 -0.682 
    ## # … with 990 more rows

The resulting data look like this.

``` r
d %>% 
  ggplot(aes(x = y)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-46-1.png)

As you'd expect with a small-*ν* Student's *t*, some of our values are quite distinct from the central clump. If you don't recall, Student's *t*-distribution has three parameters: *ν*, *μ*, and *σ*. The Gaussian is a special case of Student's *t* for which *ν* = ∞. When *ν* gets small, the consequence is the distribution allocates more mass in the tails. From a Gaussian perspective, the small-*ν* Student's *t* expects more outliers--though it's a little odd calling them outliers from a small-*ν* Student's *t* perspective.

Let's see how well the Gaussian versus the Student's *t* likelihoods handle the data. Here we'll use fairly liberal priors.

``` r
fit3 <-
  brm(data = d,
      family = gaussian,
      y ~ 1,
      prior = c(prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = sigma)),  # by default, this has a lower bound of 0
      chains = 4, cores = 4,
      seed = 10)  

fit4 <-
  brm(data = d,
      family = student,
      y ~ 1,
      prior = c(prior(normal(0, 5), class = Intercept),
                prior(normal(0, 5), class = sigma),
                prior(gamma(2, 0.1), class = nu)),  # this is the brms default prior for nu
      chains = 4, cores = 4,
      seed = 10) 
```

In case you were curious, here's what that default `gamma(2, 0.1)` prior on `nu` looks like.

``` r
tibble(x = seq(from = 0, to = 110, by = 1)) %>% 
  ggplot(aes(x = x, ymin = 0,
             ymax = dgamma(x, 2, 0.1))) +
  geom_ribbon(size = 0, fill = "grey67") +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(paste(italic(p), (nu)))) +
  coord_cartesian(xlim = 0:100) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-47-1.png)

That prior puts most of the probability mass below 50, but the right tail gently fades off into the triple digits, allowing for the possibility of larger estimates.

We can use the `posterior_summary()` function to get a compact look at the model summaries.

``` r
posterior_summary(fit3) %>% round(digits = 2)
```

    ##             Estimate Est.Error     Q2.5    Q97.5
    ## b_Intercept    -0.03      0.04    -0.11     0.05
    ## sigma           1.25      0.03     1.20     1.31
    ## lp__        -1646.97      0.98 -1649.49 -1646.02

``` r
posterior_summary(fit4) %>% round(digits = 2)
```

    ##             Estimate Est.Error     Q2.5    Q97.5
    ## b_Intercept    -0.01      0.04    -0.08     0.06
    ## sigma           0.98      0.04     0.91     1.05
    ## nu              5.73      1.03     4.13     8.08
    ## lp__        -1590.45      1.21 -1593.64 -1589.07

Now we can compare the two approaches using information criteria.

``` r
l_fit3 <- loo(fit3)
l_fit4 <- loo(fit4)

compare_ic(l_fit3, l_fit4)
```

    ##               LOOIC     SE
    ## fit3        3292.65 113.67
    ## fit4        3172.08  59.15
    ## fit3 - fit4  120.57  79.96

Based on the LOO difference, we hace some support for preferring the Student’s *t*, but do notice how wide that `SE` was. We can also compare the models using model weights. Here we'll use the default weighting scheme.

``` r
model_weights(fit3, fit4)
```

    ##       fit3       fit4 
    ## 0.03344267 0.96655733

In this, virtually all the weight was placed on the Student's-*t* model, `fit4`.

Remember what that *p*(*ν*) looked like? Here’s our posterior distribution for *ν*.

``` r
posterior_samples(fit4) %>% 
  ggplot(aes(x = nu)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:20) +
  labs(subtitle = expression(paste("Recall that for the Gaussian, ", nu, " = infinity.")),
       x = expression(paste(italic(p), "(", nu, "|", italic(D), ")"))) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-50-1.png)

Even though our prior for *ν* was relatively weak, the posterior ended up concentrated on values in the middle-single-digit range. Recall the data-generating value was 7.

We can also compare the models using posterior-predictive checks. There are a variety of ways we might do this, but the most convenient way is with `brms::pp_check()`, which is itself a wrapper for the family of `ppc` functions from the bayesplot package.

``` r
pp_check(fit3)
```

![](10_files/figure-markdown_github/unnamed-chunk-51-1.png)

``` r
pp_check(fit4)
```

![](10_files/figure-markdown_github/unnamed-chunk-51-2.png)

The default `pp_check()` setting allows us to compare the density of the data *y* (i.e., the dark blue) with 10 density’s simulated from the posterior *y*<sub>rep</sub> (i.e., the light blue). We prefer model that produce *y*<sub>rep</sub> distributions that resemble *y*. Though the results from both models were similar, the simulated distributions from `fit4` mimicked the original data a little more convincingly. To learn more about this approach, check out Gabry's vignette [*Graphical posterior predictive checks using the bayesplot package*](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html).

10.4. Prediction: Model averaging
---------------------------------

> In many applications of model comparison, the analyst wants to identify the best model and then base predictions of future data on that single best model, denoted with index *b*. In this case, predictions of future $\\hat{y}$ are based exclusively on the likelihood function $p\_b(\\hat{y} | \\theta\_b, m = b)$ and the posterior distribution *p*<sub>*b*</sub>(*θ*<sub>*b*</sub>|*D*, *m* = *b*) of the winning model:

$$p\_b(\\hat y | D, m = b) = \\int \\text d \\theta\_b p\_b (\\hat{y} | \\theta\_b, m = b) p\_b(\\theta\_b | D, m = b)$$

> But the full model of the data is actually the complete hierarchical structure that spans all the models being compared, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior credibilities. In other words, we take a weighted average across the models, with the weights being the posterior probabilities of the models. Instead of conditionalizing on the winning model, we have

$$
$$

> This is called model averaging. (p. 289)

Okay, while the concept of model averaging is of great interest, we aren't going to be able to follow this approach to it within the Stan/brms paradigm. This, recall, is because our paradigm doesn't allow for a hierarchical organization of models in the same qay JAGS does. However, we can still play the model averaging game with extensions of our model weighting paradigm, above. Before we get into the details,

> recall that there were two models of mints that created the coin, with one mint being tail-biased with mode *ω* = 0.25 and one mint being head-biased with mode *ω* = 0.75 The two subpanels in the lower-right illustrate the posterior distributions on *ω* within each model, *p*(*θ*|*D*, *ω* = 0.25) and *p*(*θ*|*D*, *ω* = 0.75) The winning model was *ω* = 0.75, and therefore the predicted value of future data, based on the winning model alone, would use *p*(*θ*|*D*, *ω* = 0.75). (p. 289)

That is, the posterior for `fit1`.

``` r
posterior_samples(fit1) %>% 
  ggplot(aes(x = b_Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "The posterior for the probability, given fit1",
       x = expression(paste(italic(p), "(", theta, "|", italic(D), ", ", omega, " = .75)"))) +
  coord_cartesian(xlim = 0:1) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-52-1.png)

> But the overall model included *ω* = 0.75, and if we use the overall model, then the predicted value of future data should be based on the complete posterior summed across values of *ω*. The complete posterior distribution \[is\] *p*(*θ*|*D*) (p. 289).

The cool thing about the model weighting stuff we learned about earlier is that you can use those model weights to average across models. Again, we’re not weighting the models by posterior probabilities the way Kruschke discussed in text. However, the spirit is similar. We can use the `brms::pp_average()` function to make posterior predictive prediction with mixtures of the models, weighted by our chosen weighting scheme. Here, we’ll go with the default stacking weights.

``` r
nd <- tibble(y = 1)

pp_averaged <-
  pp_average(fit1, fit2, 
             newdata = nd,
             # this line is not necessary, but you should see how to choose weighing methods
             weights = "loo2",
             method = "fitted",
             summary = F) %>% 
  as_tibble()
```

    ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`.
    ## This warning is displayed once per session.

``` r
# what does this produce?
head(pp_averaged) 
```

    ## # A tibble: 6 x 1
    ##      V1
    ##   <dbl>
    ## 1 0.692
    ## 2 0.521
    ## 3 0.806
    ## 4 0.603
    ## 5 0.667
    ## 6 0.758

We can plot our model-averaged *θ* with a little help from good old `tidybayes::stat_pointintervalh()`.

``` r
pp_averaged %>% 
  ggplot(aes(x = V1)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 30) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "The posterior for the probability, given the\nweighted combination of fit1 and fit2",
       x = expression(paste(italic(p), "(", theta, "|", italic(D), ")"))) +
  coord_cartesian(xlim = 0:1) +
  theme(panel.grid = element_blank())
```

![](10_files/figure-markdown_github/unnamed-chunk-53-1.png)

As Kruschke concluded, "you can see the contribution of *p*(*θ*|*D*, *ω* = 0.25) as the extended leftward tail" (p. 289). Interestingly enough, that looks a lot like the density we made with grid approximation in Figure 10.3, doesn't it?

10.5. Model complexity naturally accounted for
----------------------------------------------

> Bayesian model comparison compensates for model complexity by the fact that each model must have a prior distribution over its parameters, and more complex models must dilute their prior distributions over larger parameter spaces than simpler models. Thus, even if a complex model has some particular combination of parameter values that fit the data well, the prior probability of that particular combination must be small because the prior is spread thinly over the broad parameter space. (p. 290)

Our two models are

-   *p*(*θ*|*D*, *κ* = 2000) (i.e., the "must-be-fair" model) and
-   *p*(*θ*|*D*, *κ* = 2) (i.e., the "anything's-possible" model).

They look like this.

``` r
# how granular to you want the theta sequence?
n <- 1e3

tibble(theta = seq(from = 0, to = 1, length.out = n) %>% rep(., times = 2),
       omega = .5,
       kappa = rep(c(1000, 2), each = n),
       model = rep(c("The must-be-fair model", "The anything's-possible model"), each = n)) %>% 
  mutate(density = dbeta(theta, 
                         shape1 =      omega  * (kappa - 2) + 1, 
                         shape2 = (1 - omega) * (kappa - 2) + 1)) %>% 
  
  ggplot(aes(x = theta, ymin = 0, ymax = density)) +
  geom_ribbon(fill = "grey67") + 
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Note that in this case, their y-axes are on the same scale.",
       x     = expression(theta)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~model)
```

![](10_files/figure-markdown_github/unnamed-chunk-54-1.png)

``` r
# the data summaries
z <- 15
n <- 20

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

    ## [1] 0.3229023

If you're confused about where those `a` and `b` values came from, you can do the math like this.

``` r
tibble(omega = .5,
       kappa = c(1000, 2),
       model = c("The must-be-fair model", "The anything's-possible model")) %>% 
  mutate(a   =      omega  * (kappa - 2) + 1,
         b   = (1 - omega) * (kappa - 2) + 1)
```

    ## # A tibble: 2 x 5
    ##   omega kappa model                             a     b
    ##   <dbl> <dbl> <chr>                         <dbl> <dbl>
    ## 1   0.5  1000 The must-be-fair model          500   500
    ## 2   0.5     2 The anything's-possible model     1     1

Let's try again with different data.

``` r
# the data summaries
z <- 11
n <- 20

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

    ## [1] 3.337148

> The anything’s-possible model loses because it pays the price of having a small prior probability on the values of *θ* near the data proportion, while the must-be- fair model has large prior probability on *θ* values sufficiently near the data proportion to be credible. Thus, in Bayesian model comparison, a simpler model can win if the data are consistent with it, even if the complex model fits just as well. The complex model pays the price of having small prior probability on parameter values that describe simple data. (p. 291)

10.6. Extreme sensitivity to prior distribution
-----------------------------------------------

### 10.5.1. Caveats regarding nested model comparison.

10.6. Extreme sensitivity to the prior distribution
---------------------------------------------------

> In many realistic applications of Bayesian model comparison, the theoretical emphasis is on the difference between the models’ likelihood functions. For example, one theory predicts planetary motions based on elliptical orbits around the sun, and another theory predicts planetary motions based on circular cycles and epicycles around the earth. The two models involve very different parameters. In these sorts of models, the form of the prior distribution on the parameters is not a focus, and is often an afterthought. But, when doing Bayesian model comparison, the form of the prior is crucial because the Bayes factor integrates the likelihood function weighted by the prior distribution. (p. 292)

However, "the sensitivity of Bayes factors to prior distributions is well known in the literature (e.g., [Kass & Raftery, 1995](https://www.stat.washington.edu/raftery/Research/PDF/kass1995.pdf); [Liu & Aitkin, 2008](http://psycnet.apa.org/record/2008-17435-006); [Vanpaemel, 2010](https://ppw.kuleuven.be/okp/_pdf/Vanpaemel2010PSITT.pdf))," and furthermore, when comparing Bayesian models using the methods Kruschke outlined in this chapter of the text, "different forms of vague priors can yield very different Bayes factors" (p. 293).

In the two BFs to follow, we compare the must-be-fair model with similar diffuse priors. In the first case, we use the good-old uniform beta(1, 1).

``` r
z <- 65
n <- 100 

p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1)
```

    ## [1] 0.125287

Now we compare the must-be-fair model with the Haldane prior, which sets the two parameters within the beta distribution to be a) equivalent and b) quite small (i.e., 0.01 in this case).

``` r
p_d(z, n, a = 500, b = 500) / p_d(z, n, a = .01, b = .01)
```

    ## [1] 5.728066

Here's how those two priors of interest compare to one another.

``` r
# how granular to you want the theta sequence?
n <- 1e3

tibble(theta = seq(from = 0, to = 1, length.out = n) %>% rep(., times = 2),
       alpha = rep(c(1, .01), each = n),
       beta  = rep(c(1, .01), each = n),
       model = rep(c("Uninformative prior, beta(1, 1)", "Haldane prior, beta(0.01, 0.01)"), each = n)) %>% 
  mutate(model = factor(model, levels = c("Uninformative prior, beta(1, 1)", "Haldane prior, beta(0.01, 0.01)"))) %>% 
  mutate(density = dbeta(theta, 
                         shape1 = alpha, 
                         shape2 = beta)) %>% 
  
  ggplot(aes(x = theta, ymin = 0, ymax = density)) +
  geom_ribbon(fill = "grey67") + 
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "These y-axes are on the same scale.",
       x        = expression(theta)) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~model)
```

![](10_files/figure-markdown_github/unnamed-chunk-60-1.png)

Before we can complete the analyses of this subsection, we'll need to define our version of Kruschke's `HDIofICDF function()`, `hdi_of_icdf`(). Like we've done in previous chapters, here we mildly reformat the funciton.

``` r
hdi_of_icdf <- function(name, width = .95, tol = 1e-8, ... ) {
  incredible_mass <-  1.0 - width
  interval_width  <- function(low_tail_prob, name, width, ...) {
    name(width + low_tail_prob, ...) - name(low_tail_prob, ...)
  }
  opt_info<- optimize(interval_width, c(0, incredible_mass), 
                      name = name, width = width, 
                      tol = tol, ...)
  hdi_lower_tail_prob <- opt_info$minimum
  return(c(name(hdi_lower_tail_prob, ...),
           name(width + hdi_lower_tail_prob, ...)))
}
```

Compute the HDI for the uniform-prior-based model, beta(1, 1).

``` r
(hdi_1 <-
  hdi_of_icdf(name   = qbeta,
              shape1 = 66,
              shape2 = 36)
 )
```

    ## [1] 0.5542689 0.7382436

Now compute the HDI for the Haldane-prior-based model, beta(0.01, 0.01).

``` r
(hdi_2 <-
  hdi_of_icdf(name   = qbeta,
              shape1 = 65.01,
              shape2 = 36.01)
 )
```

    ## [1] 0.5501074 0.7353828

And for kicks and giggles, we can compare those intervals in a plot.

``` r
tibble(prior = c("Uniform", "Haldane"),
       ll = c(hdi_1[1], hdi_2[1]),
       ul = c(hdi_1[2], hdi_2[2])) %>% 
  
  ggplot(aes(x = ll,    xend = ul,
             y = prior, yend = prior)) +
  geom_segment(size = .75) +
  coord_cartesian(xlim = 0:1) +
  labs(subtitle = "Those two HDIs are quite similar. It almost\nseems silly their respective BFs are so\ndifferent.",
       x        = expression(theta),
       y        = NULL) +
  theme(panel.grid   = element_blank(),
        axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

![](10_files/figure-markdown_github/unnamed-chunk-64-1.png)

### 10.6.1. Priors of different models should be equally informed.

"We have established that seemingly innocuous changes in the vagueness of a vague prior can dramatically change a model’s marginal likelihood, and hence its Bayes factor in comparison with other models. What can be done to ameliorate the problem" (p. 294)? Kruschke posed one method might be taking a small representative portion of the data in hand and use them to make an empirically-based prior for the remaining set of data. From our previous example, "suppose that the 10% subset has 6 heads in 10 flips, so the remaining 90% of the data has *z* = 65 − 6 and *N* = 100 − 10" (p. 294). Here are the new Bayes Factors.

``` r
z <- 65 - 6
n <- 100 - 10

p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a =   1 + 6, b =   1 + 10 - 6)
```

    ## [1] 0.05570509

``` r
p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = .01 + 6, b = .01 + 10 - 6)
```

    ## [1] 0.05748123

Now the two Bayes Factors are nearly the same.

References
----------

Kruschke, J. K. (2015). *Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan.* Burlington, MA: Academic Press/Elsevier.

Session info
------------

``` r
sessionInfo()
```

    ## R version 3.5.1 (2018-07-02)
    ## Platform: x86_64-apple-darwin15.6.0 (64-bit)
    ## Running under: macOS High Sierra 10.13.6
    ## 
    ## Matrix products: default
    ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
    ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
    ## 
    ## locale:
    ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
    ## 
    ## attached base packages:
    ## [1] stats     graphics  grDevices utils     datasets  methods   base     
    ## 
    ## other attached packages:
    ##  [1] tidybayes_1.0.3 bayesplot_1.6.0 brms_2.7.0      Rcpp_1.0.0     
    ##  [5] gridExtra_2.3   bindrcpp_0.2.2  forcats_0.3.0   stringr_1.3.1  
    ##  [9] dplyr_0.7.6     purrr_0.2.5     readr_1.1.1     tidyr_0.8.1    
    ## [13] tibble_2.0.1    ggplot2_3.1.0   tidyverse_1.2.1
    ## 
    ## loaded via a namespace (and not attached):
    ##   [1] colorspace_1.3-2          ggridges_0.5.0           
    ##   [3] rsconnect_0.8.8           rprojroot_1.3-2          
    ##   [5] ggstance_0.3              markdown_0.8             
    ##   [7] base64enc_0.1-3           rstudioapi_0.7           
    ##   [9] rstan_2.18.2              svUnit_0.7-12            
    ##  [11] DT_0.4                    fansi_0.4.0              
    ##  [13] mvtnorm_1.0-8             lubridate_1.7.4          
    ##  [15] xml2_1.2.0                codetools_0.2-15         
    ##  [17] bridgesampling_0.4-0      knitr_1.20               
    ##  [19] shinythemes_1.1.1         jsonlite_1.5             
    ##  [21] LaplacesDemon_16.1.1      broom_0.5.1              
    ##  [23] shiny_1.1.0               compiler_3.5.1           
    ##  [25] httr_1.3.1                backports_1.1.2          
    ##  [27] assertthat_0.2.0          Matrix_1.2-14            
    ##  [29] lazyeval_0.2.1            cli_1.0.1                
    ##  [31] later_0.7.3               htmltools_0.3.6          
    ##  [33] prettyunits_1.0.2         tools_3.5.1              
    ##  [35] igraph_1.2.1              coda_0.19-2              
    ##  [37] gtable_0.2.0              glue_1.3.0               
    ##  [39] reshape2_1.4.3            cellranger_1.1.0         
    ##  [41] nlme_3.1-137              crosstalk_1.0.0          
    ##  [43] ps_1.2.1                  rvest_0.3.2              
    ##  [45] mime_0.5                  miniUI_0.1.1.1           
    ##  [47] gtools_3.8.1              MASS_7.3-50              
    ##  [49] zoo_1.8-2                 scales_1.0.0             
    ##  [51] colourpicker_1.0          hms_0.4.2                
    ##  [53] promises_1.0.1            Brobdingnag_1.2-5        
    ##  [55] parallel_3.5.1            inline_0.3.15            
    ##  [57] shinystan_2.5.0           yaml_2.1.19              
    ##  [59] loo_2.0.0                 StanHeaders_2.18.0-1     
    ##  [61] stringi_1.2.3             highr_0.7                
    ##  [63] dygraphs_1.1.1.5          pkgbuild_1.0.2           
    ##  [65] rlang_0.3.1               pkgconfig_2.0.2          
    ##  [67] matrixStats_0.54.0        HDInterval_0.2.0         
    ##  [69] evaluate_0.10.1           lattice_0.20-35          
    ##  [71] bindr_0.1.1               rstantools_1.5.0         
    ##  [73] htmlwidgets_1.2           labeling_0.3             
    ##  [75] tidyselect_0.2.4          processx_3.2.1           
    ##  [77] plyr_1.8.4                magrittr_1.5             
    ##  [79] R6_2.3.0                  generics_0.0.2           
    ##  [81] pillar_1.3.1              haven_1.1.2              
    ##  [83] withr_2.1.2               xts_0.10-2               
    ##  [85] abind_1.4-5               modelr_0.1.2             
    ##  [87] crayon_1.3.4              arrayhelpers_1.0-20160527
    ##  [89] utf8_1.1.4                rmarkdown_1.10           
    ##  [91] grid_3.5.1                readxl_1.1.0             
    ##  [93] callr_3.1.0               threejs_0.3.1            
    ##  [95] digest_0.6.18             xtable_1.8-2             
    ##  [97] httpuv_1.4.4.2            stats4_3.5.1             
    ##  [99] munsell_0.5.0             viridisLite_0.3.0        
    ## [101] shinyjs_1.0
