---
title: "Chapter 19. Metric Predicted Variable with One Nominal Predictor"
author: "A Solomon Kurz"
date: "`r format(Sys.Date())`"
output:
  github_document
---

```{r set-options_19, echo = FALSE, cache = FALSE}
options(width = 100)
```

# Metric Predicted Variable with One Nominal Predictor

> This chapter considers data structures that consist of a metric predicted variable and a nominal predictor.... This type of data structure can arise from experiments or from observational studies. In experiments, the researcher assigns the categories (at random) to the experimental subjects. In observational studies, both the nominal predictor value and the metric predicted value are generated by processes outside the direct control of the researcher. In either case, the same mathematical description can be applied to the data (although causality is best inferred from experimental intervention).
>
> The traditional treatment of this sort of data structure is called single-factor analysis of variance (ANOVA), or sometimes one-way ANOVA. Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter will also consider the situation in which there is also a metric predictor that accompanies the primary nominal predictor. The metric predictor is sometimes called a covariate, and the traditional treatment of this data structure is called analysis of covariance (ANCOVA). The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups, etc. (pp. 553--554)

## 19.1. Describing multiple groups of metric data

> Figure 19.1 illustrates the conventional description of grouped metric data. Each group is represented as a position on the horizontal axis. The vertical axis represents the variable to be predicted by group membership. The data are assumed to be normally distributed within groups, with equal standard deviation in all groups. The group means are deflections from overall baseline, such that the deflections sum to zero. Figure 19.1 provides a specific numerical example, with data that were randomly generated from the model. (p. 554)

We'll want a custom data generating function for our primary group data.

```{r, warning = F, message = F}
library(tidyverse)

generate_data <- function(seed, mean){
  set.seed(seed)
  rnorm(n, mean = grand_mean + mean, sd = 2)
}

n          <- 100
grand_mean <- 101

d <-
  tibble(group     = 1:5,
         deviation = c(4, -5, -2, 6, -3)) %>% 
  mutate(d = purrr::map2(group, deviation, generate_data)) %>% 
  unnest() %>% 
  mutate(iteration = rep(1:n, times = 5))

head(d)
```

With `n <- 100`, we generated more cases that necessary for the jitter plots. To my eye, it appears Kruschke jittered about 20 points per group. However, we’ll want more values that that to get the Gaussian curves to look good. You’ll see.

In addition to the primary data, `d`, we'll wand two supplimentary tibbles to add the flourished to the plot. The `arrow` tibble will specify our light-gray arrows. The `betas` tibble will contain our annotation information.

```{r}
arrow <-
  tibble(group     = 1:5,
         d         = grand_mean,
         deviation = c(4, -5, -2, 6, -3),
         offset    = .1)

head(arrow)

betas <-
  tibble(group     = c(0:5, 0),
         d         = grand_mean,
         deviation = c(0, 4, -5, -2, 6, -3, 10),
         offset    = 1/4,
         angle     = rep(c(270, 0), times = c(6, 1)),
         label     = c("beta[0] == 101", "beta['[1]'] == 4", "beta['[2]'] == -5", "beta['[3]'] == -2", "beta['[4]'] == 6", "beta['[5]'] == 3", "sigma['all'] == 2"))

head(betas)
```

Now we're ready to plot.

```{r, fig.width = 6, fig.height = 2.75, warning = F, message = F}
library(ggridges)

d %>% 
  filter(iteration < 21) %>% 
  
  ggplot(aes(x = d, y = group, group = group)) +
  geom_vline(xintercept = grand_mean, color = "white") +
  geom_jitter(height = .05, alpha = 1/4, shape = 1) +
  # the Gausians
  geom_ridgeline(data = d %>%
                   mutate(d = seq(from = 85, to = 115, length.out = n) %>% 
                            rep(., times = 5)),
                 aes(height = dnorm(d, grand_mean + deviation, 2)),
                 fill = "transparent",color = "grey50",
                 scale = 4/3, min_height = .0075, size = 3/4) +
  # the small arrows
  geom_segment(data = arrow,
               aes(x = d, xend = d + deviation,
                   y = group - offset, yend = group - offset),
               color = "grey50", size = 1,
               arrow = arrow(length = unit(.2, "cm"))) +
  # the large arrow on the left
  geom_segment(aes(x = 80, xend = grand_mean,
                   y = 0, yend = 0),
               color = "grey50", size = 3/4,
               arrow = arrow(length = unit(.2, "cm"))) +
  # the text
  geom_text(data = betas,
            aes(x = grand_mean + deviation, y = group - offset,
                label = label, angle = angle), 
            size = 4, parse = T) +
  scale_y_continuous(breaks = 1:5,
                     labels = c("<1,0,0,0,0>", "<0,1,0,0,0>", "<0,0,1,0,0>", "<0,0,0,1,0>", "<0,0,0,0,1>")) +
  coord_flip(ylim = c(-0.5, 5.5), xlim = 90:112) +
  labs(x = NULL,
       y = NULL) +
  theme(panel.grid = element_blank())
```

For this version of the figure, we used `ggridges::geom_ridgeline()` to make the rotated Gaussians. Working within that framework, I still haven’t figured out how to get the Gaussian curves to bulge out to the left, rather than the right. If you know a slick way to do that, [please share your magic](https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues). But otherwise, just hold your horses a bit. We'll explore other options in a bit.

## 19.2. Traditional analysis of variance

> The terminology, “analysis of variance,” comes from a decomposition of overall data variance into within-group variance and between-group variance ([Fisher, 1925](http://psycnet.apa.org/record/1925-15003-000)). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word “analysis” is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556)

## 19.3. Hierarchical Bayesian approach

"Our goal is to estimate its parameters in a Bayesian framework. Therefore, all the parameters need to be given a meaningfully structured prior distribution" (p. 557). However, our approach will depart a little from the one in the text. All our parameters will **not** "have generic noncommittal prior distributions" (p. 557). Most importantly, we will not follow the example in [Gelman (2006)](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf) of putting a broad uniform prior on $\sigma_y$. Rather, we will continue using the half-Gaussian prior, as [recommended by the Stan team](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). However, we will follow Kruschke's lead for the overall intercept and use a Gaussian prior "made broad on the scale of the data" (p. 557). And like Kruschke, we will estimate $\sigma_{\beta}$ from the data.

Later on, Kruschke opined

> A crucial pre-requisite for estimating σβ from all the groups is an assumption that all the groups are representative and informative for the estimate. It only makes sense to influence the estimate of one group with data from the other groups if the groups can be meaningfully described as representative of a shared higher-level distribution. (p. 559)

Although I agree with him in spirit, this doesn't appear to strictly be the case. As odd and paradoxical as this sounds, partial pooling can be of use even when the some of the cases are of a different kind. For more on the topic, see [Efron and Morris's classic paper](http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf) and [my project](https://github.com/ASKurz/James-Stein-and-Bayesian-partial-pooling) walking out one of their examples in brms.

### 19.3.1 Implementation in ~~JAGS~~ brms

The brms setup, of course, differs a bit from JAGS.

```{r, eval = F}
fit <- 
  brm(data = my_data, 
      family = gaussian,
      y ~ 1 + (1 | categirical_variable),
      prior = c(prior(normal(0, x), class = Intercept),
                prior(normal(0, x), class = b),
                prior(cauchy(0, x), class = sd),
                prior(cauchy(0, x), class = sigma)))
```

The noise standard deviation $\sigma_y$ is depicted in the prior statement at with the `class` argument set to `sigma`. The grand mean is depicted by the first `1` in the modle formula and its prior is indicated by the `class = Intercept` argument. We indicate we'd like group-based deviations from the grand mean with the `(1 | categirical_variable)` syntax, where the `1` on the left side of the bar indicates we'd like our intercepts to vary by group and the `categirical_variable` part simply representing the name of a given categorical variable we'd like those intercepts to vary by. The brms default is to do this with deviance scores, the mean for which will be zero. Although it's not obvious in the formula syntax, the model presumes the group-based deviations are normally distributed with a mean of zero and a standard deviation, which Kruschke termed $\sigma_\beta$. There is no prior for the mean. It's set at zero. But there is a prior for $\sigma_\beta$, which is denoted by the argument `class = sd`. We, of course, are not using a uniform prior on any of our variance parameters. But in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there's no need to worry about doing so ourselves. So even though the syntax only indicates `cauchy`, it's understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes is a half-Cauchy. 

Kruschke set the upper bound for his $\sigma_y$ to 10 times the standard deviation of the criterion variable. The tails of the half-Cauchy are sufficiently fat that, in practice, I've found it doesn't matter much what you set the SD of its prior to. One is often a sensible default for reasonably scaled data. But if we want to taka a more principled approach, we can set it to the size of the criterion's $SD$ or perhaps even 10 times that.

Kruschke suggested using a gamma on $\sigma_\beta$, which is a sensible alternative to half-Cauchy often used within the Stan universe. Especially in situations in which you would like to (a) keep the variance parameter above zero, but (b) still allow it to be arbitrarily close to zero, and also (c) let the likelihood dominate the posterior, the Stan team recommends the gamma(2, 0) prior, based on the [paper by Chung and colleagues](http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf). But you should note that I don't mean a literal 0 for the second parameter in the gamma distribution, but rather some small value like 0.1 or so. This is all clarified in [Chung et al](http://www.stat.columbia.edu/~gelman/research/published/chung_etal_Pmetrika2013.pdf). Here's what gamma(2, 0.1) looks like. 

```{r, fig.width = 6, fig.height = 2}
tibble(x = seq(from = 0, to = 110, by = .1)) %>% 
  
  ggplot(aes(x    = x,
             ymin = 0,
             ymax = dgamma(x, 2, .1))) +
  geom_ribbon() +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:100) +
  theme(panel.grid = element_blank())
```

And if you'd like that prior be even less informative, just reduce it to like gamma(2, 0.01) or so. Kruschke goes further to recommend "the shape and rate parameters of the gamma distribution are set so its mode is `sd(y)/2` and its standard deviation is `2*sd(y)`, using the function `gammaShRaFromModeSD` explained in Section 9.2.2." (pp. 560--561). That function, recall, follows the form:

```{r}
gamma_a_b_from_omega_sigma <- function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate <- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape <- 1 + mode * rate
  return(list(shape = shape, rate = rate))
}
```

So in the case of standardized data where `sd(1)` = 1,

```{r}
sd_y  <- 1 

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

And that produces the following gamma distribution.

```{r, fig.width = 6, fig.height = 2}
tibble(x = seq(from = 0, to = 50, by = .01)) %>% 
  
  ggplot(aes(x    = x,
             ymin = 0,
             ymax = dgamma(x, s_r$shape, s_r$rate))) +
  geom_ribbon() +
  scale_x_continuous(breaks = c(0, 1, 5, 10)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:10) +
  theme(panel.grid = element_blank())
```

In the parameter space that matters, from zero to one, that gamma is pretty noninformative. It peaks between the two, slopes very gently rightward, but has the nice steep slope on the left keeping the estimates off the zero boundary. And even though that right slope is very gentle given the scale of the data, it’s aggressive enough that it should keep the MCMC chains from spending a lot of time in ridiculous parts of the parameter space. I.e., when working with finite numbers of iterations, we want our MCMC chains wasting exactly zero iterations investigating what the density might be for $\sigma_\beta \approx 1e10$ for standardized data.

### 19.3.2 Example: Sex and death

Let's load and `glimpse()` at the data.

```{r, message = F}
my_data <- read_csv("data.R/FruitflyDataReduced.csv")

glimpse(my_data)
```

We can use `geom_density_ridges()` to help get a sense of how our criterion `Longevity` is distributed across groups of `CompanionNumber`.

```{r, fig.width = 6, fig.height = 2.5}
my_data %>% 
  group_by(CompanionNumber) %>% 
  mutate(group_mean = mean(Longevity)) %>% 
  
  ggplot(aes(x = Longevity, y = reorder(CompanionNumber, group_mean), fill = group_mean)) +
  geom_density_ridges(scale = 3/2, size = .2, color = "grey92") +
  scale_fill_viridis_c(option = "A", end = .92) +
  ylab(NULL) +
  theme(panel.grid      = element_blank(),
        legend.position = "none",
        axis.ticks.y    = element_blank(),
        axis.text.y     = element_text(hjust = 0))
```

Let's fire up brms.

```{r, warning = F, message = F}
library(brms)
```

We'll want to do the prepatory work to define our `stanvars`.

```{r}
(mean_y <- mean(my_data$Longevity))
(sd_y   <- sd(my_data$Longevity))

omega   <- sd_y / 2
sigma   <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

With the prep work is done, here are our `stanvars`.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the model, our hierarchical Bayesian alternative to ANOVA.

```{r fit1, cache = T, message = F, warning = F}
fit1 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.999),
      stanvars = stanvars)
``` 

Much like Kruschke's JAGS chains, our brms chains are well behaved.

```{r}
plot(fit1)
```

Also like Kruschke, our chains are moderately autocorrelated, too.

```{r, message = F, warning = F}
post <- posterior_samples(fit1, add_chain = T)

library(bayesplot)

theme_set(theme_grey() +
            theme(panel.grid = element_blank()))

mcmc_acf(post, pars = c("b_Intercept", "sd_CompanionNumber__Intercept", "sigma"), lags = 10)
```

Here's the model summary.

```{r}
print(fit1)
```

Here are the summaries of the group-specific deflections.

```{r}
ranef(fit1)
```

And here are those same group-level summaries in a non-deflection metric.

```{r}
coef(fit1)
```

Those are all estimates of the group-specific means. Since it wasn't modeled, all have the same parameter estimates for $\sigma_y$.

```{r}
posterior_summary(fit1)["sigma", ]
```

To prepare for our version of the top panel of Figure 19.3, we'll use `sample_n()` to randomly sample from the posterior draws.

```{r}
# how many random draws from the posterior would you like?
n_draws <- 20

set.seed(19)
post_draws <-
  post %>% 
  sample_n(size = n_draws, replace = F)

glimpse(post_draws)
```

Before we make our version of the top panel, let’s make a corresponding plot of the fixed intercept, the grand mean.

```{r, fig.height = 2.5, fig.width = 4}
tibble(x = c(0, 150)) %>% 

  ggplot(aes(x = x)) +
  mapply(function(mean, sd) {
    stat_function(fun   = dnorm, 
                  args  = list(mean = mean, sd = sd), 
                  alpha = 2/3, 
                  size = 1/3,
                  color = "grey50")
    }, 
    # Enter means and standard deviations here
    mean = post_draws[, "b_Intercept"],
    sd   = post_draws[, "sigma"]
    ) +
  geom_jitter(data = my_data, aes(x = Longevity, y = -0.001),
              height = .001, 
              alpha = 1/2) +
  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 25)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:110) + 
  labs(title = "Posterior Predictive Distribution",
       subtitle = "The jittered dots are the ungrouped Longevity data. The\nGaussians are posterior draws depicting the overall\ndistribution, the grand mean.",
       x = "Longevity")
```

Unfortunately, we can’t extend our `mapply(stat_function())` method to the group-level estimates. To my knowledge, there isn’t a way to show the group estimates at different spots along the y-axis. But there are other ways. We'll need a little help from tidybayes.

```{r, warning = F, message = F}
library(tidybayes)
```

First, here's a glimpse of what we get from `tidybayes::spread_draws()`, about which you can learn more [here](https://mjskay.github.io/tidybayes/articles/tidy-brms.html).

```{r}
sd <-
  fit1 %>% 
  spread_draws(b_Intercept, sigma, r_CompanionNumber[CompanionNumber,])
  
head(sd)
```

In our `sp` tibble, we have much of the same information we'd get from `brms::posterior_samples()`, but in the long format with respect to the random effects for `CompanionNumber`. Also notice that each row is indexed by the chain, iteration, and draw number. Among those, `.draw` if the column that corresponds to a unique row from what we'd get from `brms::posterior_samples()`. This is the index that ranges from 1 to the number of chains multiplied by the number of post-warmup iterations.  

But we need to wrangle a bit. First, it’s important to understand that by default, `spread_draws()` is grouped by our grouping variable `CompanionNumber`. As such, when we use `sample_n()`, we’ll draw the same number of rows from each group—20 in this case. Within `expand()`, we select the columns we’d like to keep within the `nesting()` function and then expand the tibble by adding an `x` value of 0 to 120 for each. This sets us up to use the `dnorm()` function in the next line to compute the density for each of those `x` values based on 20 unique normal distributions for each of the five `CompanionNumber` groups. 

The difficulty, however, is that all of these densities will have a minimum value of around 0 and all will be on the same basic scale. So we need a way to serially shift the density values up the y-axis in such a way that they’ll be sensibly separated by group. As far as I can figure, this’ll take us a couple steps. For the first step, we’ll create an intermediary variable, `g`, with which we’ll arbitrarily assign each of our five groups an integer index ranging from 0 to 4.

```{r}
set.seed(19)
sd <-
  sd %>% 
  sample_n(n_draws) %>%
  expand(nesting(.draw, b_Intercept, sigma, CompanionNumber, r_CompanionNumber), 
         x = 0:120) %>%
  mutate(density = dnorm(x,    mean = b_Intercept + r_CompanionNumber, sd = sigma),
         ll      = qnorm(.025, mean = b_Intercept + r_CompanionNumber, sd = sigma),
         ul      = qnorm(.975, mean = b_Intercept + r_CompanionNumber, sd = sigma),
         g       = recode(CompanionNumber,
                          None0     = 0,
                          Pregnant1 = 1,
                          Pregnant8 = 2,
                          Virgin1   = 3,
                          Virgin8   = 4))

head(sd)
```

This last step is tricky. Here we use our `g` integers to sequentially shift the density values up. Since our `g` value for ` None0 = 0`, those will keep 0 as their baseline. As our `g` value for `Pregnant1 = 1`, the baseline for those will now increase by 1. And so on for the other groups. But we still need to do a little more fiddling. What we want is for the maximum values of the density estimates to be a little lower than the baslines of the ones one grouping variable up. That is, we want the maximum values for the `None0` densities to fall a little bit below 1 on the y-axis. It’s with the `* .75 / max(density) ` part of the code that we accomplish that task. If you want to experiment with more or less room between the top and bottom of each density, play around with increasing/decreasing that .75 value. It'll make sense in the plot.

But so here it is, our version of the top panel of Figure 19.3.

```{r, fig.height = 4.5, fig.width = 4}
sd %>% 
  mutate(density = g + density * .75 / max(density)) %>% 
  filter(x < ul,
         x > ll) %>% 
  
  ggplot(aes(x = x, y = density)) +
  # here we make our density lines
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 2/3, size = 1/3, color = "grey50") +
  # we use an augmented version of the original data for the jittered points
  geom_jitter(data = my_data %>% 
                mutate(density = recode(CompanionNumber,
                                        None0     = 0,
                                        Pregnant1 = 1,
                                        Pregnant8 = 2,
                                        Virgin1   = 3,
                                        Virgin8   = 4)),
              aes(x = Longevity),
              height = .04, alpha = 1/2) +
  # pretty much everything below this line is aesthetic fluff
  scale_x_continuous(breaks = seq(from = 0, to = 100, by = 25)) +
  scale_y_continuous(breaks = 0:4,
                     labels = c("None0", "Pregnant1", "Pregnant8", "Virgin1", "Virgin8")) +
  coord_cartesian(xlim = 0:110) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       x = "Longevity",
       y = NULL) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

In case it wasn’t clear, our last data wrangling step with `filter()` was to trim any density values that were outside of the percentile-based 95% intervals. Since Gaussians densities technically extend across the entire expanse of the x-axis, this is what keeps the shapes at a reasonable width mating the ones in Kruschke’s figures. If it’s still not clear, just make run that last block of code, again, but with omitting the `filter()` lines.

To return to the more substantive interpretation, the top panel of

> Figure 19.3 suggests that the normal distributions with homogeneous variances appear to be reasonable descriptions of the data. There are no dramatic outliers relative to the posterior predicted curves, and the spread of the data within each group appears to be reasonably matched by the width of the posterior normal curves. (Be careful when making visual assessments of homogeneity of variance because the visual spread of the data depends on the sample size; for a reminder see the right panel of Figure 15.9, p. 442.) The range of credible group means, indicated by the peaks of the normal curves, suggests that the group Virgin8 is clearly lower than the others, and the group Virgin1 might be lower than the controls. To find out for sure, we need to examine the differences of group means, which we do in the next section. (p. 564)

### 19.3.3 Contrasts

> It is straight forward to examine the posterior distribution of credible differences. Every step in the MCMC chain provides a combination of group means that are jointly credible, given the data. Therefore, every step in the MCMC chain provides a credible difference between groups...
>
>To construct the credible differences of group 1 and group 2, at every step in the MCMC chain we compute
>
>$$
>\begin{eqnarray}
>\mu_1 - \mu_2 & = & (\beta_0 + \beta_1) - (\beta_0 + \beta_2) \\
>& = & (+1) \cdot \beta_1 + (-1) \cdot \beta_2
>\end{eqnarray}
>$$
>
> In other words, the baseline cancels out of the calculation, and the difference is a sum of weighted group deflections. Notice that the weights sum to zero. To construct the credible differences of the average of groups 1-3 and the average of groups 4-5, at every step in the MCMC chain we compute
>
>$$
>\begin{eqnarray}
>(\mu_1 + \mu_2 + \mu_3) / 3 - (\mu_4 + \mu_5) / 2 & = & ((\beta_0 + \beta_1)  + (\beta_0 + \beta_2)  >+ (\beta_0 + \beta_3) ) / 3 - ((\beta_0 + \beta_4) + (\beta_0 + \beta_5) ) / 2 \\
>& = & (\beta_1 + \beta_2 + \beta_3) / 3 - (\beta_4 + \beta_5) / 2 \\
>& = & (+ 1/3) \cdot \beta_1 + (+ 1/3) \cdot \beta_2 + (+ 1/3) \cdot \beta_3 + (- 1/2) \cdot \beta_4 + >(- 1/2) \cdot \beta_5
>\end{eqnarray}
>$$
>
> Again, the difference is a sum of weighted group deflections. The coefficients on the group deflections have the properties that they sum to zero, with the positive coefficients summing to +1 and the negative coefficients summing to −1. Such a combination is called a contrast. The differences can also be expressed in terms of effect size, by dividing the difference by $\sigma_y$ at each step in the chain. (pp. 565--566)

To warm up, here's how to compute the first contrast shown in the lower portion of Kruschke's Figure 19.3--the contrast between the two pregnant conditions and the none-control condition.

```{r, fig.width = 3, fig.height = 2.5}
post %>% 
  transmute(c = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %>% 
  
  ggplot(aes(x = c)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Pregnant1.Pregnant8 vs None0",
       x = "Difference")
```

In case you were curious, here are the HMC-based posterior mode and 95% HDIs.

```{r}
post %>% 
  transmute(difference = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) %>% 
  
  mode_hdi(difference)
```

Little difference, there. Now let's quantify the same contrast as an effect size.

```{r, fig.width = 3, fig.height = 2.5}
post %>% 
  transmute(es = ((`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`) / sigma) %>% 
  
  ggplot(aes(x = es)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(subtitle = "Pregnant1.Pregnant8 vs None0",
       x = "Effect Size")
```

Tiny. 

Okay, now lets do the rest in bulk. First we'll do the difference scores.

```{r, fig.width = 8, fig.height = 2.5}
differences <-
  post %>% 
  transmute(`Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`,
            
            `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2)

differences %>% 
  gather() %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Difference") +
  facet_wrap(~key, scales = "free_x")
```

Now the effect sizes.

```{r, fig.width = 8, fig.height = 2.5}
differences %>% 
  mutate_all(funs(. / post$sigma)) %>% 
  gather() %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Effect Size") +
  facet_wrap(~key, scales = "free_x")
```

> In traditional ANOVA, analysts often perform a so-called omnibus test that asks whether it is plausible that all the groups are simultaneously exactly equal. I find that the omnibus test is rarely meaningful, however.... In the hierarchical Bayesian estimation used here, there is no direct equivalent to an omnibus test in ANOVA, and the emphasis is on examining all the meaningful contrasts. (p. 567)

If you really wanted to, I suppose one rough analogue would be to use information criteria to compare the hierarchical model to one that includes a single intercept with no group-level deflections. Here’s what the simpler model would look like.

```{r fit1_without_deflections, cache = T, message = F, warning = F}
fit1_without_deflections <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1,
      prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      stanvars = stanvars)
``` 

Here's the model summary.

```{r}
print(fit1_without_deflections)
```

Here are their LOO values and their difference score.

```{r}
loo(fit1, fit1_without_deflections)
```

The hierarchical model has a better LOO. Here's the stacking-based model weights.

```{r}
(mw <- model_weights(fit1, fit1_without_deflections))
```

If you don't like scientific notation, just `round()`.

```{r}
mw %>% 
  round(digits = 3)
```

Yep, in complimenting the LOO difference, virtually all the stacking weight went to the hierarchical model. You might think of this another way. The conceptual question we're asking is does it make sense to say that the $\sigma_\beta$ parameter is zero. Is zero a credible value? We'll, I suppose we could just look at the posterior to assess for that.

```{r, fig.width = 4, fig.height = 2.5}
post %>% 
  ggplot(aes(x = sd_CompanionNumber__Intercept)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, binwidth = 1) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = 0:50) +
  labs(title = expression(paste("Behold the fit1 posterior for the ", sigma[beta], " parameter.")),
       subtitle = "This parameter's many things, but zero isn't one of them.",
       x = NULL)
```

Yeah, zero and other values close to zero don't look credible for that parameter. 95% of the mass is between 5 and 30, with the bulk hovering around 10. We don't need an $F$-test or even a LOO model comparison to see the writing on wall.

### 19.3.4 Multiple comparisons and shrinkage

> The previous section suggested that an analyst should investigate all contrasts of interest. This recommendation can be thought to conflict with traditional advice in the context on null hypothesis significance testing, which instead recommends that a minimal number of comparisons should be conducted in order to maximize the power of each test while keeping the overall false alarm rate capped at 5% (or whatever maximum is desired).... Instead, a Bayesian analysis can mitigate false alarms by incorporating prior knowledge into the model. In particular, hierarchical structure (which is an expression of prior knowledge) produces shrinkage of estimates, and shrinkage can help rein in estimates of spurious outlying data. For example, in the posterior distribution from the fruit fly data, the modal values of the posterior group means have a range of 23.2. The sample means of the groups have a range of 26.1. Thus, there is some shrinkage in the estimated means. The amount of shrinkage is dictated only by the data and by the prior structure, not by the intended tests. (p. 568)

We may as well compute those ranges by hand. Here's the range of the observed data.

```{r}
my_data %>% 
  group_by(CompanionNumber) %>% 
  summarise(mean  = mean(Longevity)) %>% 
  summarise(range = max(mean) - min(mean))
```

For our hierarchical model `fit1`, the posterior means are rank ordered in the same way as the empirical data.

```{r}
coef(fit1)$CompanionNumber[, , "Intercept"] %>% 
  data.frame() %>% 
  rownames_to_column() %>% 
  arrange(Estimate) %>% 
  mutate_if(is.double, round, digits = 1)
```

If we compute the range by a difference of the point estimates of the highest and lowest posterior means, we can get a quick number.

```{r}
coef(fit1)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  summarise(range = max(Estimate) - min(Estimate))
```

But this isn't fully Bayesian of us. Those means and their difference carry uncertainty with them and that uncertainty can be fully expressed if we use all the posterior samples.

```{r}
coef(fit1, summary = F)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  transmute(range = Pregnant1 - Virgin8) %>% 
  mode_hdi(range)
```

Happily, the central tendency of the range is near equivalent with both methods, but now we have 95% intervals, too. Do note how wide they are.

### 19.3.5 The two-group case

> A special case of our current scenario is when there are only two groups. The model of the present section could, in principle, be applied to the two-group case, but the hierarchical structure would do little good because there is virtually no shrinkage when there are so few groups (and the top-level prior on $\sigma_\beta$ is broad as assumed here). (p. 568)

For kicks and giggles, let’s practice. Since `Pregnant1` and `Virgin8` had the highest and lowest empirical means—making them the groups best suited to define our range, we’ll use them to fit the 2-group hierarchical model. To fit it with haste, just use `update()`.

```{r fit2, cache = T, message = F, warning = F, results = "hide"}
fit2 <-
  update(fit1,
         newdata = my_data %>% 
           filter(CompanionNumber %in% c("Pregnant1", "Virgin8")))
```

Even with just two groups, there were no gross issues with fitting the model.

```{r}
print(fit2)
```

If you compare the posteriors for $\sigma_\beta$ across the two models, you'll see how the one for `fit2` is substantially larger.

```{r}
posterior_summary(fit1)["sd_CompanionNumber__Intercept", ]
posterior_summary(fit2)["sd_CompanionNumber__Intercept", ]
```

This implies less shrinkage and a larger range. 

```{r}
coef(fit2, summary = F)$CompanionNumber[, , "Intercept"] %>% 
  as_tibble() %>% 
  transmute(range = Pregnant1 - Virgin8) %>% 
  mode_hdi(range)
```

And indeed, the range between the two groups is larger. Now the posterior mode for their difference has almost converged to that of the raw data. Kruschke then went on to recommend using a single-level model in such situations, instead.


>  That is why the two-group model in Section 16.3 did not use hierarchical structure, as illustrated in Figure 16.11 (p. 468). That model also used a $t$ distribution to accommodate outliers in the data, and that model allowed for heterogeneous variances across groups. Thus, for two groups, it is more appropriate to use the model of Section 16.3. The hierarchical multi-group model is generalized to accommodate outliers and heterogeneous variances in Section 19.5. (p. 568)

As a refresher, here's what the brms code for that chapter 16 model looked like.

```{r, eval = F}
fit3 <-
  brm(data = my_data,
      family = student,
      bf(Score ~ 0 + Group, sigma ~ 0 + Group),
      prior = c(prior(normal(mean_y, sd_y*100), class = b),
                prior(normal(0, log(sd_y)), class = b, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars)
```

Let's adjsut it for our data. Since we have a reduced data set, we'll need to recompute our `stanvars` values, whicih were based on the raw data.

```{r}
# It's easier to just make a reduced data set
my_small_data <-
  my_data %>% 
  filter(CompanionNumber %in% c("Pregnant1", "Virgin8"))
  
(mean_y <- mean(my_small_data$Longevity))
(sd_y   <- sd(my_small_data$Longevity))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))
```

Here we update `stanvars`.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

Note that our priors, here, are something of a blend of those from chapter 16 and those from our hierarchical model, `fit1`.

```{r, fit3, cache = T, message = F, warning = F}
fit3 <-
  brm(data = my_small_data,
      family = student,
      bf(Longevity ~ 0 + CompanionNumber, sigma ~ 0 + CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 10), class = b),
                prior(normal(0, log(sd_y)), class = b, dpar = sigma),
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      stanvars = stanvars)
```

Here's the model summary.

```{r}
print(fit3)
```

And here's the range in posterior means.

```{r}
fixef(fit3, summary = F) %>% 
  as_tibble() %>% 
  transmute(range = CompanionNumberPregnant1 - CompanionNumberVirgin8) %>% 
  mode_hdi(range)
```

Pretty much the same as that of the two-group hierarchical model, maybe a touch larger. Yep, Kruschke was right. Hierarchical models with two groups and permissive priors on $\sigma_\beta$ don't shrink the estimates to the grand mean all that much.

## 19.4. Including a metric predictor

"In Figure 19.3, the data within each group have a large standard deviation. For example, longevities in the Virgin8 group range from 20 to 60 days" (p. 568). Turns out Kruschke's slightly wrong on this. Probably just a typo.

```{r}
my_data %>% 
  group_by(CompanionNumber) %>% 
  summarise(min   = min(Longevity),
            max   = max(Longevity),
            range = max(Longevity) - min(Longevity))
```

But you get the point. For each group, there was quite a range. We might add predictors to the model to help account for those ranges. 

> The additional metric predictor is sometimes called a covariate. In the experimental setting, the focus of interest is usually on the nominal predictor (i.e., the experimental treatments), and the covariate is typically thought of as an ancillary predictor to help isolate the effect of the nominal predictor. But mathematically the nominal and metric predictors have equal status in the model. Let’s denote the value of the metric covariate for subject $i$ as $x_\text{cov}(i)$. Then the expected value of the predicted variable for subject $i$ is
>􏰍
>$$\mu (i) = \beta_0 + \sum_j \beta_{[j]} x_{[j]} (i) + \beta_\text{cov}  x_\text{cov}(i)$$
>
with the usual sum-to-zero constraint on the deflections of the nominal predictor stated in Equation 19.2. In words, Equation 19.5 says that the predicted value for subject $i$ is a baseline plus a deflection due to the group of $i$ plus a shift due to the value of $i$ on the covariate. (p. 569)

And the $j$ subscript, recall, denotes group membership. In this context, it often

> makes sense to set the intercept as the mean of predicted values if the covariate is re-centered at its mean value, which is denoted $\overline x_\text{cov}$. Therefore Equation 19.5 is algebraically reformulated to make the baseline respect those constraints.... The first equation below is simply Equation 19.5 with $x_\text{cov}$ recentered on its mean, $\overline x_\text{cov}$. The second line below merely algebraically rearranges the terms so that the nominal deflections sum to zero and the constants are combined into the overall baseline:
>
>$$
>\begin{eqnarray}
>\mu & = & \alpha_0 + \sum_j \alpha_{[j]} x_{[j]} + \alpha_\text{cov} (x_\text{cov} - \overline{x}_\text{cov}) \\
>& = & \underbrace{\alpha_0 + \overline{\alpha} - \alpha_\text{cov} \overline{x}_\text{cov}}_{\beta_0} + \sum_j \underbrace{(\alpha_{[j]} - \overline{\alpha})}_{\beta_j} x_{[j]} + \underbrace{\alpha_\text{cov}}_{\beta_{\text{cov}}} x_\text{cov} \\
>&& \text{where } \overline{\alpha} = \frac{1}{J} \sum^J_{j = 1} \alpha_{[j]}
>\end{eqnarray}
>$$
> (pp. 569--570)

### 19.4.1 Example: Sex, death, and size

Kruschke recalled `fit1`'s estimate for $\sigma_y$ had a posterior mode around 14.8. Let's confirm with a plot.

```{r, fig.width = 4, fig.height = 2}
posterior_samples(fit1) %>% 
  ggplot(aes(x = sigma, y = 0)) +
  geom_halfeyeh(point_range = mode_hdi, .width = c(.5, .95)) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[y])) +
  theme(panel.grid = element_blank())
```

Yep, that looks about right. That large of a difference in days would indeed make it difficult to detect between-group differences if those differences were typically on the scale of just a few days. Since `Thorax` is moderately correlated with `Longevity`, including `Thorax` in the statistical model should help shrink that $\sigma_y$ estimate, making it easier to compare group means. Following the sensibilities from the equations just above, here we'll mean-center our covariate, first.

```{r}
my_data <-
  my_data %>% 
  mutate(Thorax_c = Thorax - mean(Thorax))

head(my_data)
```

Our model code follows the structure of that in Kruschke's `Jags-Ymet-Xnom1met1-MnormalHom-Example.R` and `Jags-Ymet-Xnom1met1-MnormalHom.R` files. As a preparatory step, we redefine the values necessary for `stanvars`.

```{r}
(mean_y      <- mean(my_data$Longevity))
(sd_y        <- sd(my_data$Longevity))
(sd_Thorax_c <- sd(my_data$Thorax_c))

omega <- sd_y / 2
sigma <- 2 * sd_y

(s_r <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))

stanvars <- 
  stanvar(mean_y,      name = "mean_y") + 
  stanvar(sd_y,        name = "sd_y") +
  stanvar(sd_Thorax_c, name = "sd_Thorax_c") +
  stanvar(s_r$shape,   name = "alpha") +
  stanvar(s_r$rate,    name = "beta")
```

Now we're ready to fit the `brm()` model, our hierarchical alternative to ANCOVA.

```{r fit4, cache = T, message = F, warning = F}
fit4 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + Thorax_c + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 5),          class = Intercept),
                prior(normal(0, 2 * sd_y / sd_Thorax_c), class = b),
                prior(gamma(alpha, beta),                class = sd),
                prior(cauchy(0, sd_y),                   class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.99),
      stanvars = stanvars)
``` 

Here's the model summary.

```{r}
print(fit4)
```

Let's see if that $\sigma_y$ posterior shrank.

```{r, fig.width = 4, fig.height = 2}
posterior_samples(fit4) %>% 
  ggplot(aes(x = sigma, y = 0)) +
  geom_halfeyeh(point_range = mode_hdi, .width = c(.5, .95)) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab(expression(sigma[y]))
```

Yep, sure did! Now our between-group comparissons should be more precise. Heck, if we wanted to we could even make a difference plot.

```{r, fig.width = 4, fig.height = 2.25}
dif <- posterior_samples(fit1) %>% select(sigma) - posterior_samples(fit4) %>% select(sigma)
  
dif %>% 
  ggplot(aes(x = sigma, y = 0)) +
  geom_halfeyeh(point_range = mode_hdi, .width = c(.5, .95)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "This is a difference distribution",
       x = expression(sigma[y]))
```

If you want a quick and dirty plot of the relation between `Thorax_c` and `Longevity`, you can use  `brms::marginal_effects()`.

```{r, fig.width = 2.75, fig.height = 3}
marginal_effects(fit4)
```

But to make plots like the ones at the top of Figure 19.5, we'll have to work a little harder. First, we need the posterior samples.

```{r}
post <- posterior_samples(fit4)
```

We need some intermediary values marking off the three values along the `Thorax`-axis Kruschke singled out in his top panel plots. As far as I can tell, they were the `min()`, the `max()`, and the `mean()`.

```{r}
Thorax_m <- mean(my_data$Thorax)
(r       <- range(my_data$Thorax))
mean(r)
```

Now for each of the five `CompanionNumber` groups, we'll use `fitted()` to compute the model-implied fitted values to make the diagonal regression line(s). Since these are of a simple linear coefficient, all we need are values at two extremes, between which we'll draw the lines. `Thorax_c = c(-1, 1)` will work.

```{r}
nd <-
  my_data %>% 
  distinct(CompanionNumber) %>% 
  arrange(CompanionNumber) %>% 
  expand(CompanionNumber, Thorax_c = c(-1, 1))

f <- 
  fitted(fit4,
         newdata = nd,
         summary = F) %>% 
  as_tibble() %>% 
  set_names(str_c(my_data %>% 
                    distinct(CompanionNumber) %>%
                    arrange(CompanionNumber) %>% 
                    pull() %>% 
                    rep(., each = 2), 
                  "_", c(-1, 1)))

head(f)
```

Since we just want some modest number of random draws for each, here we'll once again use `sample_n()` and then wrangle the results a bit to make them more suitable for ggplot2.

```{r}
n_draws <- 100

set.seed(19)

f <-
  f %>% 
  mutate(iter = 1:n()) %>% 
  sample_n(n_draws) %>% 
  gather(key, value, -iter) %>% 
  separate(key, into = c("CompanionNumber", "Thorax_c"), sep = "_") %>% 
  rename(Longevity = value) %>% 
  mutate(Thorax_c = as.double(Thorax_c)) %>% 
  mutate(Thorax   = Thorax_c + mean(my_data$Thorax))

head(f)
```

Now we'll use our `spread_draws()` procedure like from a few plots above to make the Gaussians. Do note how this time we `expand()` by a combination of `Longevity` and `Thorax` values.

```{r}
set.seed(19)

sd <-
  fit4 %>% 
  spread_draws(b_Intercept, b_Thorax_c, sigma, r_CompanionNumber[CompanionNumber,]) %>% 
  sample_n(n_draws) %>%
  expand(nesting(.draw, b_Intercept, b_Thorax_c, sigma, CompanionNumber, r_CompanionNumber), 
         Longevity = -10:120, Thorax = c(r[1], mean(r), r[2])) %>%
  mutate(Thorax_c = Thorax - mean(my_data$Thorax)) %>% 
  mutate(condition_mean = b_Intercept + (Thorax_c * b_Thorax_c) + r_CompanionNumber) %>% 
  mutate(density  = dnorm(Longevity, mean = condition_mean, sd = sigma),
         ll       = qnorm(.025, mean = condition_mean, sd = sigma),
         ul       = qnorm(.975, mean = condition_mean, sd = sigma)) %>% 
  group_by(CompanionNumber) %>% 
  mutate(density  = .075 * density / max(density)) %>% 
  filter(Longevity < ul,
         Longevity > ll)

head(sd)
```

Here are the plots.

```{r, fig.width = 8, fig.height = 2.5}
f %>% 
  ggplot(aes(x = Thorax, y = Longevity)) +
  geom_line(aes(group = iter),
            alpha = 1/5, size = 1/5, color = "grey50") +
  # it's important to use `geom_path()` instead of `geom_line()`, here
  geom_path(data = sd,
            aes(x = Thorax - density,
                group = interaction(Thorax, .draw)),
            alpha = 1/5, size = 1/5, color = "grey50") +
  geom_line(data = sd,
            aes(group = interaction(Thorax, .draw)),
            alpha = 1/5, size = 1/5, color = "grey50") +
  geom_point(data = my_data,
             alpha = 1/2) +
  scale_y_continuous(breaks = seq(from = 0, to = 100, by = 20)) +
  labs(title = "Data with Posterior Predictive Distribution") +
  coord_cartesian(xlim = c(.58, 1),
                  ylim = 0:110) +
  theme(panel.grid = element_blank()) +
  facet_wrap(~CompanionNumber, ncol = 5)
```

Now we have a covariate in the model, we have to decide on which of its values we want to base our group comparisons. Unless there's a substantive reason for another value, the mean is a standard choice. And since the covariate `Thorax_c` is already mean centered, that means we can effectively leave it out of the equation. Here they are in the simple difference metric.

```{r, fig.width = 8, fig.height = 2.5}
differences <-
  post %>% 
  transmute(`Pregnant1.Pregnant8 vs None0` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]`) / 2 - `r_CompanionNumber[None0,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - `r_CompanionNumber[Virgin1,Intercept]`,
            
            `Virgin1 vs Virgin8` = `r_CompanionNumber[Virgin1,Intercept]` - `r_CompanionNumber[Virgin8,Intercept]`,
            
            `Pregnant1.Pregnant8.None0 vs Virgin1.Virgin8` = (`r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[Pregnant1,Intercept]` + `r_CompanionNumber[None0,Intercept]`) / 3 - (`r_CompanionNumber[Virgin1,Intercept]` + `r_CompanionNumber[Virgin8,Intercept]`) / 2)

differences %>% 
  gather() %>%   
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Difference") +
  theme(strip.text = element_text(size = 6.4)) +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

Now in the effect size metric.

```{r, fig.width = 8, fig.height = 2.5}
differences %>% 
  mutate_all(funs(. / post$sigma)) %>% 
  gather() %>%   
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Effect Size") +
  theme(strip.text = element_text(size = 6.4)) +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

"The HDI widths of all the contrasts have gotten smaller by virtue of including the covariate in the analysis" (p. 571).

### 19.4.2 Analogous to traditional ANCOVA

In contrast with ANCOVA,

> Bayesian methods do not partition the least-squares variance to make estimates, and therefore the Bayesian method is analogous to ANCOVA but is not ANCOVA. Frequentist practitioners are urged to test (with $p$ values) whether the assumptions of (a) equal slope in all groups, (b) equal standard deviation in all groups, and (c) normally distributed noise can be rejected. In a Bayesian approach, the descriptive model is generalized to address these concerns, as will be discussed in Section 19.5. (p. 572)

### 19.4.3 Relation to hierarchical linear regression

Here Kruschke contrasts our last model with the one from way back in chapter 17, section 3. As a refresher, here's what that code looked like.

```{r, eval = F}
fit4 <-
  brm(data = my_data,
      family = student,
      Y_z ~ 1 + X_z + (1 + X_z || Subj),
      prior = c(prior(normal(0, 100), class = Intercept),
                prior(normal(0, 100), class = b),
                prior(normal(0, 1), class = sigma),
                prior(normal(0, 1), class = sd),
                prior(exponential(one_over_twentynine) + 1, class = nu)),
      chains = 4, cores = 4,
      stanvars = stanvars)
```

And for convenience, here's the code from the model we just fit.

```{r eval = F}
fit5 <-
  brm(data = my_data,
      family = gaussian,
      Longevity ~ 1 + Thorax_c + (1 | CompanionNumber),
      prior = c(prior(normal(mean_y, sd_y * 5),          class = Intercept),
                prior(normal(0, 2 * sd_y / sd_Thorax_c), class = b),
                prior(gamma(alpha, beta),                class = sd),
                prior(cauchy(0, sd_y),                   class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.99),
      stanvars = stanvars)
``` 

It's easy to get lost in the differences in the priors and the technical details  with the model chains and such. The main thing to notice, here, is the differences in the modle formulas (i.e., the likelihoods). Both models had intercepts and slopes. But whereas the model from 17.3 set both parameters to random, only the intercept in our last modes was random. The covariate `Thorax_c` was fixed--it did not vary by group. Had we wanted it to, or formula syntax would have been something like `Longevity ~ 1 + Thorax_c + (1 + Thorax_c || CompanionNumber)`. And again, as noted in chapter 17, the `||` portion of the syntax set the random interepts and slopes to be orthogonal. As we'll see, this will often not be the case. But let's not get ahead of ourselves.

> Conceptually, the main difference between the models is merely the focus of attention. In the hierarchical linear regression model, the focus was on the slope coefficient. In that case, we were trying to estimate the magnitude of the slope, simultaneously for individuals and overall. The intercepts, which describe the levels of the nominal predictor, were of ancillary interest. In the present section, on the other hand, the focus of attention is reversed. We are most interested in the intercepts and their differences between groups, with the slopes on the covariate being of ancillary interest. (p. 573)

## 19.5. Heterogeneous variances and robustness against outliers

On page 574, Kruschke laid out the schematic for a hierarchical Student's-$t$ model in for which both the $\nu$ and $\sigma$ parameters are random. Bürkner calls these [distributional models](https://cran.r-project.org/web/packages/brms/vignettes/brms_distreg.html) and they are indeed available within the brms framework. But there's a catch. Though we can model $\sigma$ all day long and we can even make it hierarchical, brms limits us to modeling the hierarchical $\sigma$ parameters within the typical Gaussian framework. That is, we will depart from Kruschke's schematic in that we will be

* modeling the log of $\sigma$,
* indicating its grand mean with the `sigma ~ 1` syntax,
* modeling the group-level deviations a Gaussian with a mean of 0 and standard deviation $\sigma_\sigma$ estimated from the data,
* and choosing a sensible prior for $\sigma_\sigma$ that is left-bound at 0 and gently slopes to the right (i.e., a folded $t$ or gamma distribution).

Since we're modeling $\text{log}(\sigma)$, we might Gaussian prior centered on `sd(my_data$y) %>% log()` and a reasonable spread like 1. We can simulate a little to get a sense of what those distributions look like.

```{r, fig.width = 6, fig.height = 2}
n_draws <- 1e3

set.seed(19)
tibble(prior = rnorm(n_draws, mean = log(1), sd = 1)) %>% 
  mutate(prior_exp = exp(prior)) %>% 
  gather(key, value) %>% 

  ggplot(aes(x = value)) +
  geom_density(fill = "black", color = "transparent") +
  facet_wrap(~key, scales = "free")
```

Here's what is looks like with `sd = 2`.

```{r, fig.width = 3, fig.height = 2}
set.seed(19)
tibble(prior = rnorm(n_draws, mean = log(1), sd = 2)) %>% 
  mutate(prior_exp = exp(prior)) %>% 

  ggplot(aes(x = prior_exp)) +
  geom_density(fill = "black", color = "transparent") +
  coord_cartesian(xlim = 0:17)
```

Though we're still peaking around 1, there's more mass in the tail, making it easier for the likelihood to pull away from the prior mode.

But all this is the prior on the fixed effect, the grand mean of $\text{log}(\sigma)$. Keep in mind we’re also estimating group-level deflections using a hierarchical model. The good old folded $t$ on the unit scale is already pretty permissive for an estimate that is itself on the log scale. To make it more conservative, set $\nu$ to infinity and go with a folded Gaussian. Or keep your regularization loose and go with a low-$\nu$ folded $t$ or even a folded Cauchy. And, of course, one could even go with a gamma.

Consider we have data `my_data` for which our primary variable of interest is `y`. Starting from preparing our `stanvars` values, here's what the model code might look like.

```{r, eval = F}
# get ready for `stanvars`
mean_y <- mean(my_data$y)
sd_y   <- sd(my_data$y)

omega  <- sd_y / 2
sigma  <- 2 * sd_y

s_r    <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma)

# define `stanvars`
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")

# fit the model
fit <-
  brm(data = my_data,
      family = student,
      bf(Longevity ~ 1 + (1 | CompanionNumber), 
         sigma     ~ 1 + (1 | CompanionNumber)),
      prior = c(# grand means
                prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma),
                # the priors controlling the spread for our hierarchical deflections
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1), class = sd, dpar = sigma),
                # don't forget our student-t nu
                prior(exponential(one_over_twentynine), class = nu)),
      stanvars = stanvars)
``` 

### 19.5.1 Example: Contrast of means with different variances

Let's load and take a look at Kruschke's simulated data.

```{r, message = F}
my_data <- read_csv("data.R/NonhomogVarData.csv")

head(my_data)
```

Here are the means and $SD$s for each `Group`.

```{r}
my_data %>% 
  group_by(Group) %>% 
  summarise(mean = mean(Y),
            sd   = sd(Y))
```

First we'll fit the model with homogeneous variances. To keep things simple, here we'll fit a conventional model following the form of our original `fit1`. Here are our `stanvars`.

```{r}
(mean_y <- mean(my_data$Y))
(sd_y   <- sd(my_data$Y))

omega   <- sd_y / 2
sigma   <- 2 * sd_y

(s_r    <- gamma_a_b_from_omega_sigma(mode = omega, sd = sigma))

# define `stanvars`
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta")
```

Now fit the homogeneous-variances model.

```{r fit5, cache = T, message = F, warning = F}
fit5 <-
  brm(data = my_data,
      family = gaussian,
      Y ~ 1 + (1 | Group),
      prior = c(prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(gamma(alpha, beta), class = sd),
                prior(cauchy(0, sd_y), class = sigma)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.999),
      stanvars = stanvars)
``` 

Here's the model summary.

```{r}
print(fit5)
```

Let's get ready to make our version of the top of Figure 19.7. First we wrangle.

```{r}
# How many model-implied Gaussians would you like?
n_draws <- 20
set.seed(19)

sd <-
  fit5 %>% 
  spread_draws(b_Intercept, sigma, r_Group[Group,]) %>% 
  sample_n(n_draws) %>% 
  expand(nesting(.draw, b_Intercept, sigma, Group, r_Group),
         Y = seq(from = 50, to = 150, length.out = 200)) %>%
  mutate(density = dnorm(Y, mean = b_Intercept + r_Group, sd = sigma),
         ll      = qnorm(.025, mean = b_Intercept + r_Group, sd = sigma),
         ul      = qnorm(.975, mean = b_Intercept + r_Group, sd = sigma),
         g       = recode(Group,
                          A = 0,
                          B = 1,
                          C = 2,
                          D = 3)) %>% 
  mutate(density = g + density * .75 / max(density)) %>% 
  filter(Y < ul,
         Y > ll)

head(sd)
```

Now we're ready to make our version of the top panel of Figure 19.7.

```{r, fig.height = 3.5, fig.width = 3.75}
sd %>% 
  ggplot(aes(x = Y, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 2/3, size = 1/3, color = "grey50") +
  geom_jitter(data = my_data %>% 
                mutate(density = recode(Group,
                                        A = 0,
                                        B = 1,
                                        C = 2,
                                        D = 3)),
              aes(x = Y),
              height = .04, alpha = 1/2) +
  # pretty much everything below this line is aesthetic fluff
  scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) +
  scale_y_continuous(breaks = 0:3,
                     labels = LETTERS[1:4]) +
  coord_cartesian(xlim = 75:125) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       y = NULL) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

Here are the difference distributions in the middle of Figure 19.7.

```{r, fig.width = 4, fig.height = 2.5}
post <- posterior_samples(fit5)

differences <-
  post %>% 
  transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`,
            `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`)

differences %>% 
  gather() %>%
  mutate(key = factor(key, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Difference") +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

And here are the effect sizes at the bottom of the figure.

```{r, fig.width = 4, fig.height = 2.5}
differences %>% 
  mutate_all(funs(. / post$sigma)) %>% 
  gather() %>%
  mutate(key = factor(key, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Effect Size") +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

Here's the robust hierarchical variances model.

```{r}
stanvars <- 
  stanvar(mean_y,    name = "mean_y") + 
  stanvar(sd_y,      name = "sd_y") +
  stanvar(s_r$shape, name = "alpha") +
  stanvar(s_r$rate,  name = "beta") +
  stanvar(1/29,      name = "one_over_twentynine")
```

```{r fit6, cache = T, message = F, warning = F}
fit6 <-
  brm(data = my_data,
      family = student,
      bf(Y     ~ 1 + (1 | Group), 
         sigma ~ 1 + (1 | Group)),
      prior = c(# grand means
                prior(normal(mean_y, sd_y * 10), class = Intercept),
                prior(normal(log(sd_y), 1), class = Intercept, dpar = sigma),
                # the priors controlling the spread for our hierarchical deflections
                prior(gamma(alpha, beta), class = sd),
                prior(normal(0, 1), class = sd, dpar = sigma),
                # don't forget our student-t nu
                prior(exponential(one_over_twentynine), class = nu)),
      iter = 4000, warmup = 1000, chains = 4, cores = 4,
      control = list(adapt_delta = 0.99,
                     max_treedepth = 12),
      stanvars = stanvars)
``` 

```{r}
plot(fit6)
```

```{r}
print(fit6)
```

Let's get ready to make our version of the top of Figure 19.7. First we wrangle.

```{r}
# How many model-implied Gaussians would you like?
n_draws <- 20
set.seed(19)

sd <-
  fit6 %>% 
  spread_draws(b_Intercept, b_sigma_Intercept, nu, r_Group[Group,], r_Group__sigma[Group,]) %>% 
  sample_n(n_draws) %>% 
  expand(nesting(.draw, b_Intercept, b_sigma_Intercept, nu, Group, r_Group, r_Group__sigma),
         Y = seq(from = 50, to = 150, length.out = 300)) %>%
  mutate(m = b_Intercept + r_Group,
         s = exp(b_sigma_Intercept + r_Group__sigma)) %>% 
  mutate(density = dt((Y - m) / s, df = nu),
         ll      = m + qt(.025, df = nu) * s,
         ul      = m + qt(.975, df = nu) * s,
         g       = recode(Group,
                          A = 0,
                          B = 1,
                          C = 2,
                          D = 3)) %>% 
  mutate(density = g + density * .75 / max(density)) %>% 
  filter(Y < ul,
         Y > ll)

head(sd)
```


Now we're ready to make our version of the top panel of Figure 19.7.

```{r, fig.height = 3.5, fig.width = 3.75}
sd %>% 
  ggplot(aes(x = Y, y = density)) +
  geom_line(aes(group = interaction(.draw, g)),
            alpha = 2/3, size = 1/3, color = "grey50") +
  geom_jitter(data = my_data %>% 
                mutate(density = recode(Group,
                                        A = 0,
                                        B = 1,
                                        C = 2,
                                        D = 3)),
              aes(x = Y),
              height = .04, alpha = 1/2) +
  # pretty much everything below this line is aesthetic fluff
  scale_x_continuous(breaks = seq(from = 80, to = 120, by = 10)) +
  scale_y_continuous(breaks = 0:3,
                     labels = LETTERS[1:4]) +
  coord_cartesian(xlim = 75:125) +
  labs(title = "Data with Posterior Predictive Distrib.", 
       y = NULL) +
  theme(axis.ticks.y = element_blank(),
        axis.text.y  = element_text(hjust = 0))
```

Here are the difference distributions in the middle of Figure 19.8.

```{r, fig.width = 4, fig.height = 2.5}
post <- posterior_samples(fit6)

post %>% 
  transmute(`D vs A` = `r_Group[D,Intercept]` - `r_Group[A,Intercept]`,
            `C vs B` = `r_Group[C,Intercept]` - `r_Group[B,Intercept]`) %>% 
  gather() %>%
  mutate(key = factor(key, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Difference") +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

And here are the corresponding effect sizes at the bottom of the Figure 19.8.

```{r, fig.width = 4, fig.height = 2.5}
post %>% 
  transmute(`D vs A` = (`r_Group[D,Intercept]` - `r_Group[A,Intercept]`) / 
              exp((b_sigma_Intercept + `r_Group__sigma[D,Intercept]` + b_sigma_Intercept + `r_Group__sigma[A,Intercept]`) / 2),
            `C vs B` = (`r_Group[C,Intercept]` - `r_Group[B,Intercept]`) /
              exp((b_sigma_Intercept + `r_Group__sigma[C,Intercept]` + b_sigma_Intercept + `r_Group__sigma[B,Intercept]`) / 2)) %>% 
  gather() %>%
  mutate(key = factor(key, levels = c("D vs A", "C vs B"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = "Effect Size") +
  facet_wrap(~key, scales = "free_x", ncol = 4)
```

Notice that because (a) the sigma parameters were heterogeneous and (b) they were estimated on the log scale, we had to do quite a bit more data processing before they effect size estimates were ready. 

"Finally, because each group has its own estimated scale (i.e., $\sigma_j$), we can investigate differences in scales across groups" (p. 578). That's not a bad idea. Even though Kruschke didn't do this in the text, we may as well give it a go.

```{r, fig.width = 8, fig.height = 2.5}
post %>% 
  transmute(`D vs A` = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`),
            `C vs B` = exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`),
            `D vs C` = exp(b_sigma_Intercept + `r_Group__sigma[D,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[C,Intercept]`),
            `B vs A` = exp(b_sigma_Intercept + `r_Group__sigma[B,Intercept]`) - exp(b_sigma_Intercept + `r_Group__sigma[A,Intercept]`)) %>% 
  gather() %>%
  mutate(key = factor(key, levels = c("D vs A", "C vs B", "D vs C", "B vs A"))) %>% 
  
  ggplot(aes(x = value)) +
  geom_histogram(color = "grey92", fill = "grey67",
                 size = .2, bins = 40) +
  stat_pointintervalh(aes(y = 0), 
                      point_interval = mode_hdi, .width = c(.95, .5)) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(x = expression(paste("Differences in ", sigma))) +
  facet_wrap(~key, scales = "free", ncol = 4)
```

## References {-}

Kruschke, J. K. (2015). *Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan.* Burlington, MA: Academic Press/Elsevier.

## Session info {-}

```{r}
sessionInfo()
```

```{r, message = F, echo = F}
# Here we'll remove our objects
rm(generate_data, n, grand_mean, d, arrow, betas, gamma_a_b_from_omega_sigma, sd_y, omega, sigma, s_r, my_data, mean_y, stanvars, fit1, post, n_draws, post_draws, sd, fit1_without_deflections, mw, fit2, fit3, my_small_data, sd_Thorax_c, fit4, dif, Thorax_m, r, nd, f, differences, fit5, fit6)

theme_set(theme_grey())
```
