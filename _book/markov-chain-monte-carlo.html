<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7 Markov Chain Monte Carlo | Doing Bayesian Data Analysis in brms and the tidyverse</title>
  <meta name="description" content="This project is an attempt to re-express the code in Kruschke’s (2015) textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="7 Markov Chain Monte Carlo | Doing Bayesian Data Analysis in brms and the tidyverse" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This project is an attempt to re-express the code in Kruschke’s (2015) textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style." />
  <meta name="github-repo" content="ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Markov Chain Monte Carlo | Doing Bayesian Data Analysis in brms and the tidyverse" />
  <meta name="twitter:site" content="@SolomonKurz" />
  <meta name="twitter:description" content="This project is an attempt to re-express the code in Kruschke’s (2015) textbook. His models are re-fit in brms, plots are redone with ggplot2, and the general data wrangling code predominantly follows the tidyverse style." />
  

<meta name="author" content="A Solomon Kurz" />


<meta name="date" content="2020-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"/>
<link rel="next" href="jags-brms.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>What and why</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#we-have-updates"><i class="fa fa-check"></i>We have updates</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-0.1.0."><i class="fa fa-check"></i>Version 0.1.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-0.2.0."><i class="fa fa-check"></i>Version 0.2.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#version-0.3.0."><i class="fa fa-check"></i>Version 0.3.0.</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#were-not-done-yet-and-i-could-use-your-help."><i class="fa fa-check"></i>We’re not done yet and I could use your help.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#thank-yous-are-in-order"><i class="fa fa-check"></i>Thank-you’s are in order</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html"><i class="fa fa-check"></i><b>1</b> What’s in This Book (Read This First!)</a><ul>
<li class="chapter" data-level="1.1" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#real-people-can-read-this-book"><i class="fa fa-check"></i><b>1.1</b> Real people can read this book</a></li>
<li class="chapter" data-level="1.2" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#whats-in-this-book"><i class="fa fa-check"></i><b>1.2</b> What’s in this book</a></li>
<li class="chapter" data-level="1.3" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#whats-new-in-the-second-edition"><i class="fa fa-check"></i><b>1.3</b> What’s new in the second edition</a></li>
<li class="chapter" data-level="1.4" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#gimme-feedback-be-polite"><i class="fa fa-check"></i><b>1.4</b> Gimme feedback (be polite)</a></li>
<li class="chapter" data-level="1.5" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#thank-you"><i class="fa fa-check"></i><b>1.5</b> Thank you!</a></li>
<li class="chapter" data-level="" data-path="whats-in-this-book-read-this-first.html"><a href="whats-in-this-book-read-this-first.html#session-info"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html"><i class="fa fa-check"></i><b>2</b> Introduction: Credibility, Models, and Parameters</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html#bayesian-inference-is-reallocation-of-credibility-across-possibilities"><i class="fa fa-check"></i><b>2.1</b> Bayesian inference is reallocation of credibility across possibilities</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html#data-are-noisy-and-inferences-are-probabilistic."><i class="fa fa-check"></i><b>2.1.1</b> Data are noisy and inferences are probabilistic.</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html#possibilities-are-parameter-values-in-descriptive-models"><i class="fa fa-check"></i><b>2.2</b> Possibilities are parameter values in descriptive models</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html#the-steps-of-bayesian-data-analysis"><i class="fa fa-check"></i><b>2.3</b> The steps of Bayesian data analysis</a></li>
<li class="chapter" data-level="" data-path="introduction-credibility-models-and-parameters.html"><a href="introduction-credibility-models-and-parameters.html#session-info-1"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html"><i class="fa fa-check"></i><b>3</b> The R Programming Language</a><ul>
<li class="chapter" data-level="3.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#get-the-software"><i class="fa fa-check"></i><b>3.1</b> Get the software</a><ul>
<li class="chapter" data-level="3.1.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#a-look-at-rstudio."><i class="fa fa-check"></i><b>3.1.1</b> A look at RStudio.</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#a-simple-example-of-r-in-action"><i class="fa fa-check"></i><b>3.2</b> A simple example of R in action</a><ul>
<li class="chapter" data-level="3.2.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#get-the-programs-used-with-this-book."><i class="fa fa-check"></i><b>3.2.1</b> Get the programs used with this book.</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#basic-commands-and-operators-in-r"><i class="fa fa-check"></i><b>3.3</b> Basic commands and operators in R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#getting-help-in-r."><i class="fa fa-check"></i><b>3.3.1</b> Getting help in R.</a></li>
<li class="chapter" data-level="3.3.2" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#arithmetic-and-logical-operators."><i class="fa fa-check"></i><b>3.3.2</b> Arithmetic and logical operators.</a></li>
<li class="chapter" data-level="3.3.3" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#assignment-relational-operators-and-tests-of-equality."><i class="fa fa-check"></i><b>3.3.3</b> Assignment, relational operators, and tests of equality.</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#variable-types"><i class="fa fa-check"></i><b>3.4</b> Variable types</a><ul>
<li class="chapter" data-level="3.4.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#vector."><i class="fa fa-check"></i><b>3.4.1</b> Vector.</a></li>
<li class="chapter" data-level="3.4.2" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#factor."><i class="fa fa-check"></i><b>3.4.2</b> Factor.</a></li>
<li class="chapter" data-level="3.4.3" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#matrix-and-array."><i class="fa fa-check"></i><b>3.4.3</b> Matrix and array.</a></li>
<li class="chapter" data-level="3.4.4" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#list-and-data-frame."><i class="fa fa-check"></i><b>3.4.4</b> List and data frame.</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#loading-and-saving-data"><i class="fa fa-check"></i><b>3.5</b> Loading and saving data</a><ul>
<li class="chapter" data-level="3.5.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#the-read.csv-read_csv-and-read.table-read_table-functions."><i class="fa fa-check"></i><b>3.5.1</b> The <del>read.csv</del> <code>read_csv()</code> and <del>read.table</del> <code>read_table()</code> functions.</a></li>
<li class="chapter" data-level="3.5.2" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#saving-data-from-r."><i class="fa fa-check"></i><b>3.5.2</b> Saving data from R.</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#some-utility-functions"><i class="fa fa-check"></i><b>3.6</b> Some utility functions</a></li>
<li class="chapter" data-level="3.7" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#programming-in-r"><i class="fa fa-check"></i><b>3.7</b> Programming in R</a><ul>
<li class="chapter" data-level="3.7.1" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#variable-names-in-r."><i class="fa fa-check"></i><b>3.7.1</b> Variable names in R.</a></li>
<li class="chapter" data-level="3.7.2" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#running-a-program."><i class="fa fa-check"></i><b>3.7.2</b> Running a program.</a></li>
<li class="chapter" data-level="3.7.3" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#programming-a-function."><i class="fa fa-check"></i><b>3.7.3</b> Programming a function.</a></li>
<li class="chapter" data-level="3.7.4" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#conditions-and-loops."><i class="fa fa-check"></i><b>3.7.4</b> Conditions and loops.</a></li>
<li class="chapter" data-level="3.7.5" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#measuring-processing-time."><i class="fa fa-check"></i><b>3.7.5</b> Measuring processing time.</a></li>
<li class="chapter" data-level="3.7.6" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#debugging."><i class="fa fa-check"></i><b>3.7.6</b> Debugging.</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#graphical-plots-opening-and-saving"><i class="fa fa-check"></i><b>3.8</b> Graphical plots: Opening and saving</a></li>
<li class="chapter" data-level="" data-path="the-r-programming-language.html"><a href="the-r-programming-language.html#session-info-2"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html"><i class="fa fa-check"></i><b>4</b> What is This Stuff Called Probability?</a><ul>
<li class="chapter" data-level="4.1" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#the-set-of-all-possible-events"><i class="fa fa-check"></i><b>4.1</b> The set of all possible events</a></li>
<li class="chapter" data-level="4.2" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#probability-outside-or-inside-the-head"><i class="fa fa-check"></i><b>4.2</b> Probability: Outside or inside the head</a><ul>
<li class="chapter" data-level="4.2.1" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#outside-the-head-long-run-relative-frequency."><i class="fa fa-check"></i><b>4.2.1</b> Outside the head: Long-run relative frequency.</a></li>
<li class="chapter" data-level="4.2.2" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#inside-the-head-subjective-belief."><i class="fa fa-check"></i><b>4.2.2</b> Inside the head: Subjective belief.</a></li>
<li class="chapter" data-level="4.2.3" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#probabilities-assign-numbers-to-possibilities."><i class="fa fa-check"></i><b>4.2.3</b> Probabilities assign numbers to possibilities.</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#probability-distributions"><i class="fa fa-check"></i><b>4.3</b> Probability distributions</a><ul>
<li class="chapter" data-level="4.3.1" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#discrete-distributions-probability-mass."><i class="fa fa-check"></i><b>4.3.1</b> Discrete distributions: Probability mass.</a></li>
<li class="chapter" data-level="4.3.2" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#continuous-distributions-rendezvous-with-density."><i class="fa fa-check"></i><b>4.3.2</b> Continuous distributions: Rendezvous with density.</a></li>
<li class="chapter" data-level="4.3.3" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#mean-and-variance-of-a-distribution."><i class="fa fa-check"></i><b>4.3.3</b> Mean and variance of a distribution.</a></li>
<li class="chapter" data-level="4.3.4" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#highest-density-interval-hdi."><i class="fa fa-check"></i><b>4.3.4</b> Highest density interval (HDI).</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#two-way-distributions"><i class="fa fa-check"></i><b>4.4</b> Two-way distributions</a><ul>
<li class="chapter" data-level="4.4.1" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#conditional-probability."><i class="fa fa-check"></i><b>4.4.1</b> Conditional probability.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="what-is-this-stuff-called-probability.html"><a href="what-is-this-stuff-called-probability.html#session-info-3"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>5</b> Bayes’ Rule</a><ul>
<li class="chapter" data-level="5.1" data-path="bayes-rule.html"><a href="bayes-rule.html#bayes-rule-1"><i class="fa fa-check"></i><b>5.1</b> Bayes’ rule</a><ul>
<li class="chapter" data-level="5.1.1" data-path="bayes-rule.html"><a href="bayes-rule.html#derived-from-definitions-of-conditional-probability."><i class="fa fa-check"></i><b>5.1.1</b> Derived from definitions of conditional probability.</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="bayes-rule.html"><a href="bayes-rule.html#applied-to-parameters-and-data"><i class="fa fa-check"></i><b>5.2</b> Applied to parameters and data</a></li>
<li class="chapter" data-level="5.3" data-path="bayes-rule.html"><a href="bayes-rule.html#complete-examples-estimating-bias-in-a-coin"><i class="fa fa-check"></i><b>5.3</b> Complete examples: Estimating bias in a coin</a><ul>
<li class="chapter" data-level="5.3.1" data-path="bayes-rule.html"><a href="bayes-rule.html#influence-of-sample-size-on-the-posterior."><i class="fa fa-check"></i><b>5.3.1</b> Influence of sample size on the posterior.</a></li>
<li class="chapter" data-level="5.3.2" data-path="bayes-rule.html"><a href="bayes-rule.html#influence-of-prior-on-the-posterior."><i class="fa fa-check"></i><b>5.3.2</b> Influence of prior on the posterior.</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="bayes-rule.html"><a href="bayes-rule.html#why-bayesian-inference-can-be-difficult"><i class="fa fa-check"></i><b>5.4</b> Why Bayesian inference can be difficult</a></li>
<li class="chapter" data-level="" data-path="bayes-rule.html"><a href="bayes-rule.html#session-info-4"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><i class="fa fa-check"></i><b>6</b> Inferring a Binomial Probability via Exact Mathematical Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-likelihood-function-the-bernoulli-distribution"><i class="fa fa-check"></i><b>6.1</b> The likelihood function: The Bernoulli distribution</a></li>
<li class="chapter" data-level="6.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#a-description-of-credibilities-the-beta-distribution"><i class="fa fa-check"></i><b>6.2</b> A description of credibilities: The beta distribution</a><ul>
<li class="chapter" data-level="6.2.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#specifying-a-beta-prior."><i class="fa fa-check"></i><b>6.2.1</b> Specifying a beta prior.</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#the-posterior-beta"><i class="fa fa-check"></i><b>6.3</b> The posterior beta</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#posterior-is-compromise-of-prior-and-likelihood."><i class="fa fa-check"></i><b>6.3.1</b> Posterior is compromise of prior and likelihood.</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#examples"><i class="fa fa-check"></i><b>6.4</b> Examples</a><ul>
<li class="chapter" data-level="6.4.1" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#prior-knowledge-expressed-as-a-beta-distribution."><i class="fa fa-check"></i><b>6.4.1</b> Prior knowledge expressed as a beta distribution.</a></li>
<li class="chapter" data-level="6.4.2" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#prior-knowledge-that-cannot-be-expressed-as-a-beta-distribution."><i class="fa fa-check"></i><b>6.4.2</b> Prior knowledge that cannot be expressed as a beta distribution.</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#summary"><i class="fa fa-check"></i><b>6.5</b> Summary</a></li>
<li class="chapter" data-level="" data-path="inferring-a-binomial-probability-via-exact-mathematical-analysis.html"><a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#session-info-5"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>7</b> Markov Chain Monte Carlo</a><ul>
<li class="chapter" data-level="7.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#approximating-a-distribution-with-a-large-sample"><i class="fa fa-check"></i><b>7.1</b> Approximating a distribution with a large sample</a></li>
<li class="chapter" data-level="7.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#a-simple-case-of-the-metropolis-algorithm"><i class="fa fa-check"></i><b>7.2</b> A simple case of the Metropolis algorithm</a><ul>
<li class="chapter" data-level="7.2.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#a-politician-stumbles-upon-the-metropolis-algorithm."><i class="fa fa-check"></i><b>7.2.1</b> A politician stumbles upon the Metropolis algorithm.</a></li>
<li class="chapter" data-level="7.2.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#a-random-walk."><i class="fa fa-check"></i><b>7.2.2</b> A random walk.</a></li>
<li class="chapter" data-level="7.2.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#general-properties-of-a-random-walk."><i class="fa fa-check"></i><b>7.2.3</b> General properties of a random walk.</a></li>
<li class="chapter" data-level="7.2.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#why-we-care."><i class="fa fa-check"></i><b>7.2.4</b> Why we care.</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-metropolis-algorithm-more-generally"><i class="fa fa-check"></i><b>7.3</b> The Metropolis algorithm more generally</a><ul>
<li class="chapter" data-level="7.3.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#metropolis-algorithm-applied-to-bernoulli-likelihood-and-beta-prior."><i class="fa fa-check"></i><b>7.3.1</b> Metropolis algorithm applied to Bernoulli likelihood and beta prior.</a></li>
<li class="chapter" data-level="7.3.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#summary-of-metropolis-algorithm."><i class="fa fa-check"></i><b>7.3.2</b> Summary of Metropolis algorithm.</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#toward-gibbs-sampling-estimating-two-coin-biases"><i class="fa fa-check"></i><b>7.4</b> Toward Gibbs sampling: Estimating two coin biases</a><ul>
<li class="chapter" data-level="7.4.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#prior-likelihood-and-posterior-for-two-biases."><i class="fa fa-check"></i><b>7.4.1</b> Prior, likelihood and posterior for two biases.</a></li>
<li class="chapter" data-level="7.4.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-posterior-via-exact-formal-analysis."><i class="fa fa-check"></i><b>7.4.2</b> The posterior via exact formal analysis.</a></li>
<li class="chapter" data-level="7.4.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#the-posterior-via-the-metropolis-algorithm."><i class="fa fa-check"></i><b>7.4.3</b> The posterior via the Metropolis algorithm.</a></li>
<li class="chapter" data-level="7.4.4" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#gibbs-hamiltonian-monte-carlo-sampling."><i class="fa fa-check"></i><b>7.4.4</b> <del>Gibbs</del> Hamiltonian Monte Carlo sampling.</a></li>
<li class="chapter" data-level="7.4.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#is-there-a-difference-between-biases"><i class="fa fa-check"></i><b>7.4.5</b> Is there a difference between biases?</a></li>
<li class="chapter" data-level="7.4.6" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#terminology-mcmc."><i class="fa fa-check"></i><b>7.4.6</b> Terminology: MCMC.</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-representativeness-accuracy-and-efficiency"><i class="fa fa-check"></i><b>7.5</b> MCMC representativeness, accuracy, and efficiency</a><ul>
<li class="chapter" data-level="7.5.1" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-representativeness."><i class="fa fa-check"></i><b>7.5.1</b> MCMC representativeness.</a></li>
<li class="chapter" data-level="7.5.2" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-accuracy."><i class="fa fa-check"></i><b>7.5.2</b> MCMC accuracy.</a></li>
<li class="chapter" data-level="7.5.3" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#mcmc-efficiency."><i class="fa fa-check"></i><b>7.5.3</b> MCMC efficiency.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="markov-chain-monte-carlo.html"><a href="markov-chain-monte-carlo.html#session-info-6"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags-brms.html"><a href="jags-brms.html"><i class="fa fa-check"></i><b>8</b> <del>JAGS</del> brms</a><ul>
<li class="chapter" data-level="8.1" data-path="jags-brms.html"><a href="jags-brms.html#jags-brms-and-its-relation-to-r"><i class="fa fa-check"></i><b>8.1</b> <del>JAGS</del> brms and its relation to R</a></li>
<li class="chapter" data-level="8.2" data-path="jags-brms.html"><a href="jags-brms.html#a-complete-example"><i class="fa fa-check"></i><b>8.2</b> A complete example</a><ul>
<li class="chapter" data-level="8.2.1" data-path="jags-brms.html"><a href="jags-brms.html#load-data."><i class="fa fa-check"></i><b>8.2.1</b> Load data.</a></li>
<li class="chapter" data-level="8.2.2" data-path="jags-brms.html"><a href="jags-brms.html#specify-model."><i class="fa fa-check"></i><b>8.2.2</b> Specify model.</a></li>
<li class="chapter" data-level="8.2.3" data-path="jags-brms.html"><a href="jags-brms.html#initialize-chains."><i class="fa fa-check"></i><b>8.2.3</b> Initialize chains.</a></li>
<li class="chapter" data-level="8.2.4" data-path="jags-brms.html"><a href="jags-brms.html#generate-chains."><i class="fa fa-check"></i><b>8.2.4</b> Generate chains.</a></li>
<li class="chapter" data-level="8.2.5" data-path="jags-brms.html"><a href="jags-brms.html#examine-chains."><i class="fa fa-check"></i><b>8.2.5</b> Examine chains.</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="jags-brms.html"><a href="jags-brms.html#simplified-scripts-for-frequently-used-analyses"><i class="fa fa-check"></i><b>8.3</b> Simplified scripts for frequently used analyses</a></li>
<li class="chapter" data-level="8.4" data-path="jags-brms.html"><a href="jags-brms.html#example-difference-of-biases"><i class="fa fa-check"></i><b>8.4</b> Example: Difference of biases</a></li>
<li class="chapter" data-level="8.5" data-path="jags-brms.html"><a href="jags-brms.html#sampling-from-the-prior-distribution-in-jags-brms"><i class="fa fa-check"></i><b>8.5</b> Sampling from the prior distribution in <del>JAGS</del> brms</a></li>
<li class="chapter" data-level="8.6" data-path="jags-brms.html"><a href="jags-brms.html#probability-distributions-available-in-jags-brms"><i class="fa fa-check"></i><b>8.6</b> Probability distributions available in <del>JAGS</del> brms</a><ul>
<li class="chapter" data-level="8.6.1" data-path="jags-brms.html"><a href="jags-brms.html#defining-new-likelihood-functions."><i class="fa fa-check"></i><b>8.6.1</b> Defining new likelihood functions.</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="jags-brms.html"><a href="jags-brms.html#faster-sampling-with-parallel-processing-in-runjags-brmsbrm"><i class="fa fa-check"></i><b>8.7</b> Faster sampling with parallel processing in <del>runjags</del> <code>brms::brm()</code></a></li>
<li class="chapter" data-level="8.8" data-path="jags-brms.html"><a href="jags-brms.html#tips-for-expanding-jags-brms-models"><i class="fa fa-check"></i><b>8.8</b> Tips for expanding <del>JAGS</del> brms models</a></li>
<li class="chapter" data-level="" data-path="jags-brms.html"><a href="jags-brms.html#session-info-7"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="hierarchical-models.html"><a href="hierarchical-models.html"><i class="fa fa-check"></i><b>9</b> Hierarchical Models</a><ul>
<li class="chapter" data-level="9.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#a-single-coin-from-a-single-mint"><i class="fa fa-check"></i><b>9.1</b> A single coin from a single mint</a><ul>
<li class="chapter" data-level="9.1.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#posterior-via-grid-approximation."><i class="fa fa-check"></i><b>9.1.1</b> Posterior via grid approximation.</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="hierarchical-models.html"><a href="hierarchical-models.html#multiple-coins-from-a-single-mint"><i class="fa fa-check"></i><b>9.2</b> Multiple coins from a single mint</a><ul>
<li class="chapter" data-level="9.2.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#posterior-via-grid-approximation.-1"><i class="fa fa-check"></i><b>9.2.1</b> Posterior via grid approximation.</a></li>
<li class="chapter" data-level="9.2.2" data-path="hierarchical-models.html"><a href="hierarchical-models.html#a-realistic-model-with-mcmc."><i class="fa fa-check"></i><b>9.2.2</b> A realistic model with MCMC.</a></li>
<li class="chapter" data-level="9.2.3" data-path="hierarchical-models.html"><a href="hierarchical-models.html#doing-it-with-jags-brms."><i class="fa fa-check"></i><b>9.2.3</b> Doing it with <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="9.2.4" data-path="hierarchical-models.html"><a href="hierarchical-models.html#example-therapeutic-touch."><i class="fa fa-check"></i><b>9.2.4</b> Example: Therapeutic touch.</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="hierarchical-models.html"><a href="hierarchical-models.html#shrinkage-in-hierarchical-models"><i class="fa fa-check"></i><b>9.3</b> Shrinkage in hierarchical models</a></li>
<li class="chapter" data-level="9.4" data-path="hierarchical-models.html"><a href="hierarchical-models.html#speeding-up-jags-brms"><i class="fa fa-check"></i><b>9.4</b> Speeding up <del>JAGS</del> brms</a></li>
<li class="chapter" data-level="9.5" data-path="hierarchical-models.html"><a href="hierarchical-models.html#extending-the-hierarchy-subjects-within-categories"><i class="fa fa-check"></i><b>9.5</b> Extending the hierarchy: Subjects within categories</a><ul>
<li class="chapter" data-level="9.5.1" data-path="hierarchical-models.html"><a href="hierarchical-models.html#example-baseball-batting-abilities-by-position."><i class="fa fa-check"></i><b>9.5.1</b> Example: Baseball batting abilities by position.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hierarchical-models.html"><a href="hierarchical-models.html#session-info-8"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html"><i class="fa fa-check"></i><b>10</b> Model Comparison and Hierarchical Modeling</a><ul>
<li class="chapter" data-level="10.1" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#general-formula-and-the-bayes-factor"><i class="fa fa-check"></i><b>10.1</b> General formula and the Bayes factor</a></li>
<li class="chapter" data-level="10.2" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#example-two-factories-of-coins"><i class="fa fa-check"></i><b>10.2</b> Example: Two factories of coins</a><ul>
<li class="chapter" data-level="10.2.1" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#solution-by-formal-analysis."><i class="fa fa-check"></i><b>10.2.1</b> Solution by formal analysis.</a></li>
<li class="chapter" data-level="10.2.2" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#solution-by-grid-approximation."><i class="fa fa-check"></i><b>10.2.2</b> Solution by grid approximation.</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#solution-by-mcmc"><i class="fa fa-check"></i><b>10.3</b> Solution by MCMC</a><ul>
<li class="chapter" data-level="10.3.1" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#nonhierarchical-mcmc-computation-of-each-models-marginal-likelihood."><i class="fa fa-check"></i><b>10.3.1</b> Nonhierarchical MCMC computation of each model’s marginal likelihood.</a></li>
<li class="chapter" data-level="10.3.2" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#hierarchical-mcmc-computation-of-relative-model-probability-is-not-available-in-brms-well-cover-information-criteria-instead."><i class="fa fa-check"></i><b>10.3.2</b> Hierarchical MCMC computation <del>of relative model probability</del> is not available in brms: We’ll cover information criteria instead.</a></li>
<li class="chapter" data-level="10.3.3" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#models-with-different-noise-distributions-in-jags-brms."><i class="fa fa-check"></i><b>10.3.3</b> Models with different “noise” distributions in <del>JAGS</del> brms.</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#prediction-model-averaging"><i class="fa fa-check"></i><b>10.4</b> Prediction: Model averaging</a></li>
<li class="chapter" data-level="10.5" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#model-complexity-naturally-accounted-for"><i class="fa fa-check"></i><b>10.5</b> Model complexity naturally accounted for</a><ul>
<li class="chapter" data-level="10.5.1" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#caveats-regarding-nested-model-comparison."><i class="fa fa-check"></i><b>10.5.1</b> Caveats regarding nested model comparison.</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#extreme-sensitivity-to-prior-distribution"><i class="fa fa-check"></i><b>10.6</b> Extreme sensitivity to prior distribution</a><ul>
<li class="chapter" data-level="10.6.1" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#priors-of-different-models-should-be-equally-informed."><i class="fa fa-check"></i><b>10.6.1</b> Priors of different models should be equally informed.</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#bonus-theres-danger-ahead"><i class="fa fa-check"></i><b>10.7</b> Bonus: There’s danger ahead</a></li>
<li class="chapter" data-level="" data-path="model-comparison-and-hierarchical-modeling.html"><a href="model-comparison-and-hierarchical-modeling.html#session-info-9"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html"><i class="fa fa-check"></i><b>11</b> Null Hypothesis Significance Testing</a><ul>
<li class="chapter" data-level="11.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#paved-with-good-intentions"><i class="fa fa-check"></i><b>11.1</b> Paved with good intentions</a><ul>
<li class="chapter" data-level="11.1.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#definition-of-p-value."><i class="fa fa-check"></i><b>11.1.1</b> Definition of <span class="math inline">\(p\)</span> value.</a></li>
<li class="chapter" data-level="11.1.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#with-intention-to-fix-n."><i class="fa fa-check"></i><b>11.1.2</b> With intention to fix <span class="math inline">\(N\)</span>.</a></li>
<li class="chapter" data-level="11.1.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#with-intention-to-fix-z."><i class="fa fa-check"></i><b>11.1.3</b> With intention to fix <span class="math inline">\(z\)</span>.</a></li>
<li class="chapter" data-level="11.1.4" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#with-intention-to-fix-duration."><i class="fa fa-check"></i><b>11.1.4</b> With intention to fix duration.</a></li>
<li class="chapter" data-level="11.1.5" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#with-intention-to-make-multiple-tests."><i class="fa fa-check"></i><b>11.1.5</b> With intention to make multiple tests.</a></li>
<li class="chapter" data-level="11.1.6" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#soul-searching."><i class="fa fa-check"></i><b>11.1.6</b> Soul searching.</a></li>
<li class="chapter" data-level="11.1.7" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#bayesian-analysis."><i class="fa fa-check"></i><b>11.1.7</b> Bayesian analysis.</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#prior-knowledge"><i class="fa fa-check"></i><b>11.2</b> Prior knowledge</a><ul>
<li class="chapter" data-level="11.2.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#nhst-analysis."><i class="fa fa-check"></i><b>11.2.1</b> NHST analysis.</a></li>
<li class="chapter" data-level="11.2.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#bayesian-analysis.-1"><i class="fa fa-check"></i><b>11.2.2</b> Bayesian analysis.</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#confidence-interval-and-highest-density-interval"><i class="fa fa-check"></i><b>11.3</b> Confidence interval and highest density interval</a><ul>
<li class="chapter" data-level="11.3.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#ci-depends-on-intention."><i class="fa fa-check"></i><b>11.3.1</b> CI depends on intention.</a></li>
<li class="chapter" data-level="11.3.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#bayesian-hdi."><i class="fa fa-check"></i><b>11.3.2</b> Bayesian HDI.</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#multiple-comparisons"><i class="fa fa-check"></i><b>11.4</b> Multiple comparisons</a><ul>
<li class="chapter" data-level="11.4.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#nhst-correction-for-experiment-wise-error."><i class="fa fa-check"></i><b>11.4.1</b> NHST correction for experiment wise error.</a></li>
<li class="chapter" data-level="11.4.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#just-one-bayesian-posterior-no-matter-how-you-look-at-it."><i class="fa fa-check"></i><b>11.4.2</b> Just one Bayesian posterior no matter how you look at it.</a></li>
<li class="chapter" data-level="11.4.3" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#how-bayesian-analysis-mitigates-false-alarms."><i class="fa fa-check"></i><b>11.4.3</b> How Bayesian analysis mitigates false alarms.</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#what-a-sampling-distribution-is-good-for"><i class="fa fa-check"></i><b>11.5</b> What a sampling distribution is good for</a><ul>
<li class="chapter" data-level="11.5.1" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#planning-an-experiment."><i class="fa fa-check"></i><b>11.5.1</b> Planning an experiment.</a></li>
<li class="chapter" data-level="11.5.2" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#exploring-model-predictions-posterior-predictive-check."><i class="fa fa-check"></i><b>11.5.2</b> Exploring model predictions (posterior predictive check).</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="null-hypothesis-significance-testing.html"><a href="null-hypothesis-significance-testing.html#session-info-10"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><i class="fa fa-check"></i><b>12</b> Bayesian Approaches to Testing a Point (“Null”) Hypothesis</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#the-estimation-approach"><i class="fa fa-check"></i><b>12.1</b> The estimation approach</a><ul>
<li class="chapter" data-level="12.1.1" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#region-of-practical-equivalence."><i class="fa fa-check"></i><b>12.1.1</b> Region of practical equivalence.</a></li>
<li class="chapter" data-level="12.1.2" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#some-examples."><i class="fa fa-check"></i><b>12.1.2</b> Some examples.</a></li>
<li class="chapter" data-level="12.1.3" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#differences-of-correlated-parameters."><i class="fa fa-check"></i><b>12.1.3</b> Differences of correlated parameters.</a></li>
<li class="chapter" data-level="12.1.4" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#why-hdi-and-not-equal-tailed-interval"><i class="fa fa-check"></i><b>12.1.4</b> Why HDI and not equal-tailed interval?</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#the-model-comparison-approach"><i class="fa fa-check"></i><b>12.2</b> The model-comparison approach</a><ul>
<li class="chapter" data-level="12.2.1" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#is-a-coin-fair-or-not"><i class="fa fa-check"></i><b>12.2.1</b> Is a coin fair or not?</a></li>
<li class="chapter" data-level="12.2.2" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#bayes-factor-can-accept-null-with-poor-precision."><i class="fa fa-check"></i><b>12.2.2</b> Bayes’ factor can accept null with poor precision.</a></li>
<li class="chapter" data-level="12.2.3" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#are-different-groups-equal-or-not"><i class="fa fa-check"></i><b>12.2.3</b> Are different groups equal or not?</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#relations-of-parameter-estimation-and-model-comparison"><i class="fa fa-check"></i><b>12.3</b> Relations of parameter estimation and model comparison</a></li>
<li class="chapter" data-level="12.4" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#estimation-and-model-comparison"><i class="fa fa-check"></i><b>12.4</b> Estimation and model comparison?</a></li>
<li class="chapter" data-level="" data-path="bayesian-approaches-to-testing-a-point-null-hypothesis.html"><a href="bayesian-approaches-to-testing-a-point-null-hypothesis.html#session-info-11"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html"><i class="fa fa-check"></i><b>13</b> Goals, Power, and Sample Size</a><ul>
<li class="chapter" data-level="13.1" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#the-will-to-power"><i class="fa fa-check"></i><b>13.1</b> The will to power</a><ul>
<li class="chapter" data-level="13.1.1" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#goals-and-obstacles."><i class="fa fa-check"></i><b>13.1.1</b> Goals and obstacles.</a></li>
<li class="chapter" data-level="13.1.2" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#power."><i class="fa fa-check"></i><b>13.1.2</b> Power.</a></li>
<li class="chapter" data-level="13.1.3" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#sample-size."><i class="fa fa-check"></i><b>13.1.3</b> Sample size.</a></li>
<li class="chapter" data-level="13.1.4" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#other-expressions-of-goals."><i class="fa fa-check"></i><b>13.1.4</b> Other expressions of goals.</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#computing-power-and-sample-size"><i class="fa fa-check"></i><b>13.2</b> Computing power and sample size</a><ul>
<li class="chapter" data-level="13.2.1" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#when-the-goal-is-to-exclude-a-null-value."><i class="fa fa-check"></i><b>13.2.1</b> When the goal is to exclude a null value.</a></li>
<li class="chapter" data-level="13.2.2" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#formal-solution-and-implementation-in-r."><i class="fa fa-check"></i><b>13.2.2</b> Formal solution and implementation in R.</a></li>
<li class="chapter" data-level="13.2.3" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#when-the-goal-is-precision."><i class="fa fa-check"></i><b>13.2.3</b> When the goal is precision.</a></li>
<li class="chapter" data-level="13.2.4" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#monte-carlo-approximation-of-power."><i class="fa fa-check"></i><b>13.2.4</b> Monte Carlo approximation of power.</a></li>
<li class="chapter" data-level="13.2.5" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#power-from-idealized-or-actual-data."><i class="fa fa-check"></i><b>13.2.5</b> Power from idealized or actual data.</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#sequential-testing-and-the-goal-of-precision"><i class="fa fa-check"></i><b>13.3</b> Sequential testing and the goal of precision</a><ul>
<li class="chapter" data-level="13.3.1" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#examples-ofsequential-tests."><i class="fa fa-check"></i><b>13.3.1</b> Examples ofsequential tests.</a></li>
<li class="chapter" data-level="13.3.2" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#average-behavior-of-sequential-tests."><i class="fa fa-check"></i><b>13.3.2</b> Average behavior of sequential tests.</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#discussion"><i class="fa fa-check"></i><b>13.4</b> Discussion</a><ul>
<li class="chapter" data-level="13.4.1" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#power-and-multiple-comparisons."><i class="fa fa-check"></i><b>13.4.1</b> Power and multiple comparisons.</a></li>
<li class="chapter" data-level="13.4.2" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#power-prospective-retrospective-and-replication."><i class="fa fa-check"></i><b>13.4.2</b> Power: prospective, retrospective, and replication.</a></li>
<li class="chapter" data-level="13.4.3" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#power-analysis-requires-verisimilitude-of-simulated-data."><i class="fa fa-check"></i><b>13.4.3</b> Power analysis requires verisimilitude of simulated data.</a></li>
<li class="chapter" data-level="13.4.4" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#the-importance-of-planning."><i class="fa fa-check"></i><b>13.4.4</b> The importance of planning.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="goals-power-and-sample-size.html"><a href="goals-power-and-sample-size.html#session-info-12"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="stan.html"><a href="stan.html"><i class="fa fa-check"></i><b>14</b> Stan</a><ul>
<li class="chapter" data-level="14.1" data-path="stan.html"><a href="stan.html#hmc-sampling"><i class="fa fa-check"></i><b>14.1</b> HMC sampling</a></li>
<li class="chapter" data-level="14.2" data-path="stan.html"><a href="stan.html#installing-stan"><i class="fa fa-check"></i><b>14.2</b> Installing Stan</a></li>
<li class="chapter" data-level="14.3" data-path="stan.html"><a href="stan.html#a-complete-example-1"><i class="fa fa-check"></i><b>14.3</b> A Complete example</a><ul>
<li class="chapter" data-level="14.3.1" data-path="stan.html"><a href="stan.html#reusing-the-compiled-model."><i class="fa fa-check"></i><b>14.3.1</b> Reusing the compiled model.</a></li>
<li class="chapter" data-level="14.3.2" data-path="stan.html"><a href="stan.html#general-structure-of-stan-model-specification."><i class="fa fa-check"></i><b>14.3.2</b> General structure of Stan model specification.</a></li>
<li class="chapter" data-level="14.3.3" data-path="stan.html"><a href="stan.html#think-log-probability-to-think-like-stan."><i class="fa fa-check"></i><b>14.3.3</b> Think log probability to think like Stan.</a></li>
<li class="chapter" data-level="14.3.4" data-path="stan.html"><a href="stan.html#sampling-the-prior-in-stan."><i class="fa fa-check"></i><b>14.3.4</b> Sampling the prior in Stan.</a></li>
<li class="chapter" data-level="14.3.5" data-path="stan.html"><a href="stan.html#simplified-scripts-for-frequently-used-analyses."><i class="fa fa-check"></i><b>14.3.5</b> Simplified scripts for frequently used analyses.</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="stan.html"><a href="stan.html#specify-models-top-down-in-stan"><i class="fa fa-check"></i><b>14.4</b> Specify models top-down in Stan</a></li>
<li class="chapter" data-level="14.5" data-path="stan.html"><a href="stan.html#limitations-and-extras"><i class="fa fa-check"></i><b>14.5</b> Limitations and extras</a></li>
<li class="chapter" data-level="" data-path="stan.html"><a href="stan.html#session-info-13"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html"><i class="fa fa-check"></i><b>15</b> Overview of the Generalized Linear Model</a><ul>
<li class="chapter" data-level="15.1" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#types-of-variables"><i class="fa fa-check"></i><b>15.1</b> Types of variables</a><ul>
<li class="chapter" data-level="15.1.1" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#predictor-and-predicted-variables."><i class="fa fa-check"></i><b>15.1.1</b> Predictor and predicted variables.</a></li>
<li class="chapter" data-level="15.1.2" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#scale-types-metric-ordinal-nominal-and-count."><i class="fa fa-check"></i><b>15.1.2</b> Scale types: metric, ordinal, nominal, and count.</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#linear-combination-of-predictors"><i class="fa fa-check"></i><b>15.2</b> Linear combination of predictors</a><ul>
<li class="chapter" data-level="15.2.1" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#linear-function-of-a-single-metric-predictor."><i class="fa fa-check"></i><b>15.2.1</b> Linear function of a single metric predictor.</a></li>
<li class="chapter" data-level="15.2.2" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#additive-combination-of-metric-predictors."><i class="fa fa-check"></i><b>15.2.2</b> Additive combination of metric predictors.</a></li>
<li class="chapter" data-level="15.2.3" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#nonadditive-interaction-of-metric-predictors."><i class="fa fa-check"></i><b>15.2.3</b> Nonadditive interaction of metric predictors.</a></li>
<li class="chapter" data-level="15.2.4" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#nominal-predictors."><i class="fa fa-check"></i><b>15.2.4</b> Nominal predictors.</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#linking-from-combined-predictors-to-noisy-predicted-data"><i class="fa fa-check"></i><b>15.3</b> Linking from combined predictors to noisy predicted data</a><ul>
<li class="chapter" data-level="15.3.1" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#from-predictors-to-predicted-central-tendency."><i class="fa fa-check"></i><b>15.3.1</b> From predictors to predicted central tendency.</a></li>
<li class="chapter" data-level="15.3.2" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#from-predicted-central-tendency-to-noisy-data."><i class="fa fa-check"></i><b>15.3.2</b> From predicted central tendency to noisy data.</a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#formal-expression-of-the-glm"><i class="fa fa-check"></i><b>15.4</b> Formal expression of the GLM</a><ul>
<li class="chapter" data-level="15.4.1" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#cases-of-the-glm."><i class="fa fa-check"></i><b>15.4.1</b> Cases of the GLM.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="overview-of-the-generalized-linear-model.html"><a href="overview-of-the-generalized-linear-model.html#session-info-14"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html"><i class="fa fa-check"></i><b>16</b> Metric-Predicted Variable on One or Two Groups</a><ul>
<li class="chapter" data-level="16.1" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#estimating-the-mean-and-standard-deviation-of-a-normal-distribution"><i class="fa fa-check"></i><b>16.1</b> Estimating the mean and standard deviation of a normal distribution</a><ul>
<li class="chapter" data-level="16.1.1" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#solution-by-mathematical-analysis-heads-up-on-precision."><i class="fa fa-check"></i><b>16.1.1</b> <del>Solution by mathematical analysis</del> Heads up on precision.</a></li>
<li class="chapter" data-level="16.1.2" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#approximation-by-mcmc-in-jags-hmc-in-brms."><i class="fa fa-check"></i><b>16.1.2</b> Approximation by <del>MCMC in JAGS</del> HMC in brms.</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#outliers-and-robust-estimation-the-t-distribution"><i class="fa fa-check"></i><b>16.2</b> Outliers and robust estimation: The <span class="math inline">\(t\)</span> distribution</a><ul>
<li class="chapter" data-level="16.2.1" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#using-the-t-distribution-in-jags-brms."><i class="fa fa-check"></i><b>16.2.1</b> Using the <span class="math inline">\(t\)</span> distribution in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="16.2.2" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#using-the-t-distribution-in-stan."><i class="fa fa-check"></i><b>16.2.2</b> Using the <span class="math inline">\(t\)</span> distribution in Stan.</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#two-groups"><i class="fa fa-check"></i><b>16.3</b> Two groups</a><ul>
<li class="chapter" data-level="16.3.1" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#analysis-by-nhst."><i class="fa fa-check"></i><b>16.3.1</b> Analysis by NHST.</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#other-noise-distributions-and-transforming-data"><i class="fa fa-check"></i><b>16.4</b> Other noise distributions and transforming data</a></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-on-one-or-two-groups.html"><a href="metric-predicted-variable-on-one-or-two-groups.html#session-info-15"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html"><i class="fa fa-check"></i><b>17</b> Metric Predicted Variable with One Metric Predictor</a><ul>
<li class="chapter" data-level="17.1" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#simple-linear-regression"><i class="fa fa-check"></i><b>17.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="17.2" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#robust-linear-regression"><i class="fa fa-check"></i><b>17.2</b> Robust linear regression</a><ul>
<li class="chapter" data-level="17.2.1" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#robust-linear-regression-in-jags-brms."><i class="fa fa-check"></i><b>17.2.1</b> Robust linear regression in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="17.2.2" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#robust-linear-regression-in-stan."><i class="fa fa-check"></i><b>17.2.2</b> Robust linear regression in Stan.</a></li>
<li class="chapter" data-level="17.2.3" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#stan-or-jags"><i class="fa fa-check"></i><b>17.2.3</b> Stan or JAGS?</a></li>
<li class="chapter" data-level="17.2.4" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#interpreting-the-posterior-distribution."><i class="fa fa-check"></i><b>17.2.4</b> Interpreting the posterior distribution.</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#hierarchical-regression-on-individuals-within-groups"><i class="fa fa-check"></i><b>17.3</b> Hierarchical regression on individuals within groups</a><ul>
<li class="chapter" data-level="17.3.1" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#the-model-and-implementation-in-jags-brms."><i class="fa fa-check"></i><b>17.3.1</b> The model and implementation in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="17.3.2" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#the-posterior-distribution-shrinkage-and-prediction."><i class="fa fa-check"></i><b>17.3.2</b> The posterior distribution: Shrinkage and prediction.</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#quadratic-trend-and-weighted-data"><i class="fa fa-check"></i><b>17.4</b> Quadratic trend and weighted data</a><ul>
<li class="chapter" data-level="17.4.1" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#results-and-interpretation."><i class="fa fa-check"></i><b>17.4.1</b> Results and interpretation.</a></li>
<li class="chapter" data-level="17.4.2" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#further-extensions."><i class="fa fa-check"></i><b>17.4.2</b> Further extensions.</a></li>
</ul></li>
<li class="chapter" data-level="17.5" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#procedure-and-perils-for-expanding-a-model"><i class="fa fa-check"></i><b>17.5</b> Procedure and perils for expanding a model</a></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-with-one-metric-predictor.html"><a href="metric-predicted-variable-with-one-metric-predictor.html#session-info-16"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html"><i class="fa fa-check"></i><b>18</b> Metric Predicted Variable with Multiple Metric Predictors</a><ul>
<li class="chapter" data-level="18.1" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#multiple-linear-regression"><i class="fa fa-check"></i><b>18.1</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="18.1.1" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#the-perils-of-correlated-predictors."><i class="fa fa-check"></i><b>18.1.1</b> The perils of correlated predictors.</a></li>
<li class="chapter" data-level="18.1.2" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#the-model-and-implementation."><i class="fa fa-check"></i><b>18.1.2</b> The model and implementation.</a></li>
<li class="chapter" data-level="18.1.3" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#the-posterior-distribution."><i class="fa fa-check"></i><b>18.1.3</b> The posterior distribution.</a></li>
<li class="chapter" data-level="18.1.4" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#redundant-predictors."><i class="fa fa-check"></i><b>18.1.4</b> Redundant predictors.</a></li>
<li class="chapter" data-level="18.1.5" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#informative-priors-sparse-data-and-correlated-predictors."><i class="fa fa-check"></i><b>18.1.5</b> Informative priors, sparse data, and correlated predictors.</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#multiplicative-interaction-of-metric-predictors"><i class="fa fa-check"></i><b>18.2</b> Multiplicative interaction of metric predictors</a><ul>
<li class="chapter" data-level="18.2.1" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#an-example."><i class="fa fa-check"></i><b>18.2.1</b> An example.</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#shrinkage-of-regression-coefficients"><i class="fa fa-check"></i><b>18.3</b> Shrinkage of regression coefficients</a></li>
<li class="chapter" data-level="18.4" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#variable-selection"><i class="fa fa-check"></i><b>18.4</b> Variable selection</a><ul>
<li class="chapter" data-level="18.4.1" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#inclusion-probability-is-strongly-affected-by-vagueness-of-prior."><i class="fa fa-check"></i><b>18.4.1</b> Inclusion probability is strongly affected by vagueness of prior.</a></li>
<li class="chapter" data-level="18.4.2" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#variable-selection-with-hierarchical-shrinkage."><i class="fa fa-check"></i><b>18.4.2</b> Variable selection with hierarchical shrinkage.</a></li>
<li class="chapter" data-level="18.4.3" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#what-to-report-and-what-to-conclude."><i class="fa fa-check"></i><b>18.4.3</b> What to report and what to conclude.</a></li>
<li class="chapter" data-level="18.4.4" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#caution-computational-methods."><i class="fa fa-check"></i><b>18.4.4</b> Caution: Computational methods.</a></li>
<li class="chapter" data-level="18.4.5" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#caution-interaction-variables."><i class="fa fa-check"></i><b>18.4.5</b> Caution: Interaction variables.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-with-multiple-metric-predictors.html"><a href="metric-predicted-variable-with-multiple-metric-predictors.html#session-info-17"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html"><i class="fa fa-check"></i><b>19</b> Metric Predicted Variable with One Nominal Predictor</a><ul>
<li class="chapter" data-level="19.1" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#describing-multiple-groups-of-metric-data"><i class="fa fa-check"></i><b>19.1</b> Describing multiple groups of metric data</a></li>
<li class="chapter" data-level="19.2" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#traditional-analysis-of-variance"><i class="fa fa-check"></i><b>19.2</b> Traditional analysis of variance</a></li>
<li class="chapter" data-level="19.3" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#hierarchical-bayesian-approach"><i class="fa fa-check"></i><b>19.3</b> Hierarchical Bayesian approach</a><ul>
<li class="chapter" data-level="19.3.1" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#implementation-in-jags-brms."><i class="fa fa-check"></i><b>19.3.1</b> Implementation in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="19.3.2" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#example-sex-and-death."><i class="fa fa-check"></i><b>19.3.2</b> Example: Sex and death.</a></li>
<li class="chapter" data-level="19.3.3" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#contrasts."><i class="fa fa-check"></i><b>19.3.3</b> Contrasts.</a></li>
<li class="chapter" data-level="19.3.4" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#multiple-comparisons-and-shrinkage."><i class="fa fa-check"></i><b>19.3.4</b> Multiple comparisons and shrinkage.</a></li>
<li class="chapter" data-level="19.3.5" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#the-two-group-case."><i class="fa fa-check"></i><b>19.3.5</b> The two-group case.</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#including-a-metric-predictor"><i class="fa fa-check"></i><b>19.4</b> Including a metric predictor</a><ul>
<li class="chapter" data-level="19.4.1" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#example-sex-death-and-size."><i class="fa fa-check"></i><b>19.4.1</b> Example: Sex, death, and size.</a></li>
<li class="chapter" data-level="19.4.2" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#analogous-to-traditional-ancova."><i class="fa fa-check"></i><b>19.4.2</b> Analogous to traditional ANCOVA.</a></li>
<li class="chapter" data-level="19.4.3" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#relation-to-hierarchical-linear-regression."><i class="fa fa-check"></i><b>19.4.3</b> Relation to hierarchical linear regression.</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#heterogeneous-variances-and-robustness-against-outliers"><i class="fa fa-check"></i><b>19.5</b> Heterogeneous variances and robustness against outliers</a><ul>
<li class="chapter" data-level="19.5.1" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#example-contrast-of-means-with-different-variances."><i class="fa fa-check"></i><b>19.5.1</b> Example: Contrast of means with different variances.</a></li>
</ul></li>
<li class="chapter" data-level="19.6" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#exercises-walk-out-an-effect-size"><i class="fa fa-check"></i><b>19.6</b> <del>Exercises</del> Walk out an effect size</a><ul>
<li class="chapter" data-level="19.6.1" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#populations-and-samples."><i class="fa fa-check"></i><b>19.6.1</b> Populations and samples.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-with-one-nominal-predictor.html"><a href="metric-predicted-variable-with-one-nominal-predictor.html#session-info-18"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html"><i class="fa fa-check"></i><b>20</b> Metric Predicted Variable with Multiple Nominal Predictors</a><ul>
<li class="chapter" data-level="20.1" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#describing-groups-of-metric-data-with-multiple-nominal-predictors"><i class="fa fa-check"></i><b>20.1</b> Describing groups of metric data with multiple nominal predictors</a><ul>
<li class="chapter" data-level="20.1.1" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#interaction."><i class="fa fa-check"></i><b>20.1.1</b> Interaction.</a></li>
<li class="chapter" data-level="20.1.2" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#traditional-anova."><i class="fa fa-check"></i><b>20.1.2</b> Traditional ANOVA.</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#hierarchical-bayesian-approach-1"><i class="fa fa-check"></i><b>20.2</b> Hierarchical Bayesian approach</a><ul>
<li class="chapter" data-level="20.2.1" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#implementation-in-jags-brms.-1"><i class="fa fa-check"></i><b>20.2.1</b> Implementation in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="20.2.2" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#example-its-only-money."><i class="fa fa-check"></i><b>20.2.2</b> Example: It’s only money.</a></li>
<li class="chapter" data-level="20.2.3" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#main-effect-contrasts."><i class="fa fa-check"></i><b>20.2.3</b> Main effect contrasts.</a></li>
<li class="chapter" data-level="20.2.4" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#interaction-contrasts-and-simple-effects."><i class="fa fa-check"></i><b>20.2.4</b> Interaction contrasts and simple effects.</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#rescaling-can-change-interactions-homogeneity-and-normality"><i class="fa fa-check"></i><b>20.3</b> Rescaling can change interactions, homogeneity, and normality</a></li>
<li class="chapter" data-level="20.4" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#heterogeneous-variances-and-robustness-against-outliers-1"><i class="fa fa-check"></i><b>20.4</b> Heterogeneous variances and robustness against outliers</a></li>
<li class="chapter" data-level="20.5" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#within-subject-designs"><i class="fa fa-check"></i><b>20.5</b> Within-subject designs</a><ul>
<li class="chapter" data-level="20.5.1" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#why-use-a-within-subject-design-and-why-not"><i class="fa fa-check"></i><b>20.5.1</b> Why use a within-subject design? And why not?</a></li>
<li class="chapter" data-level="20.5.2" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#split-plot-design."><i class="fa fa-check"></i><b>20.5.2</b> Split-plot design.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#session-info-19"><i class="fa fa-check"></i>Session info</a></li>
<li class="chapter" data-level="" data-path="metric-predicted-variable-with-multiple-nominal-predictors.html"><a href="metric-predicted-variable-with-multiple-nominal-predictors.html#footnote"><i class="fa fa-check"></i>Footnote</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html"><i class="fa fa-check"></i><b>21</b> Dichotomous Predicted Variable</a><ul>
<li class="chapter" data-level="21.1" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#multiple-metric-predictors"><i class="fa fa-check"></i><b>21.1</b> Multiple metric predictors</a><ul>
<li class="chapter" data-level="21.1.1" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#the-model-and-implementation-in-jags-brms.-1"><i class="fa fa-check"></i><b>21.1.1</b> The model and implementation in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="21.1.2" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#example-height-weight-and-gender."><i class="fa fa-check"></i><b>21.1.2</b> Example: Height, weight, and gender.</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#interpreting-the-regression-coefficients"><i class="fa fa-check"></i><b>21.2</b> Interpreting the regression coefficients</a><ul>
<li class="chapter" data-level="21.2.1" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#log-odds."><i class="fa fa-check"></i><b>21.2.1</b> Log odds.</a></li>
<li class="chapter" data-level="21.2.2" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#when-there-are-few-1s-or-0s-in-the-data."><i class="fa fa-check"></i><b>21.2.2</b> When there are few 1’s or 0’s in the data.</a></li>
<li class="chapter" data-level="21.2.3" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#correlated-predictors."><i class="fa fa-check"></i><b>21.2.3</b> Correlated predictors.</a></li>
<li class="chapter" data-level="21.2.4" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#interaction-of-metric-predictors."><i class="fa fa-check"></i><b>21.2.4</b> Interaction of metric predictors.</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#robust-logistic-regression"><i class="fa fa-check"></i><b>21.3</b> Robust logistic regression</a></li>
<li class="chapter" data-level="21.4" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#nominal-predictors"><i class="fa fa-check"></i><b>21.4</b> Nominal predictors</a><ul>
<li class="chapter" data-level="21.4.1" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#single-group."><i class="fa fa-check"></i><b>21.4.1</b> Single group.</a></li>
<li class="chapter" data-level="21.4.2" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#multiple-groups."><i class="fa fa-check"></i><b>21.4.2</b> Multiple groups.</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="dichotomous-predicted-variable.html"><a href="dichotomous-predicted-variable.html#session-info-20"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html"><i class="fa fa-check"></i><b>22</b> Nominal Predicted Variable</a><ul>
<li class="chapter" data-level="22.1" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#softmax-regression"><i class="fa fa-check"></i><b>22.1</b> Softmax regression</a><ul>
<li class="chapter" data-level="22.1.1" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#softmax-reduces-to-logistic-for-two-outcomes."><i class="fa fa-check"></i><b>22.1.1</b> Softmax reduces to logistic for two outcomes.</a></li>
<li class="chapter" data-level="22.1.2" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#independence-from-irrelevant-attributes."><i class="fa fa-check"></i><b>22.1.2</b> Independence from irrelevant attributes.</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#conditional-logistic-regression"><i class="fa fa-check"></i><b>22.2</b> Conditional logistic regression</a></li>
<li class="chapter" data-level="22.3" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#implementation-in-jags-brms"><i class="fa fa-check"></i><b>22.3</b> Implementation in <del>JAGS</del> brms</a><ul>
<li class="chapter" data-level="22.3.1" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#softmax-model."><i class="fa fa-check"></i><b>22.3.1</b> Softmax model.</a></li>
<li class="chapter" data-level="22.3.2" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#conditional-logistic-model."><i class="fa fa-check"></i><b>22.3.2</b> Conditional logistic model.</a></li>
<li class="chapter" data-level="22.3.3" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#results-interpreting-the-regression-coefficients."><i class="fa fa-check"></i><b>22.3.3</b> Results: Interpreting the regression coefficients.</a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#generalizations-and-variations-of-the-models"><i class="fa fa-check"></i><b>22.4</b> Generalizations and variations of the models</a></li>
<li class="chapter" data-level="" data-path="nominal-predicted-variable.html"><a href="nominal-predicted-variable.html#session-info-21"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html"><i class="fa fa-check"></i><b>23</b> Ordinal Predicted Variable</a><ul>
<li class="chapter" data-level="23.1" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#modeling-ordinal-data-with-an-underlying-metric-variable"><i class="fa fa-check"></i><b>23.1</b> Modeling ordinal data with an underlying metric variable</a></li>
<li class="chapter" data-level="23.2" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#the-case-of-a-single-group"><i class="fa fa-check"></i><b>23.2</b> The case of a single group</a><ul>
<li class="chapter" data-level="23.2.1" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#implementation-in-jags-brms.-3"><i class="fa fa-check"></i><b>23.2.1</b> Implementation in <del>JAGS</del> <strong>brms</strong>.</a></li>
<li class="chapter" data-level="23.2.2" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#examples-bayesian-estimation-recovers-true-parameter-values."><i class="fa fa-check"></i><b>23.2.2</b> Examples: Bayesian estimation recovers true parameter values.</a></li>
</ul></li>
<li class="chapter" data-level="23.3" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#the-case-of-two-groups"><i class="fa fa-check"></i><b>23.3</b> The case of two groups</a><ul>
<li class="chapter" data-level="23.3.1" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#implementation-in-jags-brms.-4"><i class="fa fa-check"></i><b>23.3.1</b> Implementation in <del>JAGS</del> <strong>brms</strong>.</a></li>
<li class="chapter" data-level="23.3.2" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#examples-not-funny."><i class="fa fa-check"></i><b>23.3.2</b> Examples: Not funny.</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#the-case-of-metric-predictors"><i class="fa fa-check"></i><b>23.4</b> The Case of metric predictors</a><ul>
<li class="chapter" data-level="23.4.1" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#implementation-in-jags-brms.-5"><i class="fa fa-check"></i><b>23.4.1</b> Implementation in <del>JAGS</del> brms.</a></li>
<li class="chapter" data-level="23.4.2" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#example-happiness-and-money."><i class="fa fa-check"></i><b>23.4.2</b> Example: Happiness and money.</a></li>
<li class="chapter" data-level="23.4.3" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#example-moviesthey-dont-make-em-like-they-used-to."><i class="fa fa-check"></i><b>23.4.3</b> Example: Movies–They don’t make ’em like they used to.</a></li>
<li class="chapter" data-level="23.4.4" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#why-are-some-thresholds-outside-the-data"><i class="fa fa-check"></i><b>23.4.4</b> Why are some thresholds outside the data?</a></li>
</ul></li>
<li class="chapter" data-level="23.5" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#posterior-prediction"><i class="fa fa-check"></i><b>23.5</b> Posterior prediction</a></li>
<li class="chapter" data-level="23.6" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#generalizations-and-extensions"><i class="fa fa-check"></i><b>23.6</b> Generalizations and extensions</a></li>
<li class="chapter" data-level="" data-path="ordinal-predicted-variable.html"><a href="ordinal-predicted-variable.html#session-info-22"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html"><i class="fa fa-check"></i><b>24</b> Count Predicted Variable</a><ul>
<li class="chapter" data-level="24.1" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#poisson-exponential-model"><i class="fa fa-check"></i><b>24.1</b> Poisson exponential model</a><ul>
<li class="chapter" data-level="24.1.1" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#data-structure."><i class="fa fa-check"></i><b>24.1.1</b> Data structure.</a></li>
<li class="chapter" data-level="24.1.2" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#exponential-link-function."><i class="fa fa-check"></i><b>24.1.2</b> Exponential link function.</a></li>
<li class="chapter" data-level="24.1.3" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#poisson-noise-distribution."><i class="fa fa-check"></i><b>24.1.3</b> Poisson noise distribution.</a></li>
<li class="chapter" data-level="24.1.4" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#the-complete-model-and-implementation-in-jags-brms."><i class="fa fa-check"></i><b>24.1.4</b> The complete model and implementation in <del>JAGS</del> brms.</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#example-hair-eye-go-again"><i class="fa fa-check"></i><b>24.2</b> Example: Hair eye go again</a></li>
<li class="chapter" data-level="24.3" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#example-interaction-contrasts-shrinkage-and-omnibus-test"><i class="fa fa-check"></i><b>24.3</b> Example: Interaction contrasts, shrinkage, and omnibus test</a></li>
<li class="chapter" data-level="24.4" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#log-linear-models-for-contingency-tables-bonus-alternative-parameterization"><i class="fa fa-check"></i><b>24.4</b> <del>Log-linear models for contingency tables</del> Bonus: Alternative parameterization</a></li>
<li class="chapter" data-level="" data-path="count-predicted-variable.html"><a href="count-predicted-variable.html#session-info-23"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html"><i class="fa fa-check"></i><b>25</b> Tools in the Trunk</a><ul>
<li class="chapter" data-level="25.1" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#reporting-a-bayesian-analysis"><i class="fa fa-check"></i><b>25.1</b> Reporting a Bayesian analysis</a><ul>
<li class="chapter" data-level="25.1.1" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#essential-points."><i class="fa fa-check"></i><b>25.1.1</b> Essential points.</a></li>
<li class="chapter" data-level="25.1.2" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#optional-points."><i class="fa fa-check"></i><b>25.1.2</b> Optional points.</a></li>
<li class="chapter" data-level="25.1.3" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#helpful-points."><i class="fa fa-check"></i><b>25.1.3</b> Helpful points.</a></li>
</ul></li>
<li class="chapter" data-level="25.2" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#functions-for-computing-highest-density-intervals"><i class="fa fa-check"></i><b>25.2</b> Functions for computing highest density intervals</a><ul>
<li class="chapter" data-level="25.2.1" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#r-code-for-computing-hdi-of-a-grid-approximation."><i class="fa fa-check"></i><b>25.2.1</b> R code for computing HDI of a grid approximation.</a></li>
<li class="chapter" data-level="25.2.2" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#hdi-of-unimodal-distribution-is-shortest-interval."><i class="fa fa-check"></i><b>25.2.2</b> HDI of unimodal distribution is shortest interval.</a></li>
<li class="chapter" data-level="25.2.3" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#r-code-for-computing-hdi-of-a-mcmc-sample."><i class="fa fa-check"></i><b>25.2.3</b> R code for computing HDI of a MCMC sample.</a></li>
<li class="chapter" data-level="25.2.4" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#r-code-for-computing-hdi-of-a-function."><i class="fa fa-check"></i><b>25.2.4</b> R code for computing HDI of a function.</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#reparameterization"><i class="fa fa-check"></i><b>25.3</b> Reparameterization</a></li>
<li class="chapter" data-level="25.4" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#censored-data-in-jags-brms"><i class="fa fa-check"></i><b>25.4</b> Censored Data in <del>JAGS</del> brms</a></li>
<li class="chapter" data-level="25.5" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#what-next"><i class="fa fa-check"></i><b>25.5</b> What Next?</a></li>
<li class="chapter" data-level="" data-path="tools-in-the-trunk.html"><a href="tools-in-the-trunk.html#session-info-24"><i class="fa fa-check"></i>Session info</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"><em>Doing Bayesian Data Analysis</em> in brms and the tidyverse</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="markov-chain-monte-carlo" class="section level1">
<h1><span class="header-section-number">7</span> Markov Chain Monte Carlo</h1>
<blockquote>
<p>This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications. The class of methods is called Markov chain Monte Carlo (MCMC), for reasons that will be explained later in the chapter. It is MCMC algorithms and software, along with fast computer hardware, that allow us to do Bayesian data analysis for realistic applications that would have been effectively impossible 30 years ago. <span class="citation">(Kruschke, <a href="#ref-kruschkeDoingBayesianData2015" role="doc-biblioref">2015</a>, p. 144)</span></p>
</blockquote>
<p>Statistician David Draper covered some of the history of MCMC in his lecture, <a href="https://www.youtube.com/watch?v=072Q18nX91I&amp;frags=pl%2Cwn"><em>Bayesian Statistical Reasoning</em></a>.</p>
<div id="approximating-a-distribution-with-a-large-sample" class="section level2">
<h2><span class="header-section-number">7.1</span> Approximating a distribution with a large sample</h2>
<blockquote>
<p>The concept of representing a distribution by a large representative sample is foundational for the approach we take to Bayesian analysis of complex models. The idea is applied intuitively and routinely in everyday life and in science. For example, polls and surveys are founded on this concept: By randomly sampling a subset of people from a population, we estimate the underlying tendencies in the entire population. The larger the sample, the better the estimation. What is new in the present application is that the population from which we are sampling is a mathematically defined distribution, such as a posterior probability distribution. (p. 145)</p>
</blockquote>
<p>Like in Chapters 4 and 6, we need to define our <code>hdi_of_icdf()</code> function.</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="markov-chain-monte-carlo.html#cb469-1"></a>hdi_of_icdf &lt;-<span class="st"> </span><span class="cf">function</span>(name, <span class="dt">width =</span> <span class="fl">.95</span>, <span class="dt">tol =</span> <span class="fl">1e-8</span>, ... ) {</span>
<span id="cb469-2"><a href="markov-chain-monte-carlo.html#cb469-2"></a>  </span>
<span id="cb469-3"><a href="markov-chain-monte-carlo.html#cb469-3"></a>  incredible_mass &lt;-<span class="st"> </span><span class="fl">1.0</span> <span class="op">-</span><span class="st"> </span>width</span>
<span id="cb469-4"><a href="markov-chain-monte-carlo.html#cb469-4"></a>  interval_width &lt;-<span class="st"> </span><span class="cf">function</span>(low_tail_prob, name, width, ...) {</span>
<span id="cb469-5"><a href="markov-chain-monte-carlo.html#cb469-5"></a>    <span class="kw">name</span>(width <span class="op">+</span><span class="st"> </span>low_tail_prob, ...) <span class="op">-</span><span class="st"> </span><span class="kw">name</span>(low_tail_prob, ...)</span>
<span id="cb469-6"><a href="markov-chain-monte-carlo.html#cb469-6"></a>  }</span>
<span id="cb469-7"><a href="markov-chain-monte-carlo.html#cb469-7"></a>  opt_info &lt;-<span class="st"> </span><span class="kw">optimize</span>(interval_width, <span class="kw">c</span>(<span class="dv">0</span>, incredible_mass), </span>
<span id="cb469-8"><a href="markov-chain-monte-carlo.html#cb469-8"></a>                       <span class="dt">name =</span> name, <span class="dt">width =</span> width, </span>
<span id="cb469-9"><a href="markov-chain-monte-carlo.html#cb469-9"></a>                       <span class="dt">tol =</span> tol, ...)</span>
<span id="cb469-10"><a href="markov-chain-monte-carlo.html#cb469-10"></a>  hdi_lower_tail_prob &lt;-<span class="st"> </span>opt_info<span class="op">$</span>minimum</span>
<span id="cb469-11"><a href="markov-chain-monte-carlo.html#cb469-11"></a>  <span class="kw">return</span>(<span class="kw">c</span>(<span class="kw">name</span>(hdi_lower_tail_prob, ...),</span>
<span id="cb469-12"><a href="markov-chain-monte-carlo.html#cb469-12"></a>           <span class="kw">name</span>(width <span class="op">+</span><span class="st"> </span>hdi_lower_tail_prob, ...)))</span>
<span id="cb469-13"><a href="markov-chain-monte-carlo.html#cb469-13"></a>  </span>
<span id="cb469-14"><a href="markov-chain-monte-carlo.html#cb469-14"></a>}</span></code></pre></div>
<p>Our <code>hdi_of_icdf()</code> function will compute the analytic 95% highest density intervals (HDIs) for the distribution under consideration in Figure 7.1, <span class="math inline">\(\operatorname{beta}(\theta | 15, 7)\)</span>.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="markov-chain-monte-carlo.html#cb470-1"></a>h &lt;-</span>
<span id="cb470-2"><a href="markov-chain-monte-carlo.html#cb470-2"></a><span class="st">  </span><span class="kw">hdi_of_icdf</span>(<span class="dt">name =</span> qbeta,</span>
<span id="cb470-3"><a href="markov-chain-monte-carlo.html#cb470-3"></a>              <span class="dt">shape1 =</span> <span class="dv">15</span>,</span>
<span id="cb470-4"><a href="markov-chain-monte-carlo.html#cb470-4"></a>              <span class="dt">shape2 =</span> <span class="dv">7</span>)</span>
<span id="cb470-5"><a href="markov-chain-monte-carlo.html#cb470-5"></a></span>
<span id="cb470-6"><a href="markov-chain-monte-carlo.html#cb470-6"></a>h</span></code></pre></div>
<pre><code>## [1] 0.4907001 0.8639305</code></pre>
<p>Using an equation from <a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html#specifying-a-beta-prior.">Chapter 6</a>, <span class="math inline">\(\omega = (a − 1) / (a + b − 2)\)</span>, we can compute the corresponding mode.</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="markov-chain-monte-carlo.html#cb472-1"></a>(omega &lt;-<span class="st"> </span>(<span class="dv">15</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">15</span> <span class="op">+</span><span class="st"> </span><span class="dv">7</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 0.7</code></pre>
<p>To get the density in the upper left panel of Figure 7.1, we’ll make use of the <code>dbeta()</code> function and of our <code>h[1:2]</code> and <code>omega</code> values.</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="markov-chain-monte-carlo.html#cb474-1"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb474-2"><a href="markov-chain-monte-carlo.html#cb474-2"></a><span class="kw">library</span>(cowplot)</span>
<span id="cb474-3"><a href="markov-chain-monte-carlo.html#cb474-3"></a></span>
<span id="cb474-4"><a href="markov-chain-monte-carlo.html#cb474-4"></a><span class="kw">tibble</span>(<span class="dt">theta =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">length.out =</span> <span class="dv">100</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb474-5"><a href="markov-chain-monte-carlo.html#cb474-5"></a><span class="st">  </span></span>
<span id="cb474-6"><a href="markov-chain-monte-carlo.html#cb474-6"></a><span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span></span>
<span id="cb474-7"><a href="markov-chain-monte-carlo.html#cb474-7"></a><span class="st">  </span><span class="kw">geom_ribbon</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">ymin =</span> <span class="dv">0</span>, <span class="dt">ymax =</span> <span class="kw">dbeta</span>(theta, <span class="dt">shape1 =</span> <span class="dv">15</span>, <span class="dt">shape2 =</span> <span class="dv">7</span>)),</span>
<span id="cb474-8"><a href="markov-chain-monte-carlo.html#cb474-8"></a>              <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb474-9"><a href="markov-chain-monte-carlo.html#cb474-9"></a><span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">x =</span> h[<span class="dv">1</span>], <span class="dt">xend =</span> h[<span class="dv">2</span>], <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">yend =</span> <span class="dv">0</span>),</span>
<span id="cb474-10"><a href="markov-chain-monte-carlo.html#cb474-10"></a>               <span class="dt">size =</span> <span class="fl">.75</span>) <span class="op">+</span></span>
<span id="cb474-11"><a href="markov-chain-monte-carlo.html#cb474-11"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x =</span> omega, <span class="dt">y =</span> <span class="dv">0</span>),</span>
<span id="cb474-12"><a href="markov-chain-monte-carlo.html#cb474-12"></a>             <span class="dt">size =</span> <span class="fl">1.5</span>, <span class="dt">shape =</span> <span class="dv">19</span>) <span class="op">+</span></span>
<span id="cb474-13"><a href="markov-chain-monte-carlo.html#cb474-13"></a><span class="st">  </span><span class="kw">annotate</span>(<span class="dt">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="dt">x =</span> <span class="fl">.675</span>, <span class="dt">y =</span> <span class="fl">.4</span>, </span>
<span id="cb474-14"><a href="markov-chain-monte-carlo.html#cb474-14"></a>           <span class="dt">label =</span> <span class="st">&quot;95% HDI&quot;</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb474-15"><a href="markov-chain-monte-carlo.html#cb474-15"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), </span>
<span id="cb474-16"><a href="markov-chain-monte-carlo.html#cb474-16"></a>                     <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>, h, omega, <span class="dv">1</span>),</span>
<span id="cb474-17"><a href="markov-chain-monte-carlo.html#cb474-17"></a>                     <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;0&quot;</span>, h <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">2</span>), omega, <span class="st">&quot;1&quot;</span>)) <span class="op">+</span></span>
<span id="cb474-18"><a href="markov-chain-monte-carlo.html#cb474-18"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Exact distribution&quot;</span>) <span class="op">+</span></span>
<span id="cb474-19"><a href="markov-chain-monte-carlo.html#cb474-19"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="kw">expression</span>(<span class="kw">p</span>(theta))) <span class="op">+</span></span>
<span id="cb474-20"><a href="markov-chain-monte-carlo.html#cb474-20"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-6-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Note how we’re continuing to use <code>theme_cowplot()</code>, which we introduced in the last chapter. The remaining panels in Figure 7.1 require we simulate the data.</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="markov-chain-monte-carlo.html#cb475-1"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb475-2"><a href="markov-chain-monte-carlo.html#cb475-2"></a></span>
<span id="cb475-3"><a href="markov-chain-monte-carlo.html#cb475-3"></a>d &lt;-</span>
<span id="cb475-4"><a href="markov-chain-monte-carlo.html#cb475-4"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">n =</span> <span class="kw">c</span>(<span class="dv">500</span>, <span class="dv">5000</span>, <span class="dv">50000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb475-5"><a href="markov-chain-monte-carlo.html#cb475-5"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">theta =</span> <span class="kw">map</span>(n, <span class="op">~</span><span class="kw">rbeta</span>(., <span class="dt">shape1 =</span> <span class="dv">15</span>, <span class="dt">shape2 =</span> <span class="dv">7</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb475-6"><a href="markov-chain-monte-carlo.html#cb475-6"></a><span class="st">  </span><span class="kw">unnest</span>(theta) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb475-7"><a href="markov-chain-monte-carlo.html#cb475-7"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">key =</span> <span class="kw">str_c</span>(<span class="st">&quot;Sample N = &quot;</span>, n))</span>
<span id="cb475-8"><a href="markov-chain-monte-carlo.html#cb475-8"></a></span>
<span id="cb475-9"><a href="markov-chain-monte-carlo.html#cb475-9"></a><span class="kw">head</span>(d)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##       n theta key           
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         
## 1   500 0.806 Sample N = 500
## 2   500 0.756 Sample N = 500
## 3   500 0.727 Sample N = 500
## 4   500 0.784 Sample N = 500
## 5   500 0.782 Sample N = 500
## 6   500 0.590 Sample N = 500</code></pre>
<p>With the data in hand, we’re ready to plot the remaining panels for Figure 7.1. This time, we’ll use the handy <code>stat_pointinterval()</code> function from the <a href="https://github.com/mjskay/tidybayes"><strong>tidybayes</strong> package</a> to mark off the mode and 95% HDIs.</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="markov-chain-monte-carlo.html#cb477-1"></a><span class="kw">library</span>(tidybayes)</span>
<span id="cb477-2"><a href="markov-chain-monte-carlo.html#cb477-2"></a></span>
<span id="cb477-3"><a href="markov-chain-monte-carlo.html#cb477-3"></a>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb477-4"><a href="markov-chain-monte-carlo.html#cb477-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb477-5"><a href="markov-chain-monte-carlo.html#cb477-5"></a><span class="st">  </span><span class="kw">stat_histinterval</span>(<span class="dt">point_interval =</span> mode_hdi, <span class="dt">.width =</span> <span class="fl">.95</span>, <span class="dt">breaks =</span> <span class="dv">30</span>) <span class="op">+</span></span>
<span id="cb477-6"><a href="markov-chain-monte-carlo.html#cb477-6"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb477-7"><a href="markov-chain-monte-carlo.html#cb477-7"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb477-8"><a href="markov-chain-monte-carlo.html#cb477-8"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb477-9"><a href="markov-chain-monte-carlo.html#cb477-9"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>key, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">scales =</span> <span class="st">&quot;free&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-8-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>If we want the exact values for the mode and 95% HDIs, we can use the <code>tidybayes::mode_hdi()</code> function.</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="markov-chain-monte-carlo.html#cb478-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb478-2"><a href="markov-chain-monte-carlo.html#cb478-2"></a><span class="st">  </span><span class="kw">group_by</span>(key) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb478-3"><a href="markov-chain-monte-carlo.html#cb478-3"></a><span class="st">  </span><span class="kw">mode_hdi</span>(theta)</span></code></pre></div>
<pre><code>## # A tibble: 3 x 7
##   key              theta .lower .upper .width .point .interval
##   &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;    
## 1 Sample N = 500   0.695  0.511  0.868   0.95 mode   hdi      
## 2 Sample N = 5000  0.688  0.497  0.870   0.95 mode   hdi      
## 3 Sample N = 50000 0.711  0.490  0.863   0.95 mode   hdi</code></pre>
<p>If you wanted a better sense of the phenomena, you could do a simulation. We’ll make a custom simulation function to compute the modes from many random draws from our <span class="math inline">\(\operatorname{beta}(\theta | 15, 7)\)</span> distribution, with varying <span class="math inline">\(N\)</span> values.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="markov-chain-monte-carlo.html#cb480-1"></a>my_mode_simulation &lt;-<span class="st"> </span><span class="cf">function</span>(seed) {</span>
<span id="cb480-2"><a href="markov-chain-monte-carlo.html#cb480-2"></a>  </span>
<span id="cb480-3"><a href="markov-chain-monte-carlo.html#cb480-3"></a>  <span class="kw">set.seed</span>(seed)</span>
<span id="cb480-4"><a href="markov-chain-monte-carlo.html#cb480-4"></a>  </span>
<span id="cb480-5"><a href="markov-chain-monte-carlo.html#cb480-5"></a>  <span class="kw">tibble</span>(<span class="dt">n =</span> <span class="kw">c</span>(<span class="dv">500</span>, <span class="dv">5000</span>, <span class="dv">50000</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb480-6"><a href="markov-chain-monte-carlo.html#cb480-6"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">theta =</span> <span class="kw">map</span>(n, <span class="op">~</span><span class="kw">rbeta</span>(., <span class="dt">shape1 =</span> <span class="dv">15</span>, <span class="dt">shape2 =</span> <span class="dv">7</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb480-7"><a href="markov-chain-monte-carlo.html#cb480-7"></a><span class="st">    </span><span class="kw">unnest</span>(theta) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb480-8"><a href="markov-chain-monte-carlo.html#cb480-8"></a><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">key =</span> <span class="kw">str_c</span>(<span class="st">&quot;Sample N = &quot;</span>, n)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb480-9"><a href="markov-chain-monte-carlo.html#cb480-9"></a><span class="st">    </span><span class="kw">group_by</span>(key) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb480-10"><a href="markov-chain-monte-carlo.html#cb480-10"></a><span class="st">    </span><span class="kw">mode_hdi</span>(theta)</span>
<span id="cb480-11"><a href="markov-chain-monte-carlo.html#cb480-11"></a>  </span>
<span id="cb480-12"><a href="markov-chain-monte-carlo.html#cb480-12"></a>}</span></code></pre></div>
<p>Here we put our <code>my_mode_simulation()</code> function to work.</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="markov-chain-monte-carlo.html#cb481-1"></a><span class="co"># we need an index of the values we set our seed with in our `my_mode_simulation()` function</span></span>
<span id="cb481-2"><a href="markov-chain-monte-carlo.html#cb481-2"></a>sim &lt;-</span>
<span id="cb481-3"><a href="markov-chain-monte-carlo.html#cb481-3"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">seed =</span> <span class="dv">1</span><span class="op">:</span><span class="fl">1e3</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb481-4"><a href="markov-chain-monte-carlo.html#cb481-4"></a><span class="st">  </span><span class="kw">group_by</span>(seed) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb481-5"><a href="markov-chain-monte-carlo.html#cb481-5"></a><span class="st">  </span><span class="co"># inserting our subsamples</span></span>
<span id="cb481-6"><a href="markov-chain-monte-carlo.html#cb481-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">modes =</span> <span class="kw">map</span>(seed, my_mode_simulation)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb481-7"><a href="markov-chain-monte-carlo.html#cb481-7"></a><span class="st">   </span><span class="co"># unnesting allows us to access our model results</span></span>
<span id="cb481-8"><a href="markov-chain-monte-carlo.html#cb481-8"></a><span class="st">  </span><span class="kw">unnest</span>(modes) </span>
<span id="cb481-9"><a href="markov-chain-monte-carlo.html#cb481-9"></a></span>
<span id="cb481-10"><a href="markov-chain-monte-carlo.html#cb481-10"></a>sim <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb481-11"><a href="markov-chain-monte-carlo.html#cb481-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> key)) <span class="op">+</span></span>
<span id="cb481-12"><a href="markov-chain-monte-carlo.html#cb481-12"></a><span class="st">  </span><span class="kw">geom_vline</span>(<span class="dt">xintercept =</span> <span class="fl">.7</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb481-13"><a href="markov-chain-monte-carlo.html#cb481-13"></a><span class="st">  </span><span class="kw">stat_histinterval</span>(<span class="dt">.width =</span> <span class="kw">c</span>(.<span class="dv">5</span>, <span class="fl">.95</span>), <span class="dt">breaks =</span> <span class="dv">20</span>, <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb481-14"><a href="markov-chain-monte-carlo.html#cb481-14"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">expression</span>(<span class="st">&quot;Variability of the mode for simulations of &quot;</span><span class="op">*</span><span class="kw">beta</span>(theta<span class="op">*</span><span class="st">&#39;|&#39;</span><span class="op">*</span><span class="dv">15</span><span class="op">*</span><span class="st">&#39;, &#39;</span><span class="op">*</span><span class="dv">7</span>)<span class="op">*</span><span class="st">&quot;, the true mode of which is .7&quot;</span>),</span>
<span id="cb481-15"><a href="markov-chain-monte-carlo.html#cb481-15"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;For each sample size, the dot is the median, the inner thick line is the percentile-based 50% interval,</span><span class="ch">\n</span><span class="st">and the outer thin line the percentile-based 95% interval. Although the central tendency</span><span class="ch">\n</span><span class="st">approximates the true value for all three conditions, the variability of the mode estimate is inversely</span><span class="ch">\n</span><span class="st">related to the sample size.&quot;</span>,</span>
<span id="cb481-16"><a href="markov-chain-monte-carlo.html#cb481-16"></a>       <span class="dt">x =</span> <span class="st">&quot;mode&quot;</span>, </span>
<span id="cb481-17"><a href="markov-chain-monte-carlo.html#cb481-17"></a>       <span class="dt">y =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb481-18"><a href="markov-chain-monte-carlo.html#cb481-18"></a><span class="st">  </span><span class="kw">coord_cartesian</span>(<span class="dt">xlim =</span> <span class="kw">c</span>(.<span class="dv">6</span>, <span class="fl">.8</span>),</span>
<span id="cb481-19"><a href="markov-chain-monte-carlo.html#cb481-19"></a>                  <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="fl">1.25</span>, <span class="fl">3.5</span>)) <span class="op">+</span></span>
<span id="cb481-20"><a href="markov-chain-monte-carlo.html#cb481-20"></a><span class="st">  </span><span class="kw">theme_cowplot</span>(<span class="dt">font_size =</span> <span class="fl">11.5</span>) <span class="op">+</span></span>
<span id="cb481-21"><a href="markov-chain-monte-carlo.html#cb481-21"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.y =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="dv">0</span>),</span>
<span id="cb481-22"><a href="markov-chain-monte-carlo.html#cb481-22"></a>        <span class="dt">axis.ticks.y =</span> <span class="kw">element_blank</span>())</span></code></pre></div>
<p><img src="07_files/figure-gfm/sim-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
<div id="a-simple-case-of-the-metropolis-algorithm" class="section level2">
<h2><span class="header-section-number">7.2</span> A simple case of the Metropolis algorithm</h2>
<blockquote>
<p>Our goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution? (p. 146).</p>
</blockquote>
<p>The answer, my friends, is MCMC.</p>
<div id="a-politician-stumbles-upon-the-metropolis-algorithm." class="section level3">
<h3><span class="header-section-number">7.2.1</span> A politician stumbles upon the Metropolis algorithm.</h3>
<p>I’m not going to walk out Kruschke’s politician example in any detail, here. But if we denote <span class="math inline">\(P_\text{proposed}\)</span> as the population of the proposed island and <span class="math inline">\(P_\text{current}\)</span> as the population of the current island, then</p>
<p><span class="math display">\[p_\text{move} = \frac{P_\text{proposed}}{P_\text{current}}.\]</span></p>
<p>“What’s amazing about this heuristic is that it works: In the long run, the probability that the politician is on any one of the islands exactly matches the relative population of the island” (p. 147)!</p>
</div>
<div id="a-random-walk." class="section level3">
<h3><span class="header-section-number">7.2.2</span> A random walk.</h3>
<p>The code below will allow us to reproduce Kruschke’s random walk. To give credit where it’s due, this is a mild amendment to the code from Chapter 8 of McElreath’s <span class="citation">(<a href="#ref-mcelreathStatisticalRethinkingBayesian2015" role="doc-biblioref">2015</a>)</span> text, <a href="https://xcelab.net/rm/statistical-rethinking/"><em>Statistical rethinking: A Bayesian course with examples in R and Stan</em></a>.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="markov-chain-monte-carlo.html#cb482-1"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb482-2"><a href="markov-chain-monte-carlo.html#cb482-2"></a></span>
<span id="cb482-3"><a href="markov-chain-monte-carlo.html#cb482-3"></a>num_days  &lt;-<span class="st"> </span><span class="fl">5e4</span></span>
<span id="cb482-4"><a href="markov-chain-monte-carlo.html#cb482-4"></a>positions &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, num_days)</span>
<span id="cb482-5"><a href="markov-chain-monte-carlo.html#cb482-5"></a>current   &lt;-<span class="st"> </span><span class="dv">4</span></span>
<span id="cb482-6"><a href="markov-chain-monte-carlo.html#cb482-6"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>num_days) {</span>
<span id="cb482-7"><a href="markov-chain-monte-carlo.html#cb482-7"></a>  <span class="co"># record current position</span></span>
<span id="cb482-8"><a href="markov-chain-monte-carlo.html#cb482-8"></a>  positions[i] &lt;-<span class="st"> </span>current</span>
<span id="cb482-9"><a href="markov-chain-monte-carlo.html#cb482-9"></a>  <span class="co"># flip coin to generate proposal</span></span>
<span id="cb482-10"><a href="markov-chain-monte-carlo.html#cb482-10"></a>  proposal &lt;-<span class="st"> </span>current <span class="op">+</span><span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb482-11"><a href="markov-chain-monte-carlo.html#cb482-11"></a>  <span class="co"># now make sure he loops around from 7 back to 1</span></span>
<span id="cb482-12"><a href="markov-chain-monte-carlo.html#cb482-12"></a>  <span class="cf">if</span> (proposal <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) proposal &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb482-13"><a href="markov-chain-monte-carlo.html#cb482-13"></a>  <span class="cf">if</span> (proposal <span class="op">&gt;</span><span class="st"> </span><span class="dv">7</span>) proposal &lt;-<span class="st"> </span><span class="dv">1</span></span>
<span id="cb482-14"><a href="markov-chain-monte-carlo.html#cb482-14"></a>  <span class="co"># move?</span></span>
<span id="cb482-15"><a href="markov-chain-monte-carlo.html#cb482-15"></a>  prob_accept_the_proposal &lt;-<span class="st"> </span>proposal<span class="op">/</span>current</span>
<span id="cb482-16"><a href="markov-chain-monte-carlo.html#cb482-16"></a>  current &lt;-<span class="st"> </span><span class="kw">ifelse</span>(<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob_accept_the_proposal, proposal, current)</span>
<span id="cb482-17"><a href="markov-chain-monte-carlo.html#cb482-17"></a>}</span></code></pre></div>
<p>If you missed it, <code>positions</code> is the main product of our simulation. Here we’ll put <code>positions</code> in a tibble and reproduce the top portion of Figure 7.2.</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="markov-chain-monte-carlo.html#cb483-1"></a><span class="kw">tibble</span>(<span class="dt">theta =</span> positions) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb483-2"><a href="markov-chain-monte-carlo.html#cb483-2"></a><span class="st">  </span></span>
<span id="cb483-3"><a href="markov-chain-monte-carlo.html#cb483-3"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta)) <span class="op">+</span></span>
<span id="cb483-4"><a href="markov-chain-monte-carlo.html#cb483-4"></a><span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb483-5"><a href="markov-chain-monte-carlo.html#cb483-5"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="op">+</span></span>
<span id="cb483-6"><a href="markov-chain-monte-carlo.html#cb483-6"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="op">+</span></span>
<span id="cb483-7"><a href="markov-chain-monte-carlo.html#cb483-7"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-11-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Did you notice that <code>scale_y_continuous()</code> line in the code? Claus Wilke, the author of the <strong>cowplot</strong> package, has a lot of thoughts on data visualization. He even wrote a <span class="citation">(<a href="#ref-wilkeFundamentalsDataVisualization2019" role="doc-biblioref">2019</a><a href="#ref-wilkeFundamentalsDataVisualization2019" role="doc-biblioref">a</a>)</span> book on it: <a href="https://clauswilke.com/dataviz/"><em>Fundamentals of data visualization</em></a>. In his <span class="citation">(<a href="#ref-Wilke2019Themes" role="doc-biblioref">2019</a><a href="#ref-Wilke2019Themes" role="doc-biblioref">b</a>)</span> <a href="https://wilkelab.org/cowplot/articles/themes.html"><em>Themes</em></a> vignette, Wilke recommended against allowing for space between the bottoms of the bars in a bar plot and the <span class="math inline">\(x\)</span>-axis line. The <strong>ggplot2</strong> default is to allow for such a space. Here we followed Wilke and suppressed that space with <code>expand = expansion(mult = c(0, 0.05))</code>. You can learn more about the <code>ggplot2::expansion()</code> function <a href="https://ggplot2.tidyverse.org/reference/expansion.html">here</a>.</p>
<p>Here’s the middle portion of Figure 7.2.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="markov-chain-monte-carlo.html#cb484-1"></a><span class="kw">tibble</span>(<span class="dt">t     =</span> <span class="dv">1</span><span class="op">:</span><span class="fl">5e4</span>,</span>
<span id="cb484-2"><a href="markov-chain-monte-carlo.html#cb484-2"></a>       <span class="dt">theta =</span> positions) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb484-3"><a href="markov-chain-monte-carlo.html#cb484-3"></a><span class="st">  </span><span class="kw">slice</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb484-4"><a href="markov-chain-monte-carlo.html#cb484-4"></a><span class="st">  </span></span>
<span id="cb484-5"><a href="markov-chain-monte-carlo.html#cb484-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">y =</span> t)) <span class="op">+</span></span>
<span id="cb484-6"><a href="markov-chain-monte-carlo.html#cb484-6"></a><span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb484-7"><a href="markov-chain-monte-carlo.html#cb484-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb484-8"><a href="markov-chain-monte-carlo.html#cb484-8"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="op">+</span></span>
<span id="cb484-9"><a href="markov-chain-monte-carlo.html#cb484-9"></a><span class="st">  </span><span class="kw">scale_y_log10</span>(<span class="st">&quot;Time Step&quot;</span>, <span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">20</span>, <span class="dv">100</span>, <span class="dv">500</span>)) <span class="op">+</span></span>
<span id="cb484-10"><a href="markov-chain-monte-carlo.html#cb484-10"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-12-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>And now we make the bottom.</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="markov-chain-monte-carlo.html#cb485-1"></a><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>,</span>
<span id="cb485-2"><a href="markov-chain-monte-carlo.html#cb485-2"></a>       <span class="dt">y =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb485-3"><a href="markov-chain-monte-carlo.html#cb485-3"></a><span class="st">  </span></span>
<span id="cb485-4"><a href="markov-chain-monte-carlo.html#cb485-4"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span></span>
<span id="cb485-5"><a href="markov-chain-monte-carlo.html#cb485-5"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">width =</span> <span class="fl">.2</span>, <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb485-6"><a href="markov-chain-monte-carlo.html#cb485-6"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="op">+</span></span>
<span id="cb485-7"><a href="markov-chain-monte-carlo.html#cb485-7"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(<span class="kw">p</span>(theta)), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="op">+</span></span>
<span id="cb485-8"><a href="markov-chain-monte-carlo.html#cb485-8"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-13-1.png" width="576" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Notice that the sampled relative frequencies closely mimic the actual relative populations in the bottom panel! In fact, a sequence generated this way will converge, as the sequence gets longer, to an arbitrarily close approximation of the actual relative probabilities. (p. 149)</p>
</blockquote>
</div>
<div id="general-properties-of-a-random-walk." class="section level3">
<h3><span class="header-section-number">7.2.3</span> General properties of a random walk.</h3>
<blockquote>
<p>The tajectory shown in Figure 7.2 is just one possible sequence of positions when the movement heuristic is applied. At each time step, the direction of the proposed move is random, and if the relative probability of the proposed position is less than that of the current position, then acceptance of the proposed move is also random. Because of the randomness, if the process were started over again, then the specific trajectory would almost certainly be different. Regardless of the specific trajectory, in the long run the relative frequency of visits mimics the target distribution.</p>
<p>Figure 7.3 shows the probability of being in each position as a function of time. (p. 149)</p>
</blockquote>
<p>I was initially stumped on how to reproduce the simulation depicted in Figure 7.3. However, fellow enthusiast <a href="https://github.com/cmoten">Cardy Moten III</a> kindly <a href="https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/14">shared a solution</a> which was itself based on Kruschke’s blog post from 2012, <a href="https://doingbayesiandataanalysis.blogspot.com/2012/08/metropolis-algorithm-discrete-position.html"><em>Metropolis algorithm: Discrete position probabilities</em></a>. Here’s a mild reworking of their solutions. First, we simulate.</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="markov-chain-monte-carlo.html#cb486-1"></a>nslots   &lt;-<span class="st"> </span><span class="dv">7</span></span>
<span id="cb486-2"><a href="markov-chain-monte-carlo.html#cb486-2"></a>p_target &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">7</span></span>
<span id="cb486-3"><a href="markov-chain-monte-carlo.html#cb486-3"></a>p_target &lt;-<span class="st"> </span>p_target <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(p_target)</span>
<span id="cb486-4"><a href="markov-chain-monte-carlo.html#cb486-4"></a></span>
<span id="cb486-5"><a href="markov-chain-monte-carlo.html#cb486-5"></a><span class="co"># construct the transition matrix</span></span>
<span id="cb486-6"><a href="markov-chain-monte-carlo.html#cb486-6"></a>proposal_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> nslots, <span class="dt">ncol =</span> nslots)</span>
<span id="cb486-7"><a href="markov-chain-monte-carlo.html#cb486-7"></a></span>
<span id="cb486-8"><a href="markov-chain-monte-carlo.html#cb486-8"></a><span class="cf">for</span>(from_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nslots) {</span>
<span id="cb486-9"><a href="markov-chain-monte-carlo.html#cb486-9"></a>  <span class="cf">for</span>(to_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nslots) {</span>
<span id="cb486-10"><a href="markov-chain-monte-carlo.html#cb486-10"></a>    <span class="cf">if</span>(to_idx <span class="op">==</span><span class="st"> </span>from_idx <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) {proposal_matrix[from_idx, to_idx] &lt;-<span class="st"> </span><span class="fl">0.5</span>}</span>
<span id="cb486-11"><a href="markov-chain-monte-carlo.html#cb486-11"></a>    <span class="cf">if</span>(to_idx <span class="op">==</span><span class="st"> </span>from_idx <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) {proposal_matrix[from_idx, to_idx] &lt;-<span class="st"> </span><span class="fl">0.5</span>}</span>
<span id="cb486-12"><a href="markov-chain-monte-carlo.html#cb486-12"></a>  }</span>
<span id="cb486-13"><a href="markov-chain-monte-carlo.html#cb486-13"></a>}</span>
<span id="cb486-14"><a href="markov-chain-monte-carlo.html#cb486-14"></a></span>
<span id="cb486-15"><a href="markov-chain-monte-carlo.html#cb486-15"></a><span class="co"># construct the acceptance matrix</span></span>
<span id="cb486-16"><a href="markov-chain-monte-carlo.html#cb486-16"></a>acceptance_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> nslots, <span class="dt">ncol =</span> nslots)</span>
<span id="cb486-17"><a href="markov-chain-monte-carlo.html#cb486-17"></a></span>
<span id="cb486-18"><a href="markov-chain-monte-carlo.html#cb486-18"></a><span class="cf">for</span>(from_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nslots) {</span>
<span id="cb486-19"><a href="markov-chain-monte-carlo.html#cb486-19"></a>  <span class="cf">for</span>(to_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nslots) {</span>
<span id="cb486-20"><a href="markov-chain-monte-carlo.html#cb486-20"></a>    acceptance_matrix[from_idx, to_idx] &lt;-<span class="st"> </span><span class="kw">min</span>(p_target[to_idx] <span class="op">/</span><span class="st"> </span>p_target[from_idx], <span class="dv">1</span>)</span>
<span id="cb486-21"><a href="markov-chain-monte-carlo.html#cb486-21"></a>  }</span>
<span id="cb486-22"><a href="markov-chain-monte-carlo.html#cb486-22"></a>}</span>
<span id="cb486-23"><a href="markov-chain-monte-carlo.html#cb486-23"></a></span>
<span id="cb486-24"><a href="markov-chain-monte-carlo.html#cb486-24"></a><span class="co"># compute the matrix of move probabilities</span></span>
<span id="cb486-25"><a href="markov-chain-monte-carlo.html#cb486-25"></a>move_matrix &lt;-<span class="st"> </span>proposal_matrix <span class="op">*</span><span class="st"> </span>acceptance_matrix</span>
<span id="cb486-26"><a href="markov-chain-monte-carlo.html#cb486-26"></a></span>
<span id="cb486-27"><a href="markov-chain-monte-carlo.html#cb486-27"></a><span class="co"># compute the transition matrix, including the probability of staying in place</span></span>
<span id="cb486-28"><a href="markov-chain-monte-carlo.html#cb486-28"></a>transition_matrix &lt;-<span class="st"> </span>move_matrix</span>
<span id="cb486-29"><a href="markov-chain-monte-carlo.html#cb486-29"></a><span class="cf">for</span> (diag_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nslots) {</span>
<span id="cb486-30"><a href="markov-chain-monte-carlo.html#cb486-30"></a>  transition_matrix[diag_idx, diag_idx] =<span class="st"> </span><span class="fl">1.0</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(move_matrix[diag_idx, ])</span>
<span id="cb486-31"><a href="markov-chain-monte-carlo.html#cb486-31"></a>}</span>
<span id="cb486-32"><a href="markov-chain-monte-carlo.html#cb486-32"></a></span>
<span id="cb486-33"><a href="markov-chain-monte-carlo.html#cb486-33"></a><span class="co"># specify starting position vector:</span></span>
<span id="cb486-34"><a href="markov-chain-monte-carlo.html#cb486-34"></a>position_vec &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, nslots)</span>
<span id="cb486-35"><a href="markov-chain-monte-carlo.html#cb486-35"></a>position_vec[<span class="kw">round</span>(nslots <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)] &lt;-<span class="st"> </span><span class="fl">1.0</span></span>
<span id="cb486-36"><a href="markov-chain-monte-carlo.html#cb486-36"></a></span>
<span id="cb486-37"><a href="markov-chain-monte-carlo.html#cb486-37"></a>p &lt;-<span class="st"> </span><span class="kw">list</span>()</span>
<span id="cb486-38"><a href="markov-chain-monte-carlo.html#cb486-38"></a>data &lt;-<span class="st"> </span></span>
<span id="cb486-39"><a href="markov-chain-monte-carlo.html#cb486-39"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">position =</span> <span class="dv">1</span><span class="op">:</span>nslots, </span>
<span id="cb486-40"><a href="markov-chain-monte-carlo.html#cb486-40"></a>         <span class="dt">prob     =</span> position_vec)</span>
<span id="cb486-41"><a href="markov-chain-monte-carlo.html#cb486-41"></a></span>
<span id="cb486-42"><a href="markov-chain-monte-carlo.html#cb486-42"></a><span class="co"># loop through the requisite time indexes</span></span>
<span id="cb486-43"><a href="markov-chain-monte-carlo.html#cb486-43"></a><span class="co"># update the data and transition vector</span></span>
<span id="cb486-44"><a href="markov-chain-monte-carlo.html#cb486-44"></a><span class="cf">for</span>(time_idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">99</span>) {</span>
<span id="cb486-45"><a href="markov-chain-monte-carlo.html#cb486-45"></a>  </span>
<span id="cb486-46"><a href="markov-chain-monte-carlo.html#cb486-46"></a>  p[[time_idx]] &lt;-<span class="st"> </span>data</span>
<span id="cb486-47"><a href="markov-chain-monte-carlo.html#cb486-47"></a>  </span>
<span id="cb486-48"><a href="markov-chain-monte-carlo.html#cb486-48"></a>  <span class="co"># update the position vec</span></span>
<span id="cb486-49"><a href="markov-chain-monte-carlo.html#cb486-49"></a>  position_vec &lt;-<span class="st"> </span>position_vec <span class="op">%*%</span><span class="st"> </span>transition_matrix</span>
<span id="cb486-50"><a href="markov-chain-monte-carlo.html#cb486-50"></a>  </span>
<span id="cb486-51"><a href="markov-chain-monte-carlo.html#cb486-51"></a>  <span class="co"># update the data</span></span>
<span id="cb486-52"><a href="markov-chain-monte-carlo.html#cb486-52"></a>  data &lt;-<span class="st"> </span><span class="ot">NULL</span> </span>
<span id="cb486-53"><a href="markov-chain-monte-carlo.html#cb486-53"></a>  data &lt;-<span class="st"> </span></span>
<span id="cb486-54"><a href="markov-chain-monte-carlo.html#cb486-54"></a><span class="st">    </span><span class="kw">tibble</span>(<span class="dt">position =</span> <span class="dv">1</span><span class="op">:</span>nslots, </span>
<span id="cb486-55"><a href="markov-chain-monte-carlo.html#cb486-55"></a>           <span class="dt">prob     =</span> <span class="kw">t</span>(position_vec))</span>
<span id="cb486-56"><a href="markov-chain-monte-carlo.html#cb486-56"></a>}</span></code></pre></div>
<p>Now we wrangle and plot.</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="markov-chain-monte-carlo.html#cb487-1"></a>p <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-2"><a href="markov-chain-monte-carlo.html#cb487-2"></a><span class="st">  </span><span class="kw">as_tibble_col</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-3"><a href="markov-chain-monte-carlo.html#cb487-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">facet =</span> <span class="kw">str_c</span>(<span class="st">&quot;italic(t)==&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">99</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-4"><a href="markov-chain-monte-carlo.html#cb487-4"></a><span class="st">  </span><span class="kw">slice</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">14</span>, <span class="dv">99</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-5"><a href="markov-chain-monte-carlo.html#cb487-5"></a><span class="st">  </span><span class="kw">unnest</span>(value) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-6"><a href="markov-chain-monte-carlo.html#cb487-6"></a><span class="st">  </span><span class="kw">bind_rows</span>(</span>
<span id="cb487-7"><a href="markov-chain-monte-carlo.html#cb487-7"></a>    <span class="kw">tibble</span>(<span class="dt">position =</span> <span class="dv">1</span><span class="op">:</span>nslots, </span>
<span id="cb487-8"><a href="markov-chain-monte-carlo.html#cb487-8"></a>           <span class="dt">prob     =</span> p_target, </span>
<span id="cb487-9"><a href="markov-chain-monte-carlo.html#cb487-9"></a>           <span class="dt">facet    =</span> <span class="st">&quot;target&quot;</span>)</span>
<span id="cb487-10"><a href="markov-chain-monte-carlo.html#cb487-10"></a>  ) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-11"><a href="markov-chain-monte-carlo.html#cb487-11"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">facet =</span> <span class="kw">factor</span>(facet, <span class="dt">levels =</span> <span class="kw">c</span>(<span class="kw">str_c</span>(<span class="st">&quot;italic(t)==&quot;</span>, <span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">14</span>, <span class="dv">99</span>)), <span class="st">&quot;target&quot;</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb487-12"><a href="markov-chain-monte-carlo.html#cb487-12"></a><span class="st">  </span></span>
<span id="cb487-13"><a href="markov-chain-monte-carlo.html#cb487-13"></a><span class="st">  </span><span class="co"># plot!</span></span>
<span id="cb487-14"><a href="markov-chain-monte-carlo.html#cb487-14"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> position, <span class="dt">y =</span> prob, <span class="dt">fill =</span> facet <span class="op">==</span><span class="st"> &quot;target&quot;</span>)) <span class="op">+</span></span>
<span id="cb487-15"><a href="markov-chain-monte-carlo.html#cb487-15"></a><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">width =</span> <span class="fl">.2</span>) <span class="op">+</span></span>
<span id="cb487-16"><a href="markov-chain-monte-carlo.html#cb487-16"></a><span class="st">  </span><span class="kw">scale_fill_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;steelblue&quot;</span>, <span class="st">&quot;goldenrod2&quot;</span>), <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb487-17"><a href="markov-chain-monte-carlo.html#cb487-17"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="op">+</span></span>
<span id="cb487-18"><a href="markov-chain-monte-carlo.html#cb487-18"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(<span class="kw">italic</span>(p)(theta)), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.05</span>))) <span class="op">+</span></span>
<span id="cb487-19"><a href="markov-chain-monte-carlo.html#cb487-19"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb487-20"><a href="markov-chain-monte-carlo.html#cb487-20"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>facet, <span class="dt">scales =</span> <span class="st">&quot;free_y&quot;</span>, <span class="dt">labeller =</span> label_parsed)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-15-1.png" width="960" style="display: block; margin: auto;" /></p>
</div>
<div id="why-we-care." class="section level3">
<h3><span class="header-section-number">7.2.4</span> Why we care.</h3>
<p>Through the simple magic of the random walk procedure,</p>
<blockquote>
<p>we are able to do <em>indirectly</em> something we could not necessarily do directly: We can generate random samples from the target distribution. Moreover, we can generate those random samples from the target distribution even when the target distribution is not normalized.</p>
<p>This technique is profoundly useful when the target distribution <span class="math inline">\(P(\theta)\)</span> is a posterior proportional to <span class="math inline">\(p(D | \theta) p(\theta)\)</span>. Merely by evaluating <span class="math inline">\(p(D | \theta) p(\theta)\)</span>, without normalizing it by <span class="math inline">\(p(D)\)</span>, we can generate random representative values from the posterior distribution. This result is wonderful because the method obviates direct computation of the evidence <span class="math inline">\(p(D)\)</span>, which, as you’ll recall, is one of the most difficult aspects of Bayesian inference. By using MCMC techniques, we can do Bayesian inference in rich and complex models. It has only been with the development of MCMC algorithms and software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience. (p. 152, <em>emphasis</em> in the original)</p>
</blockquote>
</div>
</div>
<div id="the-metropolis-algorithm-more-generally" class="section level2">
<h2><span class="header-section-number">7.3</span> The Metropolis algorithm more generally</h2>
<p>“The procedure described in the previous section was just a special case of a more general procedure known as the Metropolis algorithm, named after the first author of a famous article <span class="citation">(Metropolis et al., <a href="#ref-metropolisEquationStateCalculations1953" role="doc-biblioref">1953</a>)</span>” (p. 156).</p>
<p>Here’s how to generate a proposed jump from a zero-mean normal distribution with a standard deviation of 0.2.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="markov-chain-monte-carlo.html#cb488-1"></a><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<pre><code>## [1] -0.1985524</code></pre>
<p>To get a sense of what draws from <code>rnorm()</code> looks like in the long run, we might plot.</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="markov-chain-monte-carlo.html#cb490-1"></a>mu    &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb490-2"><a href="markov-chain-monte-carlo.html#cb490-2"></a>sigma &lt;-<span class="st"> </span><span class="fl">0.2</span></span>
<span id="cb490-3"><a href="markov-chain-monte-carlo.html#cb490-3"></a></span>
<span id="cb490-4"><a href="markov-chain-monte-carlo.html#cb490-4"></a><span class="co"># how many proposals would you like?</span></span>
<span id="cb490-5"><a href="markov-chain-monte-carlo.html#cb490-5"></a>n  &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb490-6"><a href="markov-chain-monte-carlo.html#cb490-6"></a></span>
<span id="cb490-7"><a href="markov-chain-monte-carlo.html#cb490-7"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb490-8"><a href="markov-chain-monte-carlo.html#cb490-8"></a><span class="kw">tibble</span>(<span class="dt">proposed_jump =</span> <span class="kw">rnorm</span>(n, <span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb490-9"><a href="markov-chain-monte-carlo.html#cb490-9"></a><span class="st">  </span></span>
<span id="cb490-10"><a href="markov-chain-monte-carlo.html#cb490-10"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> proposed_jump, <span class="dt">y =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb490-11"><a href="markov-chain-monte-carlo.html#cb490-11"></a><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="dv">0</span>, <span class="dt">height =</span> <span class="fl">.1</span>, </span>
<span id="cb490-12"><a href="markov-chain-monte-carlo.html#cb490-12"></a>              <span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb490-13"><a href="markov-chain-monte-carlo.html#cb490-13"></a><span class="st">  </span><span class="co"># this is the idealized distribution</span></span>
<span id="cb490-14"><a href="markov-chain-monte-carlo.html#cb490-14"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dnorm, <span class="dt">args =</span> <span class="kw">list</span>(<span class="dt">mean =</span> mu, <span class="dt">sd =</span> sigma),</span>
<span id="cb490-15"><a href="markov-chain-monte-carlo.html#cb490-15"></a>                <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb490-16"><a href="markov-chain-monte-carlo.html#cb490-16"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="dt">from =</span> <span class="fl">-0.6</span>, <span class="dt">to =</span> <span class="fl">0.6</span>, <span class="dt">length.out =</span> <span class="dv">7</span>)) <span class="op">+</span></span>
<span id="cb490-17"><a href="markov-chain-monte-carlo.html#cb490-17"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb490-18"><a href="markov-chain-monte-carlo.html#cb490-18"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Jump proposals&quot;</span>,</span>
<span id="cb490-19"><a href="markov-chain-monte-carlo.html#cb490-19"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;The blue line shows the data generating distribution.&quot;</span>) <span class="op">+</span></span>
<span id="cb490-20"><a href="markov-chain-monte-carlo.html#cb490-20"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-17-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>Anyway,</p>
<blockquote>
<p>having generated a proposed new position, the algorithm then decides whether or not to accept the proposal. The decision rule is exactly what was already specified in Equation 7.1. In detail, this is accomplished by computing the ratio <span class="math inline">\(p_\text{move} = P(\theta_\text{proposed}) / P(\theta_\text{current})\)</span>. Then a random number from the uniform interval <span class="math inline">\([0, 1]\)</span> is generated; in R, this can be accomplished with the command <code>runif(1)</code>. If the random number is between 0 and pmove, then the move is accepted. (p. 157)</p>
</blockquote>
<p>We’ll see what that might look like in the next section. In the meantime, here’s how to use <code>runif()</code>.</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="markov-chain-monte-carlo.html#cb491-1"></a><span class="kw">runif</span>(<span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] 0.2783186</code></pre>
<p>Just for kicks, here’s what that looks like in bulk.</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="markov-chain-monte-carlo.html#cb493-1"></a><span class="co"># how many proposals would you like?</span></span>
<span id="cb493-2"><a href="markov-chain-monte-carlo.html#cb493-2"></a>n  &lt;-<span class="st"> </span><span class="dv">500</span></span>
<span id="cb493-3"><a href="markov-chain-monte-carlo.html#cb493-3"></a></span>
<span id="cb493-4"><a href="markov-chain-monte-carlo.html#cb493-4"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb493-5"><a href="markov-chain-monte-carlo.html#cb493-5"></a><span class="kw">tibble</span>(<span class="dt">draw =</span> <span class="kw">runif</span>(n)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb493-6"><a href="markov-chain-monte-carlo.html#cb493-6"></a><span class="st">  </span></span>
<span id="cb493-7"><a href="markov-chain-monte-carlo.html#cb493-7"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> draw, <span class="dt">y =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb493-8"><a href="markov-chain-monte-carlo.html#cb493-8"></a><span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">width =</span> <span class="dv">0</span>, <span class="dt">height =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, </span>
<span id="cb493-9"><a href="markov-chain-monte-carlo.html#cb493-9"></a>              <span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb493-10"><a href="markov-chain-monte-carlo.html#cb493-10"></a><span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> dunif,</span>
<span id="cb493-11"><a href="markov-chain-monte-carlo.html#cb493-11"></a>                <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb493-12"><a href="markov-chain-monte-carlo.html#cb493-12"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">5</span><span class="op">/</span><span class="dv">3</span>)) <span class="op">+</span></span>
<span id="cb493-13"><a href="markov-chain-monte-carlo.html#cb493-13"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Uniform draws&quot;</span>,</span>
<span id="cb493-14"><a href="markov-chain-monte-carlo.html#cb493-14"></a>       <span class="dt">subtitle =</span> <span class="st">&quot;The blue line shows the data generating distribution.&quot;</span>) <span class="op">+</span></span>
<span id="cb493-15"><a href="markov-chain-monte-carlo.html#cb493-15"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-19-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>We do not see a concentration towards the mean, this time. The draws are uniformly distributed across the parameter space.</p>
<div id="metropolis-algorithm-applied-to-bernoulli-likelihood-and-beta-prior." class="section level3">
<h3><span class="header-section-number">7.3.1</span> Metropolis algorithm applied to Bernoulli likelihood and beta prior.</h3>
<p>You can find Kruschke’s code in the <code>BernMetrop.R</code> file. I’m going to break it up a little.</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="markov-chain-monte-carlo.html#cb494-1"></a><span class="co"># specify the data, to be used in the likelihood function.</span></span>
<span id="cb494-2"><a href="markov-chain-monte-carlo.html#cb494-2"></a>my_data &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">6</span>), <span class="kw">rep</span>(<span class="dv">1</span>, <span class="dv">14</span>))</span>
<span id="cb494-3"><a href="markov-chain-monte-carlo.html#cb494-3"></a></span>
<span id="cb494-4"><a href="markov-chain-monte-carlo.html#cb494-4"></a><span class="co"># define the Bernoulli likelihood function, p(D|theta).</span></span>
<span id="cb494-5"><a href="markov-chain-monte-carlo.html#cb494-5"></a><span class="co"># the argument theta could be a vector, not just a scalar</span></span>
<span id="cb494-6"><a href="markov-chain-monte-carlo.html#cb494-6"></a>likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta, data) {</span>
<span id="cb494-7"><a href="markov-chain-monte-carlo.html#cb494-7"></a>  z &lt;-<span class="st"> </span><span class="kw">sum</span>(data)</span>
<span id="cb494-8"><a href="markov-chain-monte-carlo.html#cb494-8"></a>  n &lt;-<span class="st"> </span><span class="kw">length</span>(data)</span>
<span id="cb494-9"><a href="markov-chain-monte-carlo.html#cb494-9"></a>  p_data_given_theta &lt;-<span class="st"> </span>theta<span class="op">^</span>z <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span>z)</span>
<span id="cb494-10"><a href="markov-chain-monte-carlo.html#cb494-10"></a>  <span class="co"># the theta values passed into this function are generated at random,</span></span>
<span id="cb494-11"><a href="markov-chain-monte-carlo.html#cb494-11"></a>  <span class="co"># and therefore might be inadvertently greater than 1 or less than 0.</span></span>
<span id="cb494-12"><a href="markov-chain-monte-carlo.html#cb494-12"></a>  <span class="co"># the likelihood for theta &gt; 1 or for theta &lt; 0 is zero</span></span>
<span id="cb494-13"><a href="markov-chain-monte-carlo.html#cb494-13"></a>  p_data_given_theta[theta <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>theta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>] &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb494-14"><a href="markov-chain-monte-carlo.html#cb494-14"></a>  <span class="kw">return</span>(p_data_given_theta)</span>
<span id="cb494-15"><a href="markov-chain-monte-carlo.html#cb494-15"></a>}</span>
<span id="cb494-16"><a href="markov-chain-monte-carlo.html#cb494-16"></a></span>
<span id="cb494-17"><a href="markov-chain-monte-carlo.html#cb494-17"></a><span class="co"># define the prior density function. </span></span>
<span id="cb494-18"><a href="markov-chain-monte-carlo.html#cb494-18"></a>prior_d &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</span>
<span id="cb494-19"><a href="markov-chain-monte-carlo.html#cb494-19"></a>  p_theta &lt;-<span class="st"> </span><span class="kw">dbeta</span>(theta, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb494-20"><a href="markov-chain-monte-carlo.html#cb494-20"></a>  <span class="co"># the theta values passed into this function are generated at random,</span></span>
<span id="cb494-21"><a href="markov-chain-monte-carlo.html#cb494-21"></a>  <span class="co"># and therefore might be inadvertently greater than 1 or less than 0.</span></span>
<span id="cb494-22"><a href="markov-chain-monte-carlo.html#cb494-22"></a>  <span class="co"># the prior for theta &gt; 1 or for theta &lt; 0 is zero</span></span>
<span id="cb494-23"><a href="markov-chain-monte-carlo.html#cb494-23"></a>  p_theta[theta <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">|</span><span class="st"> </span>theta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>] =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb494-24"><a href="markov-chain-monte-carlo.html#cb494-24"></a>  <span class="kw">return</span>(p_theta)</span>
<span id="cb494-25"><a href="markov-chain-monte-carlo.html#cb494-25"></a>}</span>
<span id="cb494-26"><a href="markov-chain-monte-carlo.html#cb494-26"></a></span>
<span id="cb494-27"><a href="markov-chain-monte-carlo.html#cb494-27"></a><span class="co"># define the relative probability of the target distribution, </span></span>
<span id="cb494-28"><a href="markov-chain-monte-carlo.html#cb494-28"></a><span class="co"># as a function of vector theta. for our application, this</span></span>
<span id="cb494-29"><a href="markov-chain-monte-carlo.html#cb494-29"></a><span class="co"># target distribution is the unnormalized posterior distribution</span></span>
<span id="cb494-30"><a href="markov-chain-monte-carlo.html#cb494-30"></a>target_rel_prob &lt;-<span class="st"> </span><span class="cf">function</span>(theta, data) {</span>
<span id="cb494-31"><a href="markov-chain-monte-carlo.html#cb494-31"></a>  target_rel_prob &lt;-<span class="st"> </span><span class="kw">likelihood</span>(theta, data) <span class="op">*</span><span class="st"> </span><span class="kw">prior_d</span>(theta)</span>
<span id="cb494-32"><a href="markov-chain-monte-carlo.html#cb494-32"></a>  <span class="kw">return</span>(target_rel_prob)</span>
<span id="cb494-33"><a href="markov-chain-monte-carlo.html#cb494-33"></a>}</span>
<span id="cb494-34"><a href="markov-chain-monte-carlo.html#cb494-34"></a></span>
<span id="cb494-35"><a href="markov-chain-monte-carlo.html#cb494-35"></a><span class="co"># specify the length of the trajectory, i.e., the number of jumps to try:</span></span>
<span id="cb494-36"><a href="markov-chain-monte-carlo.html#cb494-36"></a>traj_length &lt;-<span class="st"> </span><span class="dv">50000</span> <span class="co"># this is just an arbitrary large number</span></span>
<span id="cb494-37"><a href="markov-chain-monte-carlo.html#cb494-37"></a></span>
<span id="cb494-38"><a href="markov-chain-monte-carlo.html#cb494-38"></a><span class="co"># initialize the vector that will store the results</span></span>
<span id="cb494-39"><a href="markov-chain-monte-carlo.html#cb494-39"></a>trajectory &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, traj_length)</span>
<span id="cb494-40"><a href="markov-chain-monte-carlo.html#cb494-40"></a></span>
<span id="cb494-41"><a href="markov-chain-monte-carlo.html#cb494-41"></a><span class="co"># specify where to start the trajectory:</span></span>
<span id="cb494-42"><a href="markov-chain-monte-carlo.html#cb494-42"></a>trajectory[<span class="dv">1</span>] &lt;-<span class="st"> </span><span class="fl">0.01</span> <span class="co"># another arbitrary value</span></span>
<span id="cb494-43"><a href="markov-chain-monte-carlo.html#cb494-43"></a></span>
<span id="cb494-44"><a href="markov-chain-monte-carlo.html#cb494-44"></a><span class="co"># specify the burn-in period</span></span>
<span id="cb494-45"><a href="markov-chain-monte-carlo.html#cb494-45"></a>burn_in &lt;-<span class="st"> </span><span class="kw">ceiling</span>(<span class="fl">0.0</span> <span class="op">*</span><span class="st"> </span>traj_length) <span class="co"># arbitrary number, less than `traj_length`</span></span>
<span id="cb494-46"><a href="markov-chain-monte-carlo.html#cb494-46"></a></span>
<span id="cb494-47"><a href="markov-chain-monte-carlo.html#cb494-47"></a><span class="co"># initialize accepted, rejected counters, just to monitor performance:</span></span>
<span id="cb494-48"><a href="markov-chain-monte-carlo.html#cb494-48"></a>n_accepted &lt;-<span class="st"> </span><span class="dv">0</span></span>
<span id="cb494-49"><a href="markov-chain-monte-carlo.html#cb494-49"></a>n_rejected &lt;-<span class="st"> </span><span class="dv">0</span></span></code></pre></div>
<p>That first part follows what Kruschke put in his script. I’m going to bundel the next large potion in a fucntion, <code>my_metropolis()</code> which will make it easier to plug the code into the <code>purrr::map()</code> function.</p>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="markov-chain-monte-carlo.html#cb495-1"></a>my_metropolis &lt;-<span class="st"> </span><span class="cf">function</span>(proposal_sd) {</span>
<span id="cb495-2"><a href="markov-chain-monte-carlo.html#cb495-2"></a>  </span>
<span id="cb495-3"><a href="markov-chain-monte-carlo.html#cb495-3"></a>  <span class="co"># now generate the random walk. the &#39;t&#39; index is time or trial in the walk.</span></span>
<span id="cb495-4"><a href="markov-chain-monte-carlo.html#cb495-4"></a>  <span class="co"># specify seed to reproduce same random walk</span></span>
<span id="cb495-5"><a href="markov-chain-monte-carlo.html#cb495-5"></a>  <span class="kw">set.seed</span>(<span class="dv">47405</span>)</span>
<span id="cb495-6"><a href="markov-chain-monte-carlo.html#cb495-6"></a>  </span>
<span id="cb495-7"><a href="markov-chain-monte-carlo.html#cb495-7"></a>  </span>
<span id="cb495-8"><a href="markov-chain-monte-carlo.html#cb495-8"></a>  <span class="co">## I&#39;m taking this section out and will replace it</span></span>
<span id="cb495-9"><a href="markov-chain-monte-carlo.html#cb495-9"></a>  </span>
<span id="cb495-10"><a href="markov-chain-monte-carlo.html#cb495-10"></a>  <span class="co"># # specify standard deviation of proposal distribution</span></span>
<span id="cb495-11"><a href="markov-chain-monte-carlo.html#cb495-11"></a>  <span class="co"># proposal_sd &lt;- c(0.02, 0.2, 2.0)[2]</span></span>
<span id="cb495-12"><a href="markov-chain-monte-carlo.html#cb495-12"></a>  </span>
<span id="cb495-13"><a href="markov-chain-monte-carlo.html#cb495-13"></a>  <span class="co">## end of the section I took out</span></span>
<span id="cb495-14"><a href="markov-chain-monte-carlo.html#cb495-14"></a>  </span>
<span id="cb495-15"><a href="markov-chain-monte-carlo.html#cb495-15"></a>  </span>
<span id="cb495-16"><a href="markov-chain-monte-carlo.html#cb495-16"></a>  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(traj_length <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) {</span>
<span id="cb495-17"><a href="markov-chain-monte-carlo.html#cb495-17"></a>    current_position &lt;-<span class="st"> </span>trajectory[t]</span>
<span id="cb495-18"><a href="markov-chain-monte-carlo.html#cb495-18"></a>    <span class="co"># use the proposal distribution to generate a proposed jump</span></span>
<span id="cb495-19"><a href="markov-chain-monte-carlo.html#cb495-19"></a>    proposed_jump &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> proposal_sd)</span>
<span id="cb495-20"><a href="markov-chain-monte-carlo.html#cb495-20"></a>    <span class="co"># compute the probability of accepting the proposed jump</span></span>
<span id="cb495-21"><a href="markov-chain-monte-carlo.html#cb495-21"></a>    prob_accept &lt;-<span class="st"> </span><span class="kw">min</span>(<span class="dv">1</span>,</span>
<span id="cb495-22"><a href="markov-chain-monte-carlo.html#cb495-22"></a>                       <span class="kw">target_rel_prob</span>(current_position <span class="op">+</span><span class="st"> </span>proposed_jump, my_data)</span>
<span id="cb495-23"><a href="markov-chain-monte-carlo.html#cb495-23"></a>                       <span class="op">/</span><span class="st"> </span><span class="kw">target_rel_prob</span>(current_position, my_data))</span>
<span id="cb495-24"><a href="markov-chain-monte-carlo.html#cb495-24"></a>    <span class="co"># generate a random uniform value from the interval [0, 1] to</span></span>
<span id="cb495-25"><a href="markov-chain-monte-carlo.html#cb495-25"></a>    <span class="co"># decide whether or not to accept the proposed jump</span></span>
<span id="cb495-26"><a href="markov-chain-monte-carlo.html#cb495-26"></a>    <span class="cf">if</span> (<span class="kw">runif</span>(<span class="dv">1</span>) <span class="op">&lt;</span><span class="st"> </span>prob_accept) {</span>
<span id="cb495-27"><a href="markov-chain-monte-carlo.html#cb495-27"></a>      <span class="co"># accept the proposed jump</span></span>
<span id="cb495-28"><a href="markov-chain-monte-carlo.html#cb495-28"></a>      trajectory[t <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>current_position <span class="op">+</span><span class="st"> </span>proposed_jump</span>
<span id="cb495-29"><a href="markov-chain-monte-carlo.html#cb495-29"></a>      <span class="co"># increment the accepted counter, just to monitor performance</span></span>
<span id="cb495-30"><a href="markov-chain-monte-carlo.html#cb495-30"></a>      <span class="cf">if</span> (t <span class="op">&gt;</span><span class="st"> </span>burn_in) {n_accepted &lt;-<span class="st"> </span>n_accepted <span class="op">+</span><span class="st"> </span><span class="dv">1</span>}</span>
<span id="cb495-31"><a href="markov-chain-monte-carlo.html#cb495-31"></a>    } <span class="cf">else</span> {</span>
<span id="cb495-32"><a href="markov-chain-monte-carlo.html#cb495-32"></a>      <span class="co"># reject the proposed jump, stay at current position</span></span>
<span id="cb495-33"><a href="markov-chain-monte-carlo.html#cb495-33"></a>      trajectory[t <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] &lt;-<span class="st"> </span>current_position</span>
<span id="cb495-34"><a href="markov-chain-monte-carlo.html#cb495-34"></a>      <span class="co"># increment the rejected counter, just to monitor performance</span></span>
<span id="cb495-35"><a href="markov-chain-monte-carlo.html#cb495-35"></a>      <span class="cf">if</span> (t <span class="op">&gt;</span><span class="st"> </span>burn_in) {n_rejected &lt;-<span class="st"> </span>n_rejected <span class="op">+</span><span class="st"> </span><span class="dv">1</span>}</span>
<span id="cb495-36"><a href="markov-chain-monte-carlo.html#cb495-36"></a>    }</span>
<span id="cb495-37"><a href="markov-chain-monte-carlo.html#cb495-37"></a>  }</span>
<span id="cb495-38"><a href="markov-chain-monte-carlo.html#cb495-38"></a>  </span>
<span id="cb495-39"><a href="markov-chain-monte-carlo.html#cb495-39"></a>  <span class="co"># extract the post-burn_in portion of the trajectory</span></span>
<span id="cb495-40"><a href="markov-chain-monte-carlo.html#cb495-40"></a>  accepted_traj &lt;-<span class="st"> </span>trajectory[(burn_in <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">:</span><span class="st"> </span><span class="kw">length</span>(trajectory)]</span>
<span id="cb495-41"><a href="markov-chain-monte-carlo.html#cb495-41"></a>  </span>
<span id="cb495-42"><a href="markov-chain-monte-carlo.html#cb495-42"></a>  <span class="kw">tibble</span>(<span class="dt">accepted_traj =</span> accepted_traj,</span>
<span id="cb495-43"><a href="markov-chain-monte-carlo.html#cb495-43"></a>         <span class="dt">n_accepted    =</span> n_accepted, </span>
<span id="cb495-44"><a href="markov-chain-monte-carlo.html#cb495-44"></a>         <span class="dt">n_rejected    =</span> n_rejected)</span>
<span id="cb495-45"><a href="markov-chain-monte-carlo.html#cb495-45"></a>  <span class="co"># end of Metropolis algorithm</span></span>
<span id="cb495-46"><a href="markov-chain-monte-carlo.html#cb495-46"></a>  </span>
<span id="cb495-47"><a href="markov-chain-monte-carlo.html#cb495-47"></a>}</span></code></pre></div>
<p>Now we have <code>my_metropolis()</code>, we can run the analysis based on the three <code>proposal_sd</code> values, nesting the results in a tibble.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="markov-chain-monte-carlo.html#cb496-1"></a>d &lt;-</span>
<span id="cb496-2"><a href="markov-chain-monte-carlo.html#cb496-2"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">proposal_sd =</span> <span class="kw">c</span>(<span class="fl">0.02</span>, <span class="fl">0.2</span>, <span class="fl">2.0</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb496-3"><a href="markov-chain-monte-carlo.html#cb496-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">accepted_traj =</span> <span class="kw">map</span>(proposal_sd, my_metropolis)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb496-4"><a href="markov-chain-monte-carlo.html#cb496-4"></a><span class="st">  </span><span class="kw">unnest</span>(accepted_traj)</span>
<span id="cb496-5"><a href="markov-chain-monte-carlo.html#cb496-5"></a></span>
<span id="cb496-6"><a href="markov-chain-monte-carlo.html#cb496-6"></a><span class="kw">glimpse</span>(d)</span></code></pre></div>
<pre><code>## Rows: 150,000
## Columns: 4
## $ proposal_sd   &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.0…
## $ accepted_traj &lt;dbl&gt; 0.01000000, 0.01000000, 0.01000000, 0.01000000, 0.01149173, 0.02550380, 0.0…
## $ n_accepted    &lt;dbl&gt; 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801…
## $ n_rejected    &lt;dbl&gt; 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 319…</code></pre>
<p>Now we have <code>d</code> in hand, here’s the top portion of Figure 7.4.</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="markov-chain-monte-carlo.html#cb498-1"></a>d &lt;-</span>
<span id="cb498-2"><a href="markov-chain-monte-carlo.html#cb498-2"></a><span class="st">  </span>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb498-3"><a href="markov-chain-monte-carlo.html#cb498-3"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">proposal_sd =</span> <span class="kw">str_c</span>(<span class="st">&quot;Proposal SD = &quot;</span>, proposal_sd),</span>
<span id="cb498-4"><a href="markov-chain-monte-carlo.html#cb498-4"></a>         <span class="dt">iter        =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50000</span>, <span class="dt">times =</span> <span class="dv">3</span>))</span>
<span id="cb498-5"><a href="markov-chain-monte-carlo.html#cb498-5"></a>  </span>
<span id="cb498-6"><a href="markov-chain-monte-carlo.html#cb498-6"></a>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb498-7"><a href="markov-chain-monte-carlo.html#cb498-7"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> accepted_traj, <span class="dt">y =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb498-8"><a href="markov-chain-monte-carlo.html#cb498-8"></a><span class="st">  </span><span class="kw">stat_histinterval</span>(<span class="dt">point_interval =</span> mode_hdi, <span class="dt">.width =</span> <span class="fl">.95</span>,</span>
<span id="cb498-9"><a href="markov-chain-monte-carlo.html#cb498-9"></a>                    <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>, <span class="dt">slab_color =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">outline_bars =</span> T,</span>
<span id="cb498-10"><a href="markov-chain-monte-carlo.html#cb498-10"></a>                    <span class="dt">breaks =</span> <span class="dv">40</span>, <span class="dt">normalize =</span> <span class="st">&quot;panels&quot;</span>) <span class="op">+</span></span>
<span id="cb498-11"><a href="markov-chain-monte-carlo.html#cb498-11"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span>) <span class="op">+</span></span>
<span id="cb498-12"><a href="markov-chain-monte-carlo.html#cb498-12"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb498-13"><a href="markov-chain-monte-carlo.html#cb498-13"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb498-14"><a href="markov-chain-monte-carlo.html#cb498-14"></a><span class="st">  </span><span class="kw">panel_border</span>() <span class="op">+</span></span>
<span id="cb498-15"><a href="markov-chain-monte-carlo.html#cb498-15"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>proposal_sd, <span class="dt">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-22-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The modes are the points and the lines depict the 95% HDIs. Also, did you notice our use of the <code>cowplot::panel_border()</code> function? The settings from <code>theme_cowplot()</code> can make it difficult to differentiate among subplots when faceting. By throwing in a call to <code>panel_border()</code> after <code>theme_cowplot()</code>, we added in lightweight panel borders.</p>
<p>Here’s the middle of Figure 7.4.</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="markov-chain-monte-carlo.html#cb499-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb499-2"><a href="markov-chain-monte-carlo.html#cb499-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> accepted_traj, <span class="dt">y =</span> iter)) <span class="op">+</span></span>
<span id="cb499-3"><a href="markov-chain-monte-carlo.html#cb499-3"></a><span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb499-4"><a href="markov-chain-monte-carlo.html#cb499-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb499-5"><a href="markov-chain-monte-carlo.html#cb499-5"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb499-6"><a href="markov-chain-monte-carlo.html#cb499-6"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Step in Chain&quot;</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">49900</span>, <span class="dv">50000</span>)) <span class="op">+</span></span>
<span id="cb499-7"><a href="markov-chain-monte-carlo.html#cb499-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;End of Chain&quot;</span>) <span class="op">+</span></span>
<span id="cb499-8"><a href="markov-chain-monte-carlo.html#cb499-8"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb499-9"><a href="markov-chain-monte-carlo.html#cb499-9"></a><span class="st">  </span><span class="kw">panel_border</span>() <span class="op">+</span></span>
<span id="cb499-10"><a href="markov-chain-monte-carlo.html#cb499-10"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>proposal_sd, <span class="dt">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-23-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>The bottom:</p>
<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb500-1"><a href="markov-chain-monte-carlo.html#cb500-1"></a>d <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb500-2"><a href="markov-chain-monte-carlo.html#cb500-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> accepted_traj, <span class="dt">y =</span> iter)) <span class="op">+</span></span>
<span id="cb500-3"><a href="markov-chain-monte-carlo.html#cb500-3"></a><span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb500-4"><a href="markov-chain-monte-carlo.html#cb500-4"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb500-5"><a href="markov-chain-monte-carlo.html#cb500-5"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">5</span> <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb500-6"><a href="markov-chain-monte-carlo.html#cb500-6"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="st">&quot;Step in Chain&quot;</span>, <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">100</span>)) <span class="op">+</span></span>
<span id="cb500-7"><a href="markov-chain-monte-carlo.html#cb500-7"></a><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;End of Chain&quot;</span>) <span class="op">+</span></span>
<span id="cb500-8"><a href="markov-chain-monte-carlo.html#cb500-8"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb500-9"><a href="markov-chain-monte-carlo.html#cb500-9"></a><span class="st">  </span><span class="kw">panel_border</span>() <span class="op">+</span></span>
<span id="cb500-10"><a href="markov-chain-monte-carlo.html#cb500-10"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>proposal_sd, <span class="dt">ncol =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-24-1.png" width="960" style="display: block; margin: auto;" /></p>
<blockquote>
<p>Regardless of the which proposal distribution in Figure 7.4 is used, the Metropolis algorithm will eventually produce an accurate representation of the posterior distribution, as is suggested by the histograms in the upper row of Figure 7.4. What differs is the efficiency of achieving a good approximation. (p. 160)</p>
</blockquote>
</div>
<div id="summary-of-metropolis-algorithm." class="section level3">
<h3><span class="header-section-number">7.3.2</span> Summary of Metropolis algorithm.</h3>
<blockquote>
<p>The motivation for methods like the Metropolis algorithm is that they provide a high-resolution picture of the posterior distribution, even though in complex models we cannot explicitly solve the mathematical integral in Bayes’ rule. The idea is that we get a handle on the posterior distribution by generating a large sample of representative values. The larger the sample, the more accurate is our approximation. As emphasized previously, this is a sample of representative credible parameter values from the posterior distribution; it is not a resampling of data (there is a fixed data set).</p>
<p>The cleverness of the method is that representative parameter values can be randomly sampled from complicated posterior distributions without solving the integral in Bayes’ rule, and by using only simple proposal distributions for which efficient random number generators already exist. (p. 161)</p>
</blockquote>
</div>
</div>
<div id="toward-gibbs-sampling-estimating-two-coin-biases" class="section level2">
<h2><span class="header-section-number">7.4</span> Toward Gibbs sampling: Estimating two coin biases</h2>
<p>“The Metropolis method is very useful, but it can be inefficient. Other methods can be more efficient in some situations” (p. 162).</p>
<div id="prior-likelihood-and-posterior-for-two-biases." class="section level3">
<h3><span class="header-section-number">7.4.1</span> Prior, likelihood and posterior for two biases.</h3>
<blockquote>
<p>We are considering situations in which there are two underlying biases, namely <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, for the two coins. We are trying to determine what we should believe about these biases after we have observed some data from the two coins. Recall that [Kruschke used] the term “bias” as the name of the parameter <span class="math inline">\(\theta\)</span>, and not to indicate that the value of <span class="math inline">\(\theta\)</span> deviates from 0.5….</p>
<p>What we have to do next is specify a particular mathematical form for the prior distribution. We will work through the mathematics of a particular case for two reasons: First, it will allow us to explore graphical displays of two-dimensional parameter spaces, which will inform our intuitions about Bayes’ rule and sampling from the posterior distribution. Second, the mathematics will set the stage for a specific example of Gibbs sampling. Later in the book when we do applied Bayesian analysis, we will <em>not</em> be doing any of this sort of mathematics. We are doing the math now, for simple cases, to understand how the methods work so we can properly interpret their outputs in realistically complex cases. (pp. 163–165, <em>emphasis</em> in the original)</p>
</blockquote>
</div>
<div id="the-posterior-via-exact-formal-analysis." class="section level3">
<h3><span class="header-section-number">7.4.2</span> The posterior via exact formal analysis.</h3>
<p>The plots in the left column of Figure 7.5 are outside of my skill set. I believe they are referred to as wireframe plots and it’s my understanding that <strong>ggplot2</strong> does not support wireframe plots at this time. However, I can reproduce versions of the right hand column. For our initial attempt for the upper right corner, we’ll simulate.</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="markov-chain-monte-carlo.html#cb501-1"></a><span class="kw">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb501-2"><a href="markov-chain-monte-carlo.html#cb501-2"></a></span>
<span id="cb501-3"><a href="markov-chain-monte-carlo.html#cb501-3"></a>betas &lt;-</span>
<span id="cb501-4"><a href="markov-chain-monte-carlo.html#cb501-4"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">theta_1 =</span> <span class="kw">rbeta</span>(<span class="fl">1e5</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>),</span>
<span id="cb501-5"><a href="markov-chain-monte-carlo.html#cb501-5"></a>         <span class="dt">theta_2 =</span> <span class="kw">rbeta</span>(<span class="fl">1e5</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>))</span>
<span id="cb501-6"><a href="markov-chain-monte-carlo.html#cb501-6"></a></span>
<span id="cb501-7"><a href="markov-chain-monte-carlo.html#cb501-7"></a>betas <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb501-8"><a href="markov-chain-monte-carlo.html#cb501-8"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb501-9"><a href="markov-chain-monte-carlo.html#cb501-9"></a><span class="st">  </span><span class="kw">stat_density_2d</span>() <span class="op">+</span></span>
<span id="cb501-10"><a href="markov-chain-monte-carlo.html#cb501-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]),</span>
<span id="cb501-11"><a href="markov-chain-monte-carlo.html#cb501-11"></a>       <span class="dt">y =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>])) <span class="op">+</span></span>
<span id="cb501-12"><a href="markov-chain-monte-carlo.html#cb501-12"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb501-13"><a href="markov-chain-monte-carlo.html#cb501-13"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/betas-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Instead of the contour lines, one might use color to depict the density variable.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="markov-chain-monte-carlo.html#cb502-1"></a>betas <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb502-2"><a href="markov-chain-monte-carlo.html#cb502-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>, <span class="dt">fill =</span> <span class="kw">stat</span>(density))) <span class="op">+</span></span>
<span id="cb502-3"><a href="markov-chain-monte-carlo.html#cb502-3"></a><span class="st">  </span><span class="kw">stat_density_2d</span>(<span class="dt">geom =</span> <span class="st">&quot;raster&quot;</span>, <span class="dt">contour =</span> F) <span class="op">+</span></span>
<span id="cb502-4"><a href="markov-chain-monte-carlo.html#cb502-4"></a><span class="st">  </span><span class="kw">scale_fill_viridis_c</span>(<span class="dt">option =</span> <span class="st">&quot;A&quot;</span>) <span class="op">+</span></span>
<span id="cb502-5"><a href="markov-chain-monte-carlo.html#cb502-5"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="kw">expression</span>(theta[<span class="dv">1</span>]),</span>
<span id="cb502-6"><a href="markov-chain-monte-carlo.html#cb502-6"></a>       <span class="dt">y =</span> <span class="kw">expression</span>(theta[<span class="dv">2</span>])) <span class="op">+</span></span>
<span id="cb502-7"><a href="markov-chain-monte-carlo.html#cb502-7"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb502-8"><a href="markov-chain-monte-carlo.html#cb502-8"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Remember how we talked about suppressing the unsightly white space between the bottom of bar-plot bars and the <span class="math inline">\(x\)</span>-axis? Well, look at all that unsightly white space between the axes and the boundaries of the parameter space in our bivariate Beta plot. We can further flex our <code>expansion()</code> skills to get rid of those in the next plot. Speaking of which, we might make a more precise version of that plot with careful use of <code>dbeta()</code>.</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="markov-chain-monte-carlo.html#cb503-1"></a>theta_sequence &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">.01</span>)</span>
<span id="cb503-2"><a href="markov-chain-monte-carlo.html#cb503-2"></a></span>
<span id="cb503-3"><a href="markov-chain-monte-carlo.html#cb503-3"></a><span class="kw">tibble</span>(<span class="dt">theta_1 =</span> theta_sequence,</span>
<span id="cb503-4"><a href="markov-chain-monte-carlo.html#cb503-4"></a>       <span class="dt">theta_2 =</span> theta_sequence) <span class="op">%&gt;%</span></span>
<span id="cb503-5"><a href="markov-chain-monte-carlo.html#cb503-5"></a><span class="st">  </span></span>
<span id="cb503-6"><a href="markov-chain-monte-carlo.html#cb503-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior_1 =</span> <span class="kw">dbeta</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>),</span>
<span id="cb503-7"><a href="markov-chain-monte-carlo.html#cb503-7"></a>         <span class="dt">prior_2 =</span> <span class="kw">dbeta</span>(<span class="dt">x =</span> theta_<span class="dv">2</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb503-8"><a href="markov-chain-monte-carlo.html#cb503-8"></a><span class="st">    </span></span>
<span id="cb503-9"><a href="markov-chain-monte-carlo.html#cb503-9"></a><span class="st">  </span><span class="kw">expand</span>(<span class="kw">nesting</span>(theta_<span class="dv">1</span>, prior_<span class="dv">1</span>), <span class="kw">nesting</span>(theta_<span class="dv">2</span>, prior_<span class="dv">2</span>)) <span class="op">%&gt;%</span></span>
<span id="cb503-10"><a href="markov-chain-monte-carlo.html#cb503-10"></a><span class="st">  </span></span>
<span id="cb503-11"><a href="markov-chain-monte-carlo.html#cb503-11"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>, <span class="dt">fill =</span> prior_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>prior_<span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb503-12"><a href="markov-chain-monte-carlo.html#cb503-12"></a><span class="st">  </span><span class="kw">geom_tile</span>() <span class="op">+</span></span>
<span id="cb503-13"><a href="markov-chain-monte-carlo.html#cb503-13"></a><span class="st">  </span><span class="kw">scale_fill_viridis_c</span>(<span class="st">&quot;joint prior density&quot;</span>, <span class="dt">option =</span> <span class="st">&quot;A&quot;</span>) <span class="op">+</span></span>
<span id="cb503-14"><a href="markov-chain-monte-carlo.html#cb503-14"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb503-15"><a href="markov-chain-monte-carlo.html#cb503-15"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb503-16"><a href="markov-chain-monte-carlo.html#cb503-16"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb503-17"><a href="markov-chain-monte-carlo.html#cb503-17"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-26-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Look at that–no more unsightly white space! We’ll need the <code>bernoulli_likelihood()</code> function from back in Chapter 6 for the middle right of Figure 7.5.</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="markov-chain-monte-carlo.html#cb504-1"></a>bernoulli_likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta, data) {</span>
<span id="cb504-2"><a href="markov-chain-monte-carlo.html#cb504-2"></a>  </span>
<span id="cb504-3"><a href="markov-chain-monte-carlo.html#cb504-3"></a>  <span class="co"># theta = success probability parameter ranging from 0 to 1</span></span>
<span id="cb504-4"><a href="markov-chain-monte-carlo.html#cb504-4"></a>  <span class="co"># data = the vector of data (i.e., a series of 0s and 1s)</span></span>
<span id="cb504-5"><a href="markov-chain-monte-carlo.html#cb504-5"></a>  n &lt;-<span class="st"> </span><span class="kw">length</span>(data)</span>
<span id="cb504-6"><a href="markov-chain-monte-carlo.html#cb504-6"></a>  z &lt;-<span class="st"> </span><span class="kw">sum</span>(data)</span>
<span id="cb504-7"><a href="markov-chain-monte-carlo.html#cb504-7"></a>  </span>
<span id="cb504-8"><a href="markov-chain-monte-carlo.html#cb504-8"></a>  <span class="kw">return</span>(theta<span class="op">^</span>z <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta)<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(data)))</span>
<span id="cb504-9"><a href="markov-chain-monte-carlo.html#cb504-9"></a>  </span>
<span id="cb504-10"><a href="markov-chain-monte-carlo.html#cb504-10"></a>}</span></code></pre></div>
<p>With our trusty <code>bernoulli_likelihood()</code> function in hand, we can now make a version of the middle right panel of Figure 7.5.</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="markov-chain-monte-carlo.html#cb505-1"></a>theta_<span class="dv">1</span>_data &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">times =</span> <span class="kw">c</span>(<span class="dv">8</span> <span class="op">-</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb505-2"><a href="markov-chain-monte-carlo.html#cb505-2"></a>theta_<span class="dv">2</span>_data &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">times =</span> <span class="kw">c</span>(<span class="dv">7</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb505-3"><a href="markov-chain-monte-carlo.html#cb505-3"></a></span>
<span id="cb505-4"><a href="markov-chain-monte-carlo.html#cb505-4"></a><span class="kw">tibble</span>(<span class="dt">theta_1 =</span> theta_sequence,</span>
<span id="cb505-5"><a href="markov-chain-monte-carlo.html#cb505-5"></a>       <span class="dt">theta_2 =</span> theta_sequence) <span class="op">%&gt;%</span></span>
<span id="cb505-6"><a href="markov-chain-monte-carlo.html#cb505-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">likelihood_1 =</span> <span class="kw">bernoulli_likelihood</span>(<span class="dt">theta =</span> theta_sequence,</span>
<span id="cb505-7"><a href="markov-chain-monte-carlo.html#cb505-7"></a>                                             <span class="dt">data  =</span> theta_<span class="dv">1</span>_data),</span>
<span id="cb505-8"><a href="markov-chain-monte-carlo.html#cb505-8"></a>         <span class="dt">likelihood_2 =</span> <span class="kw">bernoulli_likelihood</span>(<span class="dt">theta =</span> theta_sequence,</span>
<span id="cb505-9"><a href="markov-chain-monte-carlo.html#cb505-9"></a>                                             <span class="dt">data  =</span> theta_<span class="dv">2</span>_data)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb505-10"><a href="markov-chain-monte-carlo.html#cb505-10"></a><span class="st">  </span><span class="kw">expand</span>(<span class="kw">nesting</span>(theta_<span class="dv">1</span>, likelihood_<span class="dv">1</span>), <span class="kw">nesting</span>(theta_<span class="dv">2</span>, likelihood_<span class="dv">2</span>)) <span class="op">%&gt;%</span></span>
<span id="cb505-11"><a href="markov-chain-monte-carlo.html#cb505-11"></a><span class="st">  </span></span>
<span id="cb505-12"><a href="markov-chain-monte-carlo.html#cb505-12"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>, <span class="dt">fill =</span> likelihood_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>likelihood_<span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb505-13"><a href="markov-chain-monte-carlo.html#cb505-13"></a><span class="st">  </span><span class="kw">geom_tile</span>() <span class="op">+</span></span>
<span id="cb505-14"><a href="markov-chain-monte-carlo.html#cb505-14"></a><span class="st">  </span><span class="kw">scale_fill_viridis_c</span>(<span class="st">&quot;joint likelihood&quot;</span>, <span class="dt">option =</span> <span class="st">&quot;A&quot;</span>) <span class="op">+</span></span>
<span id="cb505-15"><a href="markov-chain-monte-carlo.html#cb505-15"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb505-16"><a href="markov-chain-monte-carlo.html#cb505-16"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb505-17"><a href="markov-chain-monte-carlo.html#cb505-17"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb505-18"><a href="markov-chain-monte-carlo.html#cb505-18"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-28-1.png" width="528" style="display: block; margin: auto;" /></p>
<p>Here’s the two-dimensional posterior, the lower right panel of Figure 7.5.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="markov-chain-monte-carlo.html#cb506-1"></a><span class="co"># we&#39;ve already defined these, but here they are again</span></span>
<span id="cb506-2"><a href="markov-chain-monte-carlo.html#cb506-2"></a>theta_sequence &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="dv">1</span>, <span class="dt">by =</span> <span class="fl">.01</span>)</span>
<span id="cb506-3"><a href="markov-chain-monte-carlo.html#cb506-3"></a>theta_<span class="dv">1</span>_data   &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">times =</span> <span class="kw">c</span>(<span class="dv">8</span> <span class="op">-</span><span class="st"> </span><span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb506-4"><a href="markov-chain-monte-carlo.html#cb506-4"></a>theta_<span class="dv">2</span>_data   &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">times =</span> <span class="kw">c</span>(<span class="dv">7</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb506-5"><a href="markov-chain-monte-carlo.html#cb506-5"></a></span>
<span id="cb506-6"><a href="markov-chain-monte-carlo.html#cb506-6"></a><span class="co"># this is a redo from two plots up, but saved as `d_prior`</span></span>
<span id="cb506-7"><a href="markov-chain-monte-carlo.html#cb506-7"></a>d_prior &lt;-</span>
<span id="cb506-8"><a href="markov-chain-monte-carlo.html#cb506-8"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">theta_1 =</span> theta_sequence,</span>
<span id="cb506-9"><a href="markov-chain-monte-carlo.html#cb506-9"></a>         <span class="dt">theta_2 =</span> theta_sequence) <span class="op">%&gt;%</span></span>
<span id="cb506-10"><a href="markov-chain-monte-carlo.html#cb506-10"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">prior_1 =</span> <span class="kw">dbeta</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>),</span>
<span id="cb506-11"><a href="markov-chain-monte-carlo.html#cb506-11"></a>         <span class="dt">prior_2 =</span> <span class="kw">dbeta</span>(<span class="dt">x =</span> theta_<span class="dv">2</span>, <span class="dt">shape1 =</span> <span class="dv">2</span>, <span class="dt">shape2 =</span> <span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-12"><a href="markov-chain-monte-carlo.html#cb506-12"></a><span class="st">  </span><span class="kw">expand</span>(<span class="kw">nesting</span>(theta_<span class="dv">1</span>, prior_<span class="dv">1</span>), <span class="kw">nesting</span>(theta_<span class="dv">2</span>, prior_<span class="dv">2</span>))</span>
<span id="cb506-13"><a href="markov-chain-monte-carlo.html#cb506-13"></a></span>
<span id="cb506-14"><a href="markov-chain-monte-carlo.html#cb506-14"></a><span class="co"># this is a redo from one plot up, but saved as `d_likelihood`</span></span>
<span id="cb506-15"><a href="markov-chain-monte-carlo.html#cb506-15"></a>d_likelihood &lt;-</span>
<span id="cb506-16"><a href="markov-chain-monte-carlo.html#cb506-16"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">theta_1 =</span> theta_sequence,</span>
<span id="cb506-17"><a href="markov-chain-monte-carlo.html#cb506-17"></a>         <span class="dt">theta_2 =</span> theta_sequence) <span class="op">%&gt;%</span></span>
<span id="cb506-18"><a href="markov-chain-monte-carlo.html#cb506-18"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">likelihood_1 =</span> <span class="kw">bernoulli_likelihood</span>(<span class="dt">theta =</span> theta_sequence,</span>
<span id="cb506-19"><a href="markov-chain-monte-carlo.html#cb506-19"></a>                                             <span class="dt">data  =</span> theta_<span class="dv">1</span>_data),</span>
<span id="cb506-20"><a href="markov-chain-monte-carlo.html#cb506-20"></a>         <span class="dt">likelihood_2 =</span> <span class="kw">bernoulli_likelihood</span>(<span class="dt">theta =</span> theta_sequence,</span>
<span id="cb506-21"><a href="markov-chain-monte-carlo.html#cb506-21"></a>                                             <span class="dt">data  =</span> theta_<span class="dv">2</span>_data)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-22"><a href="markov-chain-monte-carlo.html#cb506-22"></a><span class="st">  </span><span class="kw">expand</span>(<span class="kw">nesting</span>(theta_<span class="dv">1</span>, likelihood_<span class="dv">1</span>), <span class="kw">nesting</span>(theta_<span class="dv">2</span>, likelihood_<span class="dv">2</span>))</span>
<span id="cb506-23"><a href="markov-chain-monte-carlo.html#cb506-23"></a></span>
<span id="cb506-24"><a href="markov-chain-monte-carlo.html#cb506-24"></a><span class="co"># here we combine `d_prior` and `d_likelihood`</span></span>
<span id="cb506-25"><a href="markov-chain-monte-carlo.html#cb506-25"></a>d_prior <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-26"><a href="markov-chain-monte-carlo.html#cb506-26"></a><span class="st">  </span><span class="kw">left_join</span>(d_likelihood, <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;theta_1&quot;</span>, <span class="st">&quot;theta_2&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-27"><a href="markov-chain-monte-carlo.html#cb506-27"></a><span class="st">  </span><span class="co"># we need the marginal likelihood, the denominator in Bayes&#39; rule</span></span>
<span id="cb506-28"><a href="markov-chain-monte-carlo.html#cb506-28"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">marginal_likelihood =</span> <span class="kw">sum</span>(prior_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>prior_<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>likelihood_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>likelihood_<span class="dv">2</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-29"><a href="markov-chain-monte-carlo.html#cb506-29"></a><span class="st">  </span><span class="co"># finally, the two-dimensional posterior</span></span>
<span id="cb506-30"><a href="markov-chain-monte-carlo.html#cb506-30"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">posterior =</span> (prior_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>prior_<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>likelihood_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>likelihood_<span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>marginal_likelihood) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb506-31"><a href="markov-chain-monte-carlo.html#cb506-31"></a><span class="st">  </span></span>
<span id="cb506-32"><a href="markov-chain-monte-carlo.html#cb506-32"></a><span class="st">  </span><span class="co"># plot!</span></span>
<span id="cb506-33"><a href="markov-chain-monte-carlo.html#cb506-33"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>, <span class="dt">fill =</span> posterior)) <span class="op">+</span></span>
<span id="cb506-34"><a href="markov-chain-monte-carlo.html#cb506-34"></a><span class="st">  </span><span class="kw">geom_tile</span>() <span class="op">+</span></span>
<span id="cb506-35"><a href="markov-chain-monte-carlo.html#cb506-35"></a><span class="st">  </span><span class="kw">scale_fill_viridis_c</span>(<span class="kw">expression</span>(<span class="kw">italic</span>(p)(theta[<span class="dv">1</span>]<span class="op">*</span><span class="st">&#39;, &#39;</span><span class="op">*</span>theta[<span class="dv">2</span>]<span class="op">*</span><span class="st">&#39;|&#39;</span><span class="op">*</span>D)), <span class="dt">option =</span> <span class="st">&quot;A&quot;</span>) <span class="op">+</span></span>
<span id="cb506-36"><a href="markov-chain-monte-carlo.html#cb506-36"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb506-37"><a href="markov-chain-monte-carlo.html#cb506-37"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb506-38"><a href="markov-chain-monte-carlo.html#cb506-38"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb506-39"><a href="markov-chain-monte-carlo.html#cb506-39"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-29-1.png" width="432" style="display: block; margin: auto;" /></p>
<p>That last plot, my friends, is a depiction of</p>
<p><span class="math display">\[p(\theta_1, \theta_2 | D) = \frac{p(D | \theta_1, \theta_2) p(\theta_1, \theta_2)}{p(D)}.\]</span></p>
</div>
<div id="the-posterior-via-the-metropolis-algorithm." class="section level3">
<h3><span class="header-section-number">7.4.3</span> The posterior via the Metropolis algorithm.</h3>
<p>I’ve got nothing on this. But we’re here to learn HMC anyways. Read on.</p>
</div>
<div id="gibbs-hamiltonian-monte-carlo-sampling." class="section level3">
<h3><span class="header-section-number">7.4.4</span> <del>Gibbs</del> Hamiltonian Monte Carlo sampling.</h3>
<p>Figure 7.7 is still out of my skill set. But let’s fit the model with our primary package, <strong>brms</strong>. First we need to load <strong>brms</strong>.</p>
<div class="sourceCode" id="cb507"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb507-1"><a href="markov-chain-monte-carlo.html#cb507-1"></a><span class="kw">library</span>(brms)</span></code></pre></div>
<p>These, recall, are the data.</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="markov-chain-monte-carlo.html#cb508-1"></a>d &lt;-</span>
<span id="cb508-2"><a href="markov-chain-monte-carlo.html#cb508-2"></a><span class="st">  </span><span class="kw">tibble</span>(<span class="dt">z1 =</span> <span class="dv">6</span>, </span>
<span id="cb508-3"><a href="markov-chain-monte-carlo.html#cb508-3"></a>         <span class="dt">z2 =</span> <span class="dv">2</span>,</span>
<span id="cb508-4"><a href="markov-chain-monte-carlo.html#cb508-4"></a>         <span class="dt">n1 =</span> <span class="dv">8</span>,</span>
<span id="cb508-5"><a href="markov-chain-monte-carlo.html#cb508-5"></a>         <span class="dt">n2 =</span> <span class="dv">7</span>)</span>
<span id="cb508-6"><a href="markov-chain-monte-carlo.html#cb508-6"></a></span>
<span id="cb508-7"><a href="markov-chain-monte-carlo.html#cb508-7"></a>d</span></code></pre></div>
<pre><code>## # A tibble: 1 x 4
##      z1    z2    n1    n2
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     6     2     8     7</code></pre>
<p>Kruschke said he was starting us out simply. But within the <strong>brms</strong> context, this is an intercepts-only multivariate model, which isn’t the simplest of things to code into <strong>brms</strong>. There are a couple ways to code a <a href="https://cran.r-project.org/web/packages/brms/vignettes/brms_multivariate.html">multivariate model in <strong>brms</strong></a> <span class="citation">(Bürkner, <a href="#ref-Bürkner2020Multivariate" role="doc-biblioref">2020</a><a href="#ref-Bürkner2020Multivariate" role="doc-biblioref">g</a>)</span>. With this one, it makes sense to specify the model for each sequence of flips separately. This results in two models, which we’ll call <code>model_1</code> and <code>model_2</code>.</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="markov-chain-monte-carlo.html#cb510-1"></a>model_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">bf</span>(z1 <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(n1) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb510-2"><a href="markov-chain-monte-carlo.html#cb510-2"></a>model_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">bf</span>(z2 <span class="op">|</span><span class="st"> </span><span class="kw">trials</span>(n2) <span class="op">~</span><span class="st"> </span><span class="dv">1</span>)</span></code></pre></div>
<p>Before we fit, we’ll have to address a technicality. The <strong>brms</strong> package does allow for multivariate Bernoulli models. However, it does not support such models with different numbers of trials across the variables. Since our first variable is of 8 trials and the second is of 7, <strong>brms</strong> will not support this model using the Bernoulli likelihood. However, we can fit the model in <strong>brms</strong> as an aggregated binomial<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> model. The main difficulty is that the regularizing <code>beta(2, 2)</code> prior won’t make sense, here. So we’ll opt for the regularizing <code>normal(0, 1)</code>, instead.</p>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="markov-chain-monte-carlo.html#cb511-1"></a>fit7<span class="fl">.1</span> &lt;-</span>
<span id="cb511-2"><a href="markov-chain-monte-carlo.html#cb511-2"></a><span class="st">  </span><span class="kw">brm</span>(<span class="dt">data =</span> d, </span>
<span id="cb511-3"><a href="markov-chain-monte-carlo.html#cb511-3"></a>      <span class="dt">family =</span> <span class="kw">binomial</span>(),</span>
<span id="cb511-4"><a href="markov-chain-monte-carlo.html#cb511-4"></a>      model_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>model_<span class="dv">2</span>,</span>
<span id="cb511-5"><a href="markov-chain-monte-carlo.html#cb511-5"></a>      <span class="kw">prior</span>(<span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb511-6"><a href="markov-chain-monte-carlo.html#cb511-6"></a>      <span class="dt">iter =</span> <span class="dv">25500</span>, <span class="dt">warmup =</span> <span class="dv">500</span>, <span class="dt">cores =</span> <span class="dv">1</span>, <span class="dt">chains =</span> <span class="dv">1</span>,</span>
<span id="cb511-7"><a href="markov-chain-monte-carlo.html#cb511-7"></a>      <span class="dt">seed =</span> <span class="dv">7</span>,</span>
<span id="cb511-8"><a href="markov-chain-monte-carlo.html#cb511-8"></a>      <span class="dt">file =</span> <span class="st">&quot;fits/fit07.01&quot;</span>)</span></code></pre></div>
<p>Here is a summary of the results.</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="markov-chain-monte-carlo.html#cb512-1"></a><span class="kw">print</span>(fit7<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>##  Family: MV(binomial, binomial) 
##   Links: mu = logit
##          mu = logit 
## Formula: z1 | trials(n1) ~ 1 
##          z2 | trials(n2) ~ 1 
##    Data: d (Number of observations: 1) 
## Samples: 1 chains, each with iter = 25500; warmup = 500; thin = 1;
##          total post-warmup samples = 25000
## 
## Population-Level Effects: 
##              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## z1_Intercept     0.72      0.61    -0.42     1.95 1.00    18953    16018
## z2_Intercept    -0.59      0.63    -1.84     0.64 1.00    23669    17197
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>As we’ll learn in later chapters, the parameters of a typical aggregated binomial model are in the log-odds scale. Over time, you will learn how to interpret them. But for now, just be happy that <strong>brms</strong> offers the <code>inv_logit_scaled()</code> function, with which we might convert our results back to the probability scale.</p>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="markov-chain-monte-carlo.html#cb514-1"></a><span class="kw">fixef</span>(fit7<span class="fl">.1</span>)[, <span class="dv">1</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()</span></code></pre></div>
<pre><code>## z1_Intercept z2_Intercept 
##    0.6728984    0.3577065</code></pre>
<p>Here we’ll use <code>posterior_samples()</code> to collect out posterior draws and save them as a data frame, which we’ll name <code>post</code>.</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="markov-chain-monte-carlo.html#cb516-1"></a>post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit7<span class="fl">.1</span>, <span class="dt">add_chain =</span> T)</span></code></pre></div>
<p>With <code>post</code> in hand, we’re ready to make our version of Figure 7.8. To reduce the overplotting, we’re only looking at the first 500 post-warmup iterations.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="markov-chain-monte-carlo.html#cb517-1"></a>post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb517-2"><a href="markov-chain-monte-carlo.html#cb517-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">theta_1 =</span> b_z1_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>(), </span>
<span id="cb517-3"><a href="markov-chain-monte-carlo.html#cb517-3"></a>         <span class="dt">theta_2 =</span> b_z2_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb517-4"><a href="markov-chain-monte-carlo.html#cb517-4"></a><span class="st">  </span><span class="kw">filter</span>(iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1001</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb517-5"><a href="markov-chain-monte-carlo.html#cb517-5"></a><span class="st">  </span></span>
<span id="cb517-6"><a href="markov-chain-monte-carlo.html#cb517-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta_<span class="dv">1</span>, <span class="dt">y =</span> theta_<span class="dv">2</span>)) <span class="op">+</span></span>
<span id="cb517-7"><a href="markov-chain-monte-carlo.html#cb517-7"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">4</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb517-8"><a href="markov-chain-monte-carlo.html#cb517-8"></a><span class="st">  </span><span class="kw">geom_path</span>(<span class="dt">size =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">10</span>, <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">color =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb517-9"><a href="markov-chain-monte-carlo.html#cb517-9"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">1</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb517-10"><a href="markov-chain-monte-carlo.html#cb517-10"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">2</span>]), <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="op">+</span></span>
<span id="cb517-11"><a href="markov-chain-monte-carlo.html#cb517-11"></a><span class="st">  </span><span class="kw">coord_equal</span>() <span class="op">+</span></span>
<span id="cb517-12"><a href="markov-chain-monte-carlo.html#cb517-12"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-36-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Just for kicks and giggles, we’ll plot the marginal posterior densities. You’ll note that even though we didn’t use beta priors, the posteriors look quite beta like.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="markov-chain-monte-carlo.html#cb518-1"></a>post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb518-2"><a href="markov-chain-monte-carlo.html#cb518-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">theta[1]</span><span class="st">`</span> =<span class="st"> </span>b_z1_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>(), </span>
<span id="cb518-3"><a href="markov-chain-monte-carlo.html#cb518-3"></a>         <span class="st">`</span><span class="dt">theta[2]</span><span class="st">`</span> =<span class="st"> </span>b_z2_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb518-4"><a href="markov-chain-monte-carlo.html#cb518-4"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="st">`</span><span class="dt">theta[1]</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">theta[2]</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb518-5"><a href="markov-chain-monte-carlo.html#cb518-5"></a><span class="st">  </span></span>
<span id="cb518-6"><a href="markov-chain-monte-carlo.html#cb518-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">y =</span> name)) <span class="op">+</span></span>
<span id="cb518-7"><a href="markov-chain-monte-carlo.html#cb518-7"></a><span class="st">  </span><span class="kw">stat_halfeye</span>(<span class="dt">point_interval =</span> mode_hdi, <span class="dt">.width =</span> <span class="kw">c</span>(.<span class="dv">5</span>, <span class="fl">.95</span>), <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb518-8"><a href="markov-chain-monte-carlo.html#cb518-8"></a><span class="st">  </span><span class="kw">scale_y_discrete</span>(<span class="ot">NULL</span>, <span class="dt">labels =</span> ggplot2<span class="op">:::</span>parse_safe, <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb518-9"><a href="markov-chain-monte-carlo.html#cb518-9"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;posterior&quot;</span>) <span class="op">+</span></span>
<span id="cb518-10"><a href="markov-chain-monte-carlo.html#cb518-10"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-37-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Did you see how our use of <code>expand = expansion(mult = 0)</code> didn’t work so well with this plot? The problem is the mode dot and HDI lines at the base of the lower density gets cut off when you remove all of the white space underneath the density. A partial solution is to adjust the value within <code>expansion()</code> to be small, but just large enough to let the mode and HDI marks breathe.</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="markov-chain-monte-carlo.html#cb519-1"></a>post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb519-2"><a href="markov-chain-monte-carlo.html#cb519-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="st">`</span><span class="dt">theta[1]</span><span class="st">`</span> =<span class="st"> </span>b_z1_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>(), </span>
<span id="cb519-3"><a href="markov-chain-monte-carlo.html#cb519-3"></a>         <span class="st">`</span><span class="dt">theta[2]</span><span class="st">`</span> =<span class="st"> </span>b_z2_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb519-4"><a href="markov-chain-monte-carlo.html#cb519-4"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="st">`</span><span class="dt">theta[1]</span><span class="st">`</span><span class="op">:</span><span class="st">`</span><span class="dt">theta[2]</span><span class="st">`</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb519-5"><a href="markov-chain-monte-carlo.html#cb519-5"></a><span class="st">  </span></span>
<span id="cb519-6"><a href="markov-chain-monte-carlo.html#cb519-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> value, <span class="dt">y =</span> name)) <span class="op">+</span></span>
<span id="cb519-7"><a href="markov-chain-monte-carlo.html#cb519-7"></a><span class="st">  </span><span class="kw">stat_halfeye</span>(<span class="dt">point_interval =</span> mode_hdi, <span class="dt">.width =</span> <span class="kw">c</span>(.<span class="dv">5</span>, <span class="fl">.95</span>), <span class="dt">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="op">+</span></span>
<span id="cb519-8"><a href="markov-chain-monte-carlo.html#cb519-8"></a><span class="st">  </span><span class="kw">scale_y_discrete</span>(<span class="ot">NULL</span>, <span class="dt">labels =</span> ggplot2<span class="op">:::</span>parse_safe, <span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="fl">0.05</span>)) <span class="op">+</span></span>
<span id="cb519-9"><a href="markov-chain-monte-carlo.html#cb519-9"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;posterior&quot;</span>) <span class="op">+</span></span>
<span id="cb519-10"><a href="markov-chain-monte-carlo.html#cb519-10"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-38-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="is-there-a-difference-between-biases" class="section level3">
<h3><span class="header-section-number">7.4.5</span> Is there a difference between biases?</h3>
<p>The difference distribution from our <strong>brms</strong>-based multivariate aggregated binomial model, <span class="math inline">\(\theta_1 - \theta_2\)</span>, is pretty similar to the ones in Figure 7.9.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="markov-chain-monte-carlo.html#cb520-1"></a>post <span class="op">%&gt;%</span><span class="st">   </span></span>
<span id="cb520-2"><a href="markov-chain-monte-carlo.html#cb520-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">theta_1 =</span> b_z1_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>(), </span>
<span id="cb520-3"><a href="markov-chain-monte-carlo.html#cb520-3"></a>         <span class="dt">theta_2 =</span> b_z2_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb520-4"><a href="markov-chain-monte-carlo.html#cb520-4"></a><span class="st">  </span><span class="kw">transmute</span>(<span class="st">`</span><span class="dt">theta_1 - theta_2</span><span class="st">`</span> =<span class="st"> </span>theta_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta_<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb520-5"><a href="markov-chain-monte-carlo.html#cb520-5"></a><span class="st">  </span></span>
<span id="cb520-6"><a href="markov-chain-monte-carlo.html#cb520-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="st">`</span><span class="dt">theta_1 - theta_2</span><span class="st">`</span>, <span class="dt">y =</span> <span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb520-7"><a href="markov-chain-monte-carlo.html#cb520-7"></a><span class="st">  </span><span class="kw">stat_histinterval</span>(<span class="dt">point_interval =</span> mode_hdi, <span class="dt">.width =</span> <span class="fl">.95</span>,</span>
<span id="cb520-8"><a href="markov-chain-monte-carlo.html#cb520-8"></a>                    <span class="dt">fill =</span> <span class="st">&quot;steelblue2&quot;</span>, <span class="dt">slab_color =</span> <span class="st">&quot;steelblue4&quot;</span>, <span class="dt">outline_bars =</span> T,</span>
<span id="cb520-9"><a href="markov-chain-monte-carlo.html#cb520-9"></a>                    <span class="dt">breaks =</span> <span class="dv">40</span>, <span class="dt">normalize =</span> <span class="st">&quot;panels&quot;</span>) <span class="op">+</span></span>
<span id="cb520-10"><a href="markov-chain-monte-carlo.html#cb520-10"></a><span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="kw">expression</span>(theta[<span class="dv">1</span>]<span class="op">-</span>theta[<span class="dv">2</span>]), <span class="dt">limits =</span> <span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>, <span class="fl">.9</span>)) <span class="op">+</span></span>
<span id="cb520-11"><a href="markov-chain-monte-carlo.html#cb520-11"></a><span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="ot">NULL</span>, <span class="dt">breaks =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb520-12"><a href="markov-chain-monte-carlo.html#cb520-12"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-39-1.png" width="312" style="display: block; margin: auto;" /></p>
<p>Here are the exact estimates of the mode and 95% HDIs for our difference distribution, <span class="math inline">\(\theta_1 - \theta_2\)</span>.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="markov-chain-monte-carlo.html#cb521-1"></a>post <span class="op">%&gt;%</span><span class="st">   </span></span>
<span id="cb521-2"><a href="markov-chain-monte-carlo.html#cb521-2"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">theta_1 =</span> b_z1_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>(), </span>
<span id="cb521-3"><a href="markov-chain-monte-carlo.html#cb521-3"></a>         <span class="dt">theta_2 =</span> b_z2_Intercept <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inv_logit_scaled</span>()) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb521-4"><a href="markov-chain-monte-carlo.html#cb521-4"></a><span class="st">  </span><span class="kw">transmute</span>(<span class="st">`</span><span class="dt">theta_1 - theta_2</span><span class="st">`</span> =<span class="st"> </span>theta_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>theta_<span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb521-5"><a href="markov-chain-monte-carlo.html#cb521-5"></a><span class="st">  </span><span class="kw">mode_hdi</span>()</span></code></pre></div>
<pre><code>##   theta_1 - theta_2      .lower    .upper .width .point .interval
## 1         0.2974906 -0.07716013 0.6437771   0.95   mode       hdi</code></pre>
<p>Given that we used both a different likelihood function, which necessitated a different prior, I think we did pretty good complimenting the results in the text.</p>
</div>
<div id="terminology-mcmc." class="section level3">
<h3><span class="header-section-number">7.4.6</span> Terminology: MCMC.</h3>
<blockquote>
<p>Any simulation that samples a lot of random values from a distribution is called a Monte Carlo simulation, named after the dice and spinners and shufflings of the famous casino locale. The appellation “Monte Carlo” is attributed <span class="citation">(Eckhardt, <a href="#ref-eckhardtStanUlamJohn1987" role="doc-biblioref">1987</a>)</span> to the mathematicians <a href="https://en.wikipedia.org/wiki/Stanislaw_Ulam">Stanislaw Ulam</a> (1909–1984) and <a href="https://en.wikipedia.org/wiki/John_von_Neumann">John von Neumann</a> (1903–1957). (p. 177)</p>
</blockquote>
<p>In case you didn’t know, <strong>brms</strong> is a user-friendly interface for the <a href="https://mc-stan.org/">Stan probabilistic programing language</a> <span class="citation">(Stan; Carpenter et al., <a href="#ref-carpenterStanProbabilisticProgramming2017" role="doc-biblioref">2017</a>)</span> and Stan is named after Stanislaw Ulam.</p>
</div>
</div>
<div id="mcmc-representativeness-accuracy-and-efficiency" class="section level2">
<h2><span class="header-section-number">7.5</span> MCMC representativeness, accuracy, and efficiency</h2>
<blockquote>
<p>We have three main goals in generating an MCMC sample from the posterior distribution:</p>
<ol style="list-style-type: decimal">
<li>The values in the chain must be <em>representative</em> of the posterior distribution. They should not be unduly influenced by the arbitrary initial value of the chain, and they should fully explore the range of the posterior distribution without getting stuck.</li>
<li>The chain should be of sufficient size so that estimates are <em>accurate</em> and <em>stable</em>. In particular, the estimates of the central tendency (such as median or mode), and the limits of the 95% HDI, should not be much different if the MCMC analysis is run again (using different seed states for the pseudorandom number generators).</li>
<li>The chain should be generated <em>efficiently</em>, with as few steps as possible, so not to exceed our patience or computing power. (p. 178, <em>emphasis</em> in the original)</li>
</ol>
</blockquote>
<div id="mcmc-representativeness." class="section level3">
<h3><span class="header-section-number">7.5.1</span> MCMC representativeness.</h3>
<p>Kruschke defined our data in the note for Figure 7.10.</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb523-1"><a href="markov-chain-monte-carlo.html#cb523-1"></a>z &lt;-<span class="st"> </span><span class="dv">35</span></span>
<span id="cb523-2"><a href="markov-chain-monte-carlo.html#cb523-2"></a>n &lt;-<span class="st"> </span><span class="dv">50</span></span>
<span id="cb523-3"><a href="markov-chain-monte-carlo.html#cb523-3"></a></span>
<span id="cb523-4"><a href="markov-chain-monte-carlo.html#cb523-4"></a>d &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">y =</span> <span class="kw">rep</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">times =</span> <span class="kw">c</span>(n <span class="op">-</span><span class="st"> </span>z, z)))</span></code></pre></div>
<p>Here we fit the model. Note how since we’re just univariate, it’s easy to switch back to directly modeling with the Bernoulli likelihood.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="markov-chain-monte-carlo.html#cb524-1"></a>fit7<span class="fl">.2</span> &lt;-</span>
<span id="cb524-2"><a href="markov-chain-monte-carlo.html#cb524-2"></a><span class="st">  </span><span class="kw">brm</span>(<span class="dt">data =</span> d, </span>
<span id="cb524-3"><a href="markov-chain-monte-carlo.html#cb524-3"></a>      <span class="dt">family =</span> <span class="kw">bernoulli</span>(<span class="dt">link =</span> identity),</span>
<span id="cb524-4"><a href="markov-chain-monte-carlo.html#cb524-4"></a>      y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</span>
<span id="cb524-5"><a href="markov-chain-monte-carlo.html#cb524-5"></a>      <span class="kw">prior</span>(<span class="kw">beta</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">class =</span> Intercept),</span>
<span id="cb524-6"><a href="markov-chain-monte-carlo.html#cb524-6"></a>      <span class="dt">iter =</span> <span class="dv">10000</span>, <span class="dt">warmup =</span> <span class="dv">500</span>, <span class="dt">cores =</span> <span class="dv">3</span>, <span class="dt">chains =</span> <span class="dv">3</span>,</span>
<span id="cb524-7"><a href="markov-chain-monte-carlo.html#cb524-7"></a>      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">adapt_delta =</span> <span class="fl">0.9</span>),</span>
<span id="cb524-8"><a href="markov-chain-monte-carlo.html#cb524-8"></a>      <span class="dt">seed =</span> <span class="dv">7</span>,</span>
<span id="cb524-9"><a href="markov-chain-monte-carlo.html#cb524-9"></a>      <span class="dt">file =</span> <span class="st">&quot;fits/fit07.02&quot;</span>)</span></code></pre></div>
<p>On page 179, Kruschke discussed <em>burn-in</em> steps within the Gibbs framework:</p>
<blockquote>
<p>The preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the <em>burn-in</em> period. For realistic applications, it is routine to apply a burn-in period of several hundred to several thousand steps.</p>
</blockquote>
<p>For each HMC chain, the first <span class="math inline">\(n\)</span> iterations are called “warmups.” In this example, <span class="math inline">\(n = 500\)</span> (i.e., <code>warmup = 500</code>). Within the Stan-HMC paradigm, <a href="https://andrewgelman.com/2017/12/15/burn-vs-warm-iterative-simulation-algorithms/">warmups are somewhat analogous to but not synonymous with burn-in iterations</a> as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. For more on warmup, check out McElreath’s lecture, <a href="https://www.youtube.com/watch?v=13mEekRdOcQ&amp;t=75s&amp;frags=pl%2Cwn">starting here</a> or, for more detail, the <a href="https://mc-stan.org/docs/2_24/reference-manual/hmc-algorithm-parameters.html"><em>HMC Algorithm Parameters</em> section (15.2)</a> of the <em>Stan reference manual</em>, version 2.24 <span class="citation">(Stan Development Team, <a href="#ref-standevelopmentteamStanReferenceManual2020" role="doc-biblioref">2020</a><a href="#ref-standevelopmentteamStanReferenceManual2020" role="doc-biblioref">a</a>)</span>.</p>
<p>It appears that the upshot of all this is some of the packages in the Stan ecosystem don’t make it easy to extract the warmup values. For example, the <code>brms::plot()</code> function excludes them from the trace plot without the option to include them.</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb525-1"><a href="markov-chain-monte-carlo.html#cb525-1"></a><span class="kw">plot</span>(fit7<span class="fl">.2</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-42-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Notice how the <span class="math inline">\(x\)</span>-axis on the trace plot ranges from 0 to 9,500. Now recall that our model code included <code>iter = 10000, warmup = 500</code>. Those 9,500 iterations in the trace plot are excluding the first 500 warmup iterations. This code is a little janky, but if you really want those warmup iterations, you can extract them from the <code>fit7.2</code> object like this.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="markov-chain-monte-carlo.html#cb526-1"></a>warmups &lt;-</span>
<span id="cb526-2"><a href="markov-chain-monte-carlo.html#cb526-2"></a><span class="st">  </span><span class="kw">c</span>(fit7<span class="fl">.2</span><span class="op">$</span>fit<span class="op">@</span>sim<span class="op">$</span>samples[[<span class="dv">1</span>]]<span class="op">$</span>b_Intercept[<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>], </span>
<span id="cb526-3"><a href="markov-chain-monte-carlo.html#cb526-3"></a>    fit7<span class="fl">.2</span><span class="op">$</span>fit<span class="op">@</span>sim<span class="op">$</span>samples[[<span class="dv">2</span>]]<span class="op">$</span>b_Intercept[<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>], </span>
<span id="cb526-4"><a href="markov-chain-monte-carlo.html#cb526-4"></a>    fit7<span class="fl">.2</span><span class="op">$</span>fit<span class="op">@</span>sim<span class="op">$</span>samples[[<span class="dv">3</span>]]<span class="op">$</span>b_Intercept[<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>]) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb526-5"><a href="markov-chain-monte-carlo.html#cb526-5"></a><span class="st">  </span><span class="co"># since these come from lists, here we&#39;ll convert them to a data frame</span></span>
<span id="cb526-6"><a href="markov-chain-monte-carlo.html#cb526-6"></a><span class="st">  </span><span class="kw">as.data.frame</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb526-7"><a href="markov-chain-monte-carlo.html#cb526-7"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">b_Intercept =</span> <span class="st">&quot;.&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb526-8"><a href="markov-chain-monte-carlo.html#cb526-8"></a><span class="st">  </span><span class="co"># we&#39;ll need to recapture the iteration and chain information</span></span>
<span id="cb526-9"><a href="markov-chain-monte-carlo.html#cb526-9"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">iter  =</span> <span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>, <span class="dt">times =</span> <span class="dv">3</span>),</span>
<span id="cb526-10"><a href="markov-chain-monte-carlo.html#cb526-10"></a>         <span class="dt">chain =</span> <span class="kw">factor</span>(<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, <span class="dt">each =</span> <span class="dv">500</span>), </span>
<span id="cb526-11"><a href="markov-chain-monte-carlo.html#cb526-11"></a>                        <span class="dt">levels =</span> <span class="kw">c</span>(<span class="st">&quot;1&quot;</span>, <span class="st">&quot;2&quot;</span>, <span class="st">&quot;3&quot;</span>)))</span>
<span id="cb526-12"><a href="markov-chain-monte-carlo.html#cb526-12"></a></span>
<span id="cb526-13"><a href="markov-chain-monte-carlo.html#cb526-13"></a>warmups <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb526-14"><a href="markov-chain-monte-carlo.html#cb526-14"></a><span class="st">  </span><span class="kw">head</span>()</span></code></pre></div>
<pre><code>##   b_Intercept iter chain
## 1   0.2981264    1     1
## 2   0.2981264    2     1
## 3   0.2981264    3     1
## 4   0.2981264    4     1
## 5   0.2991483    5     1
## 6   0.2937265    6     1</code></pre>
<p>The <a href="https://github.com/stan-dev/bayesplot"><strong>bayesplot</strong> package</a> <span class="citation">(Gabry et al., <a href="#ref-gabry2019visualization" role="doc-biblioref">2019</a>; Gabry &amp; Mahr, <a href="#ref-R-bayesplot" role="doc-biblioref">2019</a>)</span> makes it easier to reproduce some of the plots in Figure 7.10.</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="markov-chain-monte-carlo.html#cb528-1"></a><span class="kw">library</span>(bayesplot)</span></code></pre></div>
<p>We’ll reproduce the upper left panel with <code>mcmc_trace()</code>.</p>
<div class="sourceCode" id="cb529"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb529-1"><a href="markov-chain-monte-carlo.html#cb529-1"></a><span class="kw">mcmc_trace</span>(warmups, <span class="dt">pars =</span> <span class="st">&quot;b_Intercept&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-45-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As an alternative, we can also extract the warmup draws from a <code>brm()</code> fit with the <a href="https://cran.rstudio.com/package=ggmcmc"><strong>ggmcmc</strong> package</a> <span class="citation">(Fernández i Marín, <a href="#ref-fernandezGGMCMCAnalysisofMCMC2016" role="doc-biblioref">2016</a>, <a href="#ref-R-ggmcmc" role="doc-biblioref">2020</a>)</span>.</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="markov-chain-monte-carlo.html#cb530-1"></a><span class="co"># install.packages(&quot;ggmcmc&quot;, dependencies = T)</span></span>
<span id="cb530-2"><a href="markov-chain-monte-carlo.html#cb530-2"></a><span class="kw">library</span>(ggmcmc)</span></code></pre></div>
<p>The <strong>ggmcmc</strong> package has a variety of convenience functions for working with MCMC chains. The <code>ggs()</code> function extracts the posterior draws, including <code>warmup</code>, and arranges them in a tidy tibble. With those in hand, we can now make a trace plot with warmup draws.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="markov-chain-monte-carlo.html#cb531-1"></a><span class="kw">ggs</span>(fit7<span class="fl">.2</span>) <span class="op">%&gt;%</span></span>
<span id="cb531-2"><a href="markov-chain-monte-carlo.html#cb531-2"></a><span class="st">  </span><span class="kw">filter</span>(Iteration <span class="op">&lt;</span><span class="st"> </span><span class="dv">501</span> <span class="op">&amp;</span></span>
<span id="cb531-3"><a href="markov-chain-monte-carlo.html#cb531-3"></a><span class="st">           </span>Parameter <span class="op">==</span><span class="st"> &quot;b_Intercept&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb531-4"><a href="markov-chain-monte-carlo.html#cb531-4"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">chain =</span> <span class="kw">factor</span>(Chain)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb531-5"><a href="markov-chain-monte-carlo.html#cb531-5"></a><span class="st">  </span></span>
<span id="cb531-6"><a href="markov-chain-monte-carlo.html#cb531-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Iteration, <span class="dt">y =</span> value, <span class="dt">color =</span> chain)) <span class="op">+</span></span>
<span id="cb531-7"><a href="markov-chain-monte-carlo.html#cb531-7"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb531-8"><a href="markov-chain-monte-carlo.html#cb531-8"></a><span class="st">  </span><span class="kw">scale_colour_brewer</span>(<span class="dt">direction =</span> <span class="dv">-1</span>) <span class="op">+</span></span>
<span id="cb531-9"><a href="markov-chain-monte-carlo.html#cb531-9"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;My custom trace plots with warmups via ggmcmc::ggs()&quot;</span>,</span>
<span id="cb531-10"><a href="markov-chain-monte-carlo.html#cb531-10"></a>       <span class="dt">x =</span> <span class="ot">NULL</span>, <span class="dt">y =</span> <span class="ot">NULL</span>) <span class="op">+</span></span>
<span id="cb531-11"><a href="markov-chain-monte-carlo.html#cb531-11"></a><span class="st">  </span><span class="kw">theme_cowplot</span>(<span class="dt">font_size =</span> <span class="dv">12</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-47-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>It appears our HMC warmup iterations found the posterior quite quickly. Here’s the autocorrelation plot.</p>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="markov-chain-monte-carlo.html#cb532-1"></a><span class="kw">mcmc_acf</span>(warmups, <span class="dt">pars =</span> <span class="st">&quot;b_Intercept&quot;</span>, <span class="dt">lags =</span> <span class="dv">25</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-48-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Our autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs.</p>
<p>If you were unhappy with the way <code>mcmc_acf()</code> defaults to faceting the plot by chain, you could always extract the data from the function and use them to make the plot the way you prefer. E.g.,</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="markov-chain-monte-carlo.html#cb533-1"></a><span class="kw">mcmc_acf</span>(warmups)<span class="op">$</span>data <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb533-2"><a href="markov-chain-monte-carlo.html#cb533-2"></a><span class="st">  </span><span class="kw">as_tibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb533-3"><a href="markov-chain-monte-carlo.html#cb533-3"></a><span class="st">  </span><span class="kw">filter</span>(Parameter <span class="op">==</span><span class="st"> &quot;b_Intercept&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb533-4"><a href="markov-chain-monte-carlo.html#cb533-4"></a><span class="st">  </span></span>
<span id="cb533-5"><a href="markov-chain-monte-carlo.html#cb533-5"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> Lag, <span class="dt">y =</span> AC,</span>
<span id="cb533-6"><a href="markov-chain-monte-carlo.html#cb533-6"></a>             <span class="dt">color =</span> Chain <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.factor</span>())) <span class="op">+</span></span>
<span id="cb533-7"><a href="markov-chain-monte-carlo.html#cb533-7"></a><span class="st">  </span><span class="kw">geom_hline</span>(<span class="dt">yintercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;white&quot;</span>) <span class="op">+</span></span>
<span id="cb533-8"><a href="markov-chain-monte-carlo.html#cb533-8"></a><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>) <span class="op">+</span></span>
<span id="cb533-9"><a href="markov-chain-monte-carlo.html#cb533-9"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb533-10"><a href="markov-chain-monte-carlo.html#cb533-10"></a><span class="st">  </span><span class="kw">scale_colour_brewer</span>(<span class="dt">direction =</span> <span class="dv">-1</span>) <span class="op">+</span></span>
<span id="cb533-11"><a href="markov-chain-monte-carlo.html#cb533-11"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Autocorrelation&quot;</span>) <span class="op">+</span></span>
<span id="cb533-12"><a href="markov-chain-monte-carlo.html#cb533-12"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb533-13"><a href="markov-chain-monte-carlo.html#cb533-13"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-49-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Here are the overlaid densities.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="markov-chain-monte-carlo.html#cb534-1"></a><span class="kw">mcmc_dens_overlay</span>(warmups, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>))</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-50-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>The densities aren’t great, but they still appear nicer than those in for the burn-in iterations in the text. With our warmups in their current state, I’m not aware how we might conveniently make a shrink factor plot, as seen in the lower left of Figure 7.10. So it goes…</p>
<p>Figure 7.11 examined the post-burn-in iterations. We’ll follow suit with our post-warmup iterations.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="markov-chain-monte-carlo.html#cb535-1"></a>post &lt;-<span class="st"> </span><span class="kw">posterior_samples</span>(fit7<span class="fl">.2</span>, <span class="dt">add_chain =</span> T)</span>
<span id="cb535-2"><a href="markov-chain-monte-carlo.html#cb535-2"></a></span>
<span id="cb535-3"><a href="markov-chain-monte-carlo.html#cb535-3"></a><span class="kw">mcmc_trace</span>(post, <span class="dt">pars =</span> <span class="st">&quot;b_Intercept&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-51-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The autocorrelation plots:</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="markov-chain-monte-carlo.html#cb536-1"></a><span class="kw">mcmc_acf</span>(post, <span class="dt">pars =</span> <span class="st">&quot;b_Intercept&quot;</span>, <span class="dt">lags =</span> <span class="dv">40</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-52-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>As with the warmups, above, the post-warmup autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs.</p>
<p>Here are the overlaid densities.</p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="markov-chain-monte-carlo.html#cb537-1"></a><span class="kw">mcmc_dens_overlay</span>(post, <span class="dt">pars =</span> <span class="kw">c</span>(<span class="st">&quot;b_Intercept&quot;</span>))</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-53-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Now that we’re focusing on the post-warmup iterations, we can make a shrink factor plot. We’ll do so with the <code>coda::gelman.plot()</code> function. But you can’t just dump your <code>brm()</code> fit object into <code>coda::gelman.plot()</code>. It’s the wrong object type. However, <strong>brms</strong> offers the <code>as.mcmc()</code> function which will convert <code>brm()</code> objects for use in coda package functions.</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="markov-chain-monte-carlo.html#cb538-1"></a>fit7<span class="fl">.2</span>_c &lt;-<span class="st"> </span><span class="kw">as.mcmc</span>(fit7<span class="fl">.2</span>)</span>
<span id="cb538-2"><a href="markov-chain-monte-carlo.html#cb538-2"></a></span>
<span id="cb538-3"><a href="markov-chain-monte-carlo.html#cb538-3"></a>fit7<span class="fl">.2</span>_c <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">glimpse</span>()</span></code></pre></div>
<pre><code>## List of 3
##  $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.78 0.822 0.699 0.648 0.702 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ iterations: NULL
##   .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot;
##   ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1
##  $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.76 0.754 0.725 0.725 0.672 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ iterations: NULL
##   .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot;
##   ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1
##  $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.808 0.788 0.803 0.65 0.65 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ iterations: NULL
##   .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot;
##   ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1
##  - attr(*, &quot;class&quot;)= chr &quot;mcmc.list&quot;</code></pre>
<p>With our freshly-converted <code>fit2_c</code> object in hand, we’re ready to plot.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="markov-chain-monte-carlo.html#cb540-1"></a>coda<span class="op">::</span><span class="kw">gelman.plot</span>(fit7<span class="fl">.2</span>_c[, <span class="st">&quot;b_Intercept&quot;</span>, ])</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-55-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Looks great. As Kruschke explained on page 181, that plot is based on the potential scale reduction factor, or <span class="math inline">\(\widehat R\)</span> as it’s typically referred to in the Stan ecosystem. Happily, <strong>brms</strong> reports the <span class="math inline">\(\widehat R\)</span> values for the major model parameters using <code>print()</code> or <code>summary()</code>.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="markov-chain-monte-carlo.html#cb541-1"></a><span class="kw">print</span>(fit7<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>##  Family: bernoulli 
##   Links: mu = identity 
## Formula: y ~ 1 
##    Data: d (Number of observations: 50) 
## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1;
##          total post-warmup samples = 28500
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.69      0.06     0.56     0.80 1.00     9215     9302
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Instead of a running value, you get a single statistic in the ‘Rhat’ column.</p>
<p>On page 181, Kruschke discussed how his overlaid density plots include the HDIs, by chain. The convenience functions from <strong>brms</strong> and <strong>bayesplot</strong> don’t easily get us there. But we can get those easy enough with a little help <code>tidybayes::stat_halfeye()</code>.</p>
<div class="sourceCode" id="cb543"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb543-1"><a href="markov-chain-monte-carlo.html#cb543-1"></a>post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb543-2"><a href="markov-chain-monte-carlo.html#cb543-2"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> b_Intercept, <span class="dt">y =</span> chain, <span class="dt">fill =</span> chain)) <span class="op">+</span></span>
<span id="cb543-3"><a href="markov-chain-monte-carlo.html#cb543-3"></a><span class="st">  </span><span class="kw">stat_halfeye</span>(<span class="dt">point_interval =</span> mode_hdi,</span>
<span id="cb543-4"><a href="markov-chain-monte-carlo.html#cb543-4"></a>                <span class="dt">.width =</span> <span class="fl">.95</span>) <span class="op">+</span></span>
<span id="cb543-5"><a href="markov-chain-monte-carlo.html#cb543-5"></a><span class="st">  </span><span class="kw">scale_fill_brewer</span>(<span class="dt">direction =</span> <span class="dv">-1</span>) <span class="op">+</span></span>
<span id="cb543-6"><a href="markov-chain-monte-carlo.html#cb543-6"></a><span class="st">  </span><span class="kw">scale_y_discrete</span>(<span class="dt">expand =</span> <span class="kw">expansion</span>(<span class="dt">mult =</span> <span class="fl">0.025</span>)) <span class="op">+</span></span>
<span id="cb543-7"><a href="markov-chain-monte-carlo.html#cb543-7"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">+</span></span>
<span id="cb543-8"><a href="markov-chain-monte-carlo.html#cb543-8"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-57-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="mcmc-accuracy." class="section level3">
<h3><span class="header-section-number">7.5.2</span> MCMC accuracy.</h3>
<p>We’ll wrangle our <code>post</code> object a bit to make it easier to reproduce Figure 7.12.</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="markov-chain-monte-carlo.html#cb544-1"></a>lagged_post &lt;-</span>
<span id="cb544-2"><a href="markov-chain-monte-carlo.html#cb544-2"></a><span class="st">  </span>post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb544-3"><a href="markov-chain-monte-carlo.html#cb544-3"></a><span class="st">  </span><span class="kw">filter</span>(chain <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb544-4"><a href="markov-chain-monte-carlo.html#cb544-4"></a><span class="st">  </span><span class="kw">select</span>(b_Intercept, iter) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb544-5"><a href="markov-chain-monte-carlo.html#cb544-5"></a><span class="st">  </span><span class="kw">rename</span>(<span class="dt">lag_0 =</span> b_Intercept) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb544-6"><a href="markov-chain-monte-carlo.html#cb544-6"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lag_1  =</span> <span class="kw">lag</span>(lag_<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb544-7"><a href="markov-chain-monte-carlo.html#cb544-7"></a>         <span class="dt">lag_5  =</span> <span class="kw">lag</span>(lag_<span class="dv">0</span>, <span class="dv">5</span>),</span>
<span id="cb544-8"><a href="markov-chain-monte-carlo.html#cb544-8"></a>         <span class="dt">lag_10 =</span> <span class="kw">lag</span>(lag_<span class="dv">0</span>, <span class="dv">10</span>)) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb544-9"><a href="markov-chain-monte-carlo.html#cb544-9"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="op">-</span>iter,</span>
<span id="cb544-10"><a href="markov-chain-monte-carlo.html#cb544-10"></a>               <span class="dt">names_to =</span> <span class="st">&quot;key&quot;</span>) </span>
<span id="cb544-11"><a href="markov-chain-monte-carlo.html#cb544-11"></a></span>
<span id="cb544-12"><a href="markov-chain-monte-carlo.html#cb544-12"></a><span class="kw">head</span>(lagged_post)</span></code></pre></div>
<pre><code>## # A tibble: 6 x 3
##    iter key     value
##   &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;
## 1   501 lag_0   0.780
## 2   501 lag_1  NA    
## 3   501 lag_5  NA    
## 4   501 lag_10 NA    
## 5   502 lag_0   0.822
## 6   502 lag_1   0.780</code></pre>
<p>Here’s our version of the top row.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="markov-chain-monte-carlo.html#cb546-1"></a>p1 &lt;-</span>
<span id="cb546-2"><a href="markov-chain-monte-carlo.html#cb546-2"></a><span class="st">  </span>lagged_post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-3"><a href="markov-chain-monte-carlo.html#cb546-3"></a><span class="st">  </span><span class="kw">filter</span>(key <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;lag_0&quot;</span>, <span class="st">&quot;lag_1&quot;</span>),</span>
<span id="cb546-4"><a href="markov-chain-monte-carlo.html#cb546-4"></a>         iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-5"><a href="markov-chain-monte-carlo.html#cb546-5"></a><span class="st">  </span></span>
<span id="cb546-6"><a href="markov-chain-monte-carlo.html#cb546-6"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> iter, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) <span class="op">+</span></span>
<span id="cb546-7"><a href="markov-chain-monte-carlo.html#cb546-7"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb546-8"><a href="markov-chain-monte-carlo.html#cb546-8"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb546-9"><a href="markov-chain-monte-carlo.html#cb546-9"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;steelblue&quot;</span>)) <span class="op">+</span></span>
<span id="cb546-10"><a href="markov-chain-monte-carlo.html#cb546-10"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Index 1001:1071&quot;</span>,</span>
<span id="cb546-11"><a href="markov-chain-monte-carlo.html#cb546-11"></a>       <span class="dt">title =</span> <span class="st">&quot;Lag 1&quot;</span>)</span>
<span id="cb546-12"><a href="markov-chain-monte-carlo.html#cb546-12"></a></span>
<span id="cb546-13"><a href="markov-chain-monte-carlo.html#cb546-13"></a>p2 &lt;-</span>
<span id="cb546-14"><a href="markov-chain-monte-carlo.html#cb546-14"></a><span class="st">  </span>lagged_post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-15"><a href="markov-chain-monte-carlo.html#cb546-15"></a><span class="st">  </span><span class="kw">filter</span>(key <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;lag_0&quot;</span>, <span class="st">&quot;lag_5&quot;</span>),</span>
<span id="cb546-16"><a href="markov-chain-monte-carlo.html#cb546-16"></a>         iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-17"><a href="markov-chain-monte-carlo.html#cb546-17"></a><span class="st">  </span></span>
<span id="cb546-18"><a href="markov-chain-monte-carlo.html#cb546-18"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> iter, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) <span class="op">+</span></span>
<span id="cb546-19"><a href="markov-chain-monte-carlo.html#cb546-19"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb546-20"><a href="markov-chain-monte-carlo.html#cb546-20"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb546-21"><a href="markov-chain-monte-carlo.html#cb546-21"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;steelblue&quot;</span>)) <span class="op">+</span></span>
<span id="cb546-22"><a href="markov-chain-monte-carlo.html#cb546-22"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Index 1001:1071&quot;</span>,</span>
<span id="cb546-23"><a href="markov-chain-monte-carlo.html#cb546-23"></a>       <span class="dt">title =</span> <span class="st">&quot;Lag 5&quot;</span>)</span>
<span id="cb546-24"><a href="markov-chain-monte-carlo.html#cb546-24"></a></span>
<span id="cb546-25"><a href="markov-chain-monte-carlo.html#cb546-25"></a>p3 &lt;-</span>
<span id="cb546-26"><a href="markov-chain-monte-carlo.html#cb546-26"></a><span class="st">  </span>lagged_post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-27"><a href="markov-chain-monte-carlo.html#cb546-27"></a><span class="st">  </span><span class="kw">filter</span>(key <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;lag_0&quot;</span>, <span class="st">&quot;lag_10&quot;</span>),</span>
<span id="cb546-28"><a href="markov-chain-monte-carlo.html#cb546-28"></a>         iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb546-29"><a href="markov-chain-monte-carlo.html#cb546-29"></a><span class="st">  </span></span>
<span id="cb546-30"><a href="markov-chain-monte-carlo.html#cb546-30"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> iter, <span class="dt">y =</span> value, <span class="dt">color =</span> key)) <span class="op">+</span></span>
<span id="cb546-31"><a href="markov-chain-monte-carlo.html#cb546-31"></a><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></span>
<span id="cb546-32"><a href="markov-chain-monte-carlo.html#cb546-32"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb546-33"><a href="markov-chain-monte-carlo.html#cb546-33"></a><span class="st">  </span><span class="kw">scale_color_manual</span>(<span class="dt">values =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;steelblue&quot;</span>)) <span class="op">+</span></span>
<span id="cb546-34"><a href="markov-chain-monte-carlo.html#cb546-34"></a><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Index 1001:1071&quot;</span>,</span>
<span id="cb546-35"><a href="markov-chain-monte-carlo.html#cb546-35"></a>       <span class="dt">title =</span> <span class="st">&quot;Lag 10&quot;</span>)</span>
<span id="cb546-36"><a href="markov-chain-monte-carlo.html#cb546-36"></a></span>
<span id="cb546-37"><a href="markov-chain-monte-carlo.html#cb546-37"></a><span class="kw">library</span>(patchwork)</span>
<span id="cb546-38"><a href="markov-chain-monte-carlo.html#cb546-38"></a></span>
<span id="cb546-39"><a href="markov-chain-monte-carlo.html#cb546-39"></a>(p1 <span class="op">+</span><span class="st"> </span>p2 <span class="op">+</span><span class="st"> </span>p3) <span class="op">&amp;</span></span>
<span id="cb546-40"><a href="markov-chain-monte-carlo.html#cb546-40"></a><span class="st">  </span><span class="kw">theme_cowplot</span>() <span class="op">&amp;</span></span>
<span id="cb546-41"><a href="markov-chain-monte-carlo.html#cb546-41"></a><span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-59-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Here’s the middle row for Figure 7.12.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="markov-chain-monte-carlo.html#cb547-1"></a>lagged_post_wide &lt;-</span>
<span id="cb547-2"><a href="markov-chain-monte-carlo.html#cb547-2"></a><span class="st">  </span>lagged_post <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-3"><a href="markov-chain-monte-carlo.html#cb547-3"></a><span class="st">  </span><span class="kw">spread</span>(<span class="dt">key =</span> key, <span class="dt">value =</span> value)</span>
<span id="cb547-4"><a href="markov-chain-monte-carlo.html#cb547-4"></a></span>
<span id="cb547-5"><a href="markov-chain-monte-carlo.html#cb547-5"></a>p1 &lt;-</span>
<span id="cb547-6"><a href="markov-chain-monte-carlo.html#cb547-6"></a><span class="st">  </span>lagged_post_wide <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-7"><a href="markov-chain-monte-carlo.html#cb547-7"></a><span class="st">  </span><span class="kw">filter</span>(iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-8"><a href="markov-chain-monte-carlo.html#cb547-8"></a><span class="st">  </span></span>
<span id="cb547-9"><a href="markov-chain-monte-carlo.html#cb547-9"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lag_<span class="dv">1</span>, <span class="dt">y =</span> lag_<span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb547-10"><a href="markov-chain-monte-carlo.html#cb547-10"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb547-11"><a href="markov-chain-monte-carlo.html#cb547-11"></a><span class="st">  </span><span class="kw">geom_point</span>()</span>
<span id="cb547-12"><a href="markov-chain-monte-carlo.html#cb547-12"></a></span>
<span id="cb547-13"><a href="markov-chain-monte-carlo.html#cb547-13"></a>p2 &lt;-</span>
<span id="cb547-14"><a href="markov-chain-monte-carlo.html#cb547-14"></a><span class="st">  </span>lagged_post_wide <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-15"><a href="markov-chain-monte-carlo.html#cb547-15"></a><span class="st">  </span><span class="kw">filter</span>(iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-16"><a href="markov-chain-monte-carlo.html#cb547-16"></a><span class="st">  </span></span>
<span id="cb547-17"><a href="markov-chain-monte-carlo.html#cb547-17"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lag_<span class="dv">5</span>, <span class="dt">y =</span> lag_<span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb547-18"><a href="markov-chain-monte-carlo.html#cb547-18"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb547-19"><a href="markov-chain-monte-carlo.html#cb547-19"></a><span class="st">  </span><span class="kw">geom_point</span>()</span>
<span id="cb547-20"><a href="markov-chain-monte-carlo.html#cb547-20"></a></span>
<span id="cb547-21"><a href="markov-chain-monte-carlo.html#cb547-21"></a>p3 &lt;-</span>
<span id="cb547-22"><a href="markov-chain-monte-carlo.html#cb547-22"></a><span class="st">  </span>lagged_post_wide <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-23"><a href="markov-chain-monte-carlo.html#cb547-23"></a><span class="st">  </span><span class="kw">filter</span>(iter <span class="op">&gt;</span><span class="st"> </span><span class="dv">1000</span> <span class="op">&amp;</span><span class="st"> </span>iter <span class="op">&lt;</span><span class="st"> </span><span class="dv">1071</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb547-24"><a href="markov-chain-monte-carlo.html#cb547-24"></a><span class="st">  </span></span>
<span id="cb547-25"><a href="markov-chain-monte-carlo.html#cb547-25"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> lag_<span class="dv">10</span>, <span class="dt">y =</span> lag_<span class="dv">0</span>)) <span class="op">+</span></span>
<span id="cb547-26"><a href="markov-chain-monte-carlo.html#cb547-26"></a><span class="st">  </span><span class="kw">stat_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="op">+</span></span>
<span id="cb547-27"><a href="markov-chain-monte-carlo.html#cb547-27"></a><span class="st">  </span><span class="kw">geom_point</span>()</span>
<span id="cb547-28"><a href="markov-chain-monte-carlo.html#cb547-28"></a></span>
<span id="cb547-29"><a href="markov-chain-monte-carlo.html#cb547-29"></a>(p1 <span class="op">+</span><span class="st"> </span>p2 <span class="op">+</span><span class="st"> </span>p3) <span class="op">&amp;</span><span class="st"> </span></span>
<span id="cb547-30"><a href="markov-chain-monte-carlo.html#cb547-30"></a><span class="st">  </span><span class="kw">theme_cowplot</span>()</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-60-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>For kicks and giggles, we used <code>stat_smooth()</code> to add an OLS regression line with its 95% confidence intervals to each plot.</p>
<p>If you want the Pearson’s correlations among the lags, the <code>lowerCor()</code> function from the <a href="https://CRAN.R-project.org/package=psych"><strong>psych</strong> package</a> <span class="citation">(Revelle, <a href="#ref-R-psych" role="doc-biblioref">2020</a>)</span> can be handy.</p>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="markov-chain-monte-carlo.html#cb548-1"></a><span class="kw">library</span>(psych)</span>
<span id="cb548-2"><a href="markov-chain-monte-carlo.html#cb548-2"></a></span>
<span id="cb548-3"><a href="markov-chain-monte-carlo.html#cb548-3"></a>lagged_post_wide <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb548-4"><a href="markov-chain-monte-carlo.html#cb548-4"></a><span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>iter) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb548-5"><a href="markov-chain-monte-carlo.html#cb548-5"></a><span class="st">  </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(lag_<span class="dv">10</span>)) <span class="op">%&gt;%</span></span>
<span id="cb548-6"><a href="markov-chain-monte-carlo.html#cb548-6"></a><span class="st">  </span></span>
<span id="cb548-7"><a href="markov-chain-monte-carlo.html#cb548-7"></a><span class="st">  </span><span class="kw">lowerCor</span>(<span class="dt">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        lag_0 lag_1 lag_10 lag_5
## lag_0  1.000                   
## lag_1  0.455 1.000             
## lag_10 0.021 0.016 1.000       
## lag_5  0.053 0.080 0.053  1.000</code></pre>
<p>For our version of the bottom of Figure 7.12, we’ll use the <code>bayesplot::mcmc_acf_bar()</code> function to get the autocorrelation bar plot, by chain.</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="markov-chain-monte-carlo.html#cb550-1"></a><span class="kw">mcmc_acf_bar</span>(post,</span>
<span id="cb550-2"><a href="markov-chain-monte-carlo.html#cb550-2"></a>             <span class="dt">pars =</span> <span class="st">&quot;b_Intercept&quot;</span>,</span>
<span id="cb550-3"><a href="markov-chain-monte-carlo.html#cb550-3"></a>             <span class="dt">lags =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="07_files/figure-gfm/unnamed-chunk-62-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>All three rows of our versions for Figure 7.12 indicate in their own way how much lower our autocorrelations were than the ones in the text.</p>
<p>If you’re curious of the effective sample sizes for the parameters in your <strong>brms</strong> models, just look at the model summary using either <code>summary()</code> or <code>print()</code>.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="markov-chain-monte-carlo.html#cb551-1"></a><span class="kw">print</span>(fit7<span class="fl">.2</span>)</span></code></pre></div>
<pre><code>##  Family: bernoulli 
##   Links: mu = identity 
## Formula: y ~ 1 
##    Data: d (Number of observations: 50) 
## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1;
##          total post-warmup samples = 28500
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     0.69      0.06     0.56     0.80 1.00     9215     9302
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Look at the last two columns in the <code>Intercept</code> summary. Earlier versions of <strong>brms</strong> had one column named <code>Eff.Sample</code>, which reported the effect sample size as discussed by Kruschke. Starting with version 2.10.0, <strong>brms</strong> now returns <code>Bulk_ESS</code> and <code>Tail_ESS</code>, instead. These originate from a <span class="citation">(<a href="#ref-vehtariRanknormalizationFoldingLocalization2019" role="doc-biblioref">2019</a>)</span> <a href="https://arxiv.org/abs/1903.08008?">preprint</a> by Stan-team all-stars Vehtari, Gelman, Simpson, Carpenter, and Bürkner. From their paper, we read:</p>
<blockquote>
<p>If you plan to report quantile estimates or posterior intervals, we strongly suggest assessing the convergence of the chains for these quantiles. In <a href="what-is-this-stuff-called-probability.html#probability-distributions">Section 4.3</a> we show that convergence of Markov chains is not uniform across the parameter space and propose diagnostics and effective sample sizes specifically for extreme quantiles. This is <em>different</em> from the standard ESS estimate (which we refer to as the “bulk-ESS”), which mainly assesses how well the centre of the distribution is resolved. Instead, these “tail-ESS” measures allow the user to estimate the MCSE for interval estimates. (p. 5, <em>emphasis</em> in the original)</p>
</blockquote>
<p>For more technical details, see the paper. The <code>Bulk_ESS</code> column in current versions of <strong>brms</strong> is what was previously referred to as <code>Eff.Sample</code>. This is what corresponds to what Kruschke meant when referring to effective sample size. Now rather than focusing solely on ‘the center of the’ posterior distribution’ as indexed by <code>Bulk_ESS</code>, we also gauge the effective sample size in the posterior intervals using <code>Tail_ESS</code>.</p>
<p>Anyway, I’m not quite sure how to reproduce Kruschke’s MCMC ESS simulation studies. If you’ve got it figured out, please share your code in my <a href="https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/issues/15">GitHub issue #15</a>.</p>
<p>If you’re interested in the Monte Carlo standard error (MCSE) for your <strong>brms</strong> parameters, the easiest way is to tack <code>$fit</code> onto your fit object.</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="markov-chain-monte-carlo.html#cb553-1"></a>fit7<span class="fl">.2</span><span class="op">$</span>fit</span></code></pre></div>
<pre><code>## Inference for Stan model: f2927ce8aaeb3b4a2a85b64487208e39.
## 3 chains, each with iter=10000; warmup=500; thin=1; 
## post-warmup draws per chain=9500, total post-warmup draws=28500.
## 
##               mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat
## b_Intercept   0.69    0.00 0.06   0.56   0.64   0.69   0.73   0.80  9156    1
## lp__        -30.79    0.01 0.68 -32.75 -30.96 -30.53 -30.35 -30.31  8163    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Sep 20 15:22:05 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>This returns an <a href="https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html">rstan-like summary</a> <span class="citation">(Stan Development Team, <a href="#ref-standevelopmentteamAccessingContentsStanfit2020" role="doc-biblioref">2020</a><a href="#ref-standevelopmentteamAccessingContentsStanfit2020" role="doc-biblioref">c</a>)</span>. The ‘se_mean’ column is the MCSE.</p>
</div>
<div id="mcmc-efficiency." class="section level3">
<h3><span class="header-section-number">7.5.3</span> MCMC efficiency.</h3>
<p>Kruschke wrote: “It is often the case in realistic applications that there is strong autocorrelation for some parameters, and therefore, an extremely long chain is required to achieve an adequate ESS or MCSE” (p. 187). As we’ll see, this is generally less of a problem for HMC than for MH or Gibbs. But it does still crop up, particularly in complicated models. As he wrote on the following page, “one sampling method that can be relatively efficient is Hamiltonian Monte Carlo.” Indeed.</p>
</div>
</div>
<div id="session-info-6" class="section level2 unnumbered">
<h2>Session info</h2>
<div class="sourceCode" id="cb555"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb555-1"><a href="markov-chain-monte-carlo.html#cb555-1"></a><span class="kw">sessionInfo</span>()</span></code></pre></div>
<pre><code>## R version 3.6.3 (2020-02-29)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS Catalina 10.15.3
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] psych_1.9.12.31      patchwork_1.0.1.9000 ggmcmc_1.4.1         bayesplot_1.7.1     
##  [5] brms_2.13.5          Rcpp_1.0.5           tidybayes_2.1.1      cowplot_1.0.0.9000  
##  [9] forcats_0.5.0        stringr_1.4.0        dplyr_1.0.1          purrr_0.3.4         
## [13] readr_1.3.1          tidyr_1.1.1          tibble_3.0.3         ggplot2_3.3.2       
## [17] tidyverse_1.3.0     
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.3.1         backports_1.1.9      plyr_1.8.6           igraph_1.2.5        
##   [5] splines_3.6.3        svUnit_1.0.3         crosstalk_1.1.0.1    TH.data_1.0-10      
##   [9] rstantools_2.1.1     inline_0.3.15        digest_0.6.25        htmltools_0.5.0     
##  [13] rsconnect_0.8.16     fansi_0.4.1          magrittr_1.5         modelr_0.1.6        
##  [17] matrixStats_0.56.0   xts_0.12-0           sandwich_2.5-1       prettyunits_1.1.1   
##  [21] colorspace_1.4-1     rvest_0.3.5          ggdist_2.1.1         haven_2.2.0         
##  [25] xfun_0.13            callr_3.4.4          crayon_1.3.4         jsonlite_1.7.0      
##  [29] survival_3.1-12      zoo_1.8-7            glue_1.4.2           gtable_0.3.0        
##  [33] emmeans_1.4.5        pkgbuild_1.1.0       rstan_2.19.3         abind_1.4-5         
##  [37] scales_1.1.1         mvtnorm_1.1-0        DBI_1.1.0            GGally_2.0.0        
##  [41] miniUI_0.1.1.1       viridisLite_0.3.0    xtable_1.8-4         HDInterval_0.2.0    
##  [45] stats4_3.6.3         StanHeaders_2.21.0-1 DT_0.13              htmlwidgets_1.5.1   
##  [49] httr_1.4.1           threejs_0.3.3        arrayhelpers_1.1-0   RColorBrewer_1.1-2  
##  [53] ellipsis_0.3.1       pkgconfig_2.0.3      reshape_0.8.8        loo_2.3.1           
##  [57] farver_2.0.3         dbplyr_1.4.2         utf8_1.1.4           tidyselect_1.1.0    
##  [61] labeling_0.3         rlang_0.4.7          reshape2_1.4.4       later_1.1.0.1       
##  [65] munsell_0.5.0        cellranger_1.1.0     tools_3.6.3          cli_2.0.2           
##  [69] generics_0.0.2       broom_0.5.5          ggridges_0.5.2       evaluate_0.14       
##  [73] fastmap_1.0.1        yaml_2.2.1           processx_3.4.4       knitr_1.28          
##  [77] fs_1.4.1             nlme_3.1-144         mime_0.9             xml2_1.3.1          
##  [81] compiler_3.6.3       shinythemes_1.1.2    rstudioapi_0.11      reprex_0.3.0        
##  [85] stringi_1.4.6        ps_1.3.4             Brobdingnag_1.2-6    lattice_0.20-38     
##  [89] Matrix_1.2-18        markdown_1.1         shinyjs_1.1          vctrs_0.3.4         
##  [93] pillar_1.4.6         lifecycle_0.2.0      bridgesampling_1.0-0 estimability_1.3    
##  [97] httpuv_1.5.4         R6_2.4.1             bookdown_0.18        promises_1.1.1      
## [101] gridExtra_2.3        codetools_0.2-16     colourpicker_1.0     MASS_7.3-51.5       
## [105] gtools_3.8.2         assertthat_0.2.1     withr_2.2.0          mnormt_1.5-6        
## [109] shinystan_2.5.0      multcomp_1.4-13      mgcv_1.8-31          parallel_3.6.3      
## [113] hms_0.5.3            grid_3.6.3           coda_0.19-3          rmarkdown_2.1       
## [117] shiny_1.5.0          lubridate_1.7.8      base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bürkner2020Multivariate">
<p>Bürkner, P.-C. (2020g). <em>Estimating multivariate models with brms</em>. <a href="https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html">https://CRAN.R-project.org/package=brms/vignettes/brms_multivariate.html</a></p>
</div>
<div id="ref-carpenterStanProbabilisticProgramming2017">
<p>Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D., Goodrich, B., Betancourt, M., Brubaker, M., Guo, J., Li, P., &amp; Riddell, A. (2017). Stan: A probabilistic programming language. <em>Journal of Statistical Software</em>, <em>76</em>(1). <a href="https://doi.org/10.18637/jss.v076.i01">https://doi.org/10.18637/jss.v076.i01</a></p>
</div>
<div id="ref-eckhardtStanUlamJohn1987">
<p>Eckhardt, R. (1987). Stan Ulam, John von Neumann and the Monte Carlo method. <em>Argonne, USA</em>. <a href="https://library.sciencemadness.org/lanl1_a/lib-www/pubs/00326867.pdf">https://library.sciencemadness.org/lanl1_a/lib-www/pubs/00326867.pdf</a></p>
</div>
<div id="ref-fernandezGGMCMCAnalysisofMCMC2016">
<p>Fernández i Marín, X. (2016). ggmcmc: Analysis of MCMC samples and Bayesian inference. <em>Journal of Statistical Software</em>, <em>70</em>(9), 1–20. <a href="https://doi.org/10.18637/jss.v070.i09">https://doi.org/10.18637/jss.v070.i09</a></p>
</div>
<div id="ref-R-ggmcmc">
<p>Fernández i Marín, X. (2020). <em>ggmcmc: Tools for analyzing MCMC simulations from Bayesian inference</em> [Manual]. <a href="https://CRAN.R-project.org/package=ggmcmc">https://CRAN.R-project.org/package=ggmcmc</a></p>
</div>
<div id="ref-R-bayesplot">
<p>Gabry, J., &amp; Mahr, T. (2019). <em>bayesplot: Plotting for Bayesian models</em>. <a href="https://CRAN.R-project.org/package=bayesplot">https://CRAN.R-project.org/package=bayesplot</a></p>
</div>
<div id="ref-gabry2019visualization">
<p>Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., &amp; Gelman, A. (2019). Visualization in Bayesian workflow. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em>182</em>(2), 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a></p>
</div>
<div id="ref-kruschkeDoingBayesianData2015">
<p>Kruschke, J. K. (2015). <em>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</em>. Academic Press. <a href="https://sites.google.com/site/doingbayesiandataanalysis/">https://sites.google.com/site/doingbayesiandataanalysis/</a></p>
</div>
<div id="ref-mcelreathStatisticalRethinkingBayesian2015">
<p>McElreath, R. (2015). <em>Statistical rethinking: A Bayesian course with examples in R and Stan</em>. CRC press. <a href="https://xcelab.net/rm/statistical-rethinking/">https://xcelab.net/rm/statistical-rethinking/</a></p>
</div>
<div id="ref-metropolisEquationStateCalculations1953">
<p>Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of state calculations by fast computing machines. <em>The Journal of Chemical Physics</em>, <em>21</em>(6), 1087–1092. <a href="https://doi.org/10.1063/1.1699114">https://doi.org/10.1063/1.1699114</a></p>
</div>
<div id="ref-R-psych">
<p>Revelle, W. (2020). <em>psych: Procedures for psychological, psychometric, and personality research</em>. <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a></p>
</div>
<div id="ref-standevelopmentteamStanReferenceManual2020">
<p>Stan Development Team. (2020a). <em>Stan reference manual, Version 2.24</em>. <a href="https://mc-stan.org/docs/2_24/reference-manual/">https://mc-stan.org/docs/2_24/reference-manual/</a></p>
</div>
<div id="ref-standevelopmentteamAccessingContentsStanfit2020">
<p>Stan Development Team. (2020c). <em>Accessing the contents of a stanfit object</em>. <a href="https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html">https://CRAN.R-project.org/package=rstan/vignettes/stanfit-objects.html</a></p>
</div>
<div id="ref-vehtariRanknormalizationFoldingLocalization2019">
<p>Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P.-C. (2019). Rank-normalization, folding, and localization: An improved <span class="math inline">\(\widehat{R}\)</span> for assessing convergence of MCMC. <em>arXiv Preprint arXiv:1903.08008</em>. <a href="https://arxiv.org/abs/1903.08008?">https://arxiv.org/abs/1903.08008?</a></p>
</div>
<div id="ref-wilkeFundamentalsDataVisualization2019">
<p>Wilke, C. O. (2019a). <em>Fundamentals of data visualization</em>. <a href="https://clauswilke.com/dataviz/">https://clauswilke.com/dataviz/</a></p>
</div>
<div id="ref-Wilke2019Themes">
<p>Wilke, C. O. (2019b). <em>Themes</em>. <a href="https://wilkelab.org/cowplot/articles/themes.html">https://wilkelab.org/cowplot/articles/themes.html</a></p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>In his <a href="https://sites.google.com/site/doingbayesiandataanalysis/corrigenda">Corrigenda</a>, Kruschke further clarified: “that is true only in case there is a single predictor, not for multiple predictors. The statement could have said that ‘R^2 is algebraically constrained to fall between −1 and +1 in least-squares regression’. More relevantly, replace the statement with the following: ‘In multiple linear regression, standardized regression coefficients tend to fall between -2 and +2 unless the predictors are very strongly correlated and have strongly opposing effects. If your data have strongly correlated predictors, consider widening the prior.’”<a href="markov-chain-monte-carlo.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inferring-a-binomial-probability-via-exact-mathematical-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="jags-brms.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
