[
["index.html", "Doing Bayesian Data Analysis in brms and the tidyverse version 0.0.2 What and why Caution: Work in progress", " Doing Bayesian Data Analysis in brms and the tidyverse version 0.0.2 A Solomon Kurz 2019-10-27 What and why Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” In the same way, this project is designed to help those real people do Bayesian data analysis. My contribution is converting Kruschke’s JAGS code for use in Bürkner’s brms package, which makes it easier to fit Bayesian regression models in R using Hamiltonian Monte Carlo (HMC). I also prefer plotting and data wrangling with the packages from the tidyverse. So we’ll be using those methods, too. This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s Doing Bayesian Data Analysis. Please give the source material some love. Caution: Work in progress The first release of this project only contains Chapters 1 through 5. Welcome to version 0.0.2! The notable updates are: the addition of Chapters 6 through 10 and minor typo fixes to Chapter 5. Version 0.0.2 is also noteworthy in that it’s the first version containing an incomplete chapter. At present, I do not know how to perform the simulation for Figure 7.3, nor do I understand how to run Kruschke’s effective sample size simulations from subsection 7.5.2. Fans of this project are welcome to share solutions to those sections in the Issues section of the GitHub repository for this project. Most of the remaining chapters have completed drafts and just need another round of edits. I’ll add them, soon. In addition to checking in here, you can follow my GitHub progress log, in which I will point out other figures or sections I’m having trouble with. "],
["whats-in-this-book-read-this-first.html", "1 What’s in This Book (Read This First!) 1.1 Real people can read this book 1.2 What’s in this book 1.3 What’s new in the second edition 1.4 Gimme feedback (be polite) 1.5 Thank you! Reference Session info", " 1 What’s in This Book (Read This First!) 1.1 Real people can read this book Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” Agreed. Similarly, this project is designed to help those real people do Bayesian data analysis. While I’m at it, I may as well explicate my assumptions about you. If you’re looking at this project, I’m guessing you’re a graduate student, a post-graduate academic or researcher of some sort. Which means I’m presuming you have at least a 101-level foundation in statistics. In his text, it seems like Kruschke presumed his readers would have a good foundation in calculus, too. I make no such presumption. But if your stats 101 chops are rusty, check out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons or Navarro’s free text, Learning statistics with R: A tutorial for psychology students and other beginners. I presume a basic working fluency in R and a vague idea about what the tidyverse is. Kruschke does some R warm-up in chapter 2, and I follow suit. But if you’re totally new to R, you might also consider starting with Peng’s R Programming for Data Science. The best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science. 1.2 What’s in this book This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s Doing Bayesian Data Analysis. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, many of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project might be blank or even missing. Also beware the content herein will depart at times from the source material. Bayesian data analysis with HMC is an active area of development in terms of both statistical methods and software implementation. There will also be times when my thoughts and preferences on Bayesian data analysis diverge a bit from Kruschke’s. In those places of divergence, I will often provide references and explanations. In this project, I use a handful of formatting conventions gleaned from R4DS and R Markdown: The Definitive Guide. I put R and R packages (e.g., brms) in boldface. R code blocks and their output appear in a gray background. E.g., 2 + 2 ## [1] 4 Did you notice how there were two strips of gray background, there? The first one designated the actual code. The second one was the output of that code. The output of a code block often begins with ##. Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit what packages a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidyr::pivot_longer()). R objects, such as data or function arguments, are in typewriter font atop a gray background (e.g., d or size = 2). Hyperlinks are denoted by their typical blue-colored font. 1.3 What’s new in the second edition This is my first attempt at this project. There’s nothing new from my end. 1.4 Gimme feedback (be polite) I am not a statistician and I have no formal background in computer science. I just finished my PhD in clinical psychology in 2018. During my graduate training I developed an unexpected interest in applied statistics and, more recently, programming. I became an R user in 2015 and started learning about Bayesian statistics around 2013. There is still so much to learn, so my apologies for when my code appears dated or inelegant. There will also be occasions in which I’m not yet sure how to reproduce models or plots in the text. Which is all to say, suggestions on how to improve my code are welcome. Id you’d like to learn more about me, you can find my website here. 1.5 Thank you! While in grad school, I benefitted tremendously from free online content. This project and others like it (e.g., here or here or here) are my attempts to pay it forward. As soon as you’ve gained a little proficiency, do consider doing to same. I addition to great texts like Kruschke’s, I’d like to point out a few other important resources that have allowed me to complete a project like this: Jenny Bryan’s Happy Git and GitHub for the useR is the reference that finally got me working on Github. Again and again, I return to Grolemund and Wickham’s R for Data Science to learn about the tidyverse way of coding. Yihui Xie’s bookdown: Authoring Books and Technical Documents with R Markdown is the primary source from which I learned how to make online books like this. If you haven’t already, bookmark these resources and share them with your friends. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info At the end of every chapter, I use the sessionInfo() function to help make my results more reproducible. sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.6.0 magrittr_1.5 tools_3.6.0 htmltools_0.4.0 ## [5] yaml_2.2.0 Rcpp_1.0.2 stringi_1.4.3 rmarkdown_1.13 ## [9] knitr_1.23 stringr_1.4.0 xfun_0.10 digest_0.6.21 ## [13] rlang_0.4.0 evaluate_0.14 "],
["introduction-credibility-models-and-parameters.html", "2 Introduction: Credibility, Models, and Parameters 2.1 Bayesian inference is reallocation of credibility across possibilities 2.2 Possibilities are parameter values in descriptive models 2.3 The steps of Bayesian data analysis Reference Session info", " 2 Introduction: Credibility, Models, and Parameters The goal of this chapter is to introduce the conceptual framework of Bayesian data analysis. Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is reallocation of credibility across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models. (p. 15) 2.1 Bayesian inference is reallocation of credibility across possibilities The first step toward making Figure 2.1 is putting together a data object. And to help with that, we’ll open up the tidyverse. library(tidyverse) d &lt;- tibble(iteration = 1:3) %&gt;% expand(iteration, stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(nesting(iteration, stage), Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(.25, times = 4), 0, rep(1/3, times = 3), 0, rep(1/3, times = 3), rep(c(0, .5), each = 2), rep(c(0, .5), each = 2), rep(0, times = 3), 1)) When making data with man repetitions in the rows, it’s good to have the tidyr::expand() function up your sleeve. Go here to learn more. We can take a look at the top few rows of the data with head(). head(d) ## # A tibble: 6 x 4 ## iteration stage Possibilities Credibility ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Prior A 0.25 ## 2 1 Prior B 0.25 ## 3 1 Prior C 0.25 ## 4 1 Prior D 0.25 ## 5 1 Posterior A 0 ## 6 1 Posterior B 0.333 Before we attempt Figure 2.1, we’ll need two supplemental data frames. The first one, d_text, will supply the coordinates for the annotation in the plot. The second, d_arrow, will supply the coordinates for the arrows. d_text &lt;- tibble(Possibilities = &quot;B&quot;, Credibility = .75, label = str_c(LETTERS[1:3], &quot; is\\nimpossible&quot;), iteration = 1:3, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) d_arrow &lt;- tibble(Possibilities = LETTERS[1:3], iteration = 1:3) %&gt;% expand(nesting(Possibilities, iteration), Credibility = c(0.6, 0.01)) %&gt;% mutate(stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) Now we’re ready to code our version of Figure 2.1. d %&gt;% ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom row geom_text(data = d_text, aes(label = label)) + # arrows in the bottom row geom_line(data = d_arrow, arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;first&quot;, type = &quot;closed&quot;)) + facet_grid(stage ~ iteration) + theme(panel.grid = element_blank(), strip.text.x = element_blank(), axis.ticks.x = element_blank()) We will take a similar approach to make our version of Figure 2.2. But this time, we’ll define our supplemental data sets directly in geom_text() and geom_line(). It’s good to have both methods up your sleeve. Also notice how we simply fed our primary data set directly into ggplot() without saving it, either. # primary data tibble(stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(stage, Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(0.25, times = 4), rep(0, times = 3), 1)) %&gt;% # plot! ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom panel geom_text(data = tibble( Possibilities = &quot;B&quot;, Credibility = .8, label = &quot;D is\\nresponsible&quot;, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), aes(label = label) ) + # the arrow geom_line(data = tibble( Possibilities = LETTERS[c(4, 4)], Credibility = c(.25, .99), stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;last&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + facet_wrap(~stage, ncol = 1) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) 2.1.1 Data are noisy and inferences are probabilistic. Now on to Figure 2.3. I’m pretty sure the curves in the plot are Gaussian, which we’ll make with the dnorm() function. After a little trial and error, their standard deviations look to be 1.2. However, it’s tricky placing those curves in along with the probabilities, because the probabilities for the four discrete sizes (i.e., 1 through 4) are in a different metric than the Gaussian density curves. Since the probability metric for the four discrete sizes are the primary metric of the plot, we need to rescale the curves using a little algebra. We do that in the data code, below. After that, the code for the plot is relatively simple. # data tibble(mu = 1:4, p = .25) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot(aes(x = x)) + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(y = density, group = mu)) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = 0:5, ylim = 0:1) + labs(title = &quot;Prior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) We can use the same basic method to make the bottom panel. The new consideration is choosing the relative probabilities for the different mu values–keeping in mind they have to sum to 1. I just eyeballed them. The only other notable change from the previous plot is our addition of a geom_point() section, in which we defined the data on the fly. tibble(mu = 1:4, p = c(.1, .58, .3, .02)) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot() + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(x = x, y = density, group = mu)) + geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0), aes(x = x, y = y), size = 3, color = &quot;grey33&quot;, alpha = 3/4) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = 0:5, ylim = 0:1) + labs(title = &quot;Posterior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) In summary, the essence of Bayesian inference is reallocation of credibility across possibilities. The distribution of credibility initially reflects prior knowledge about the possibilities, which can be quite vague. Then new data are observed, and the credibility is re-allocated. Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. (p. 22) 2.2 Possibilities are parameter values in descriptive models “A key step in Bayesian analysis is defining the set of possibilities over which credibility is allocated. This is not a trivial step, because there might always be possibilities beyond the ones we include in the initial set” (p. 22, emphasis added). In the last section, we used the dnorm() function to make curves following the Normal distribution. Here we’ll do that again, but also use the rnorm() function to simulate actual data from that same Normal distribution. Behold Figure 2.4.a. # set the seed to make the simulation reproducible set.seed(2) # simulate the data with `rnorm()` d &lt;- tibble(x = rnorm(2000, mean = 10, sd = 5)) # plot! ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/10) + geom_line(data = tibble(x = seq(from = -6, to = 26, by = .01)), aes(x = x, y = dnorm(x, mean = 10, sd = 5)), color = &quot;grey33&quot;) + coord_cartesian(xlim = -5:25) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 10 and SD of 5.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) Did you notice how we made the data for the density curve within geom_line()? That’s one way to do it. In our next plot, we’ll take a different and more elegant approach with stat_function(). Here’s our Figure 2.4.b. ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + stat_function(fun = dnorm, n = 101, args = list(mean = 8, sd = 6), color = &quot;grey33&quot;, linetype = 2) + coord_cartesian(xlim = -5:25) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 8 and SD of 6.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) 2.3 The steps of Bayesian data analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25) I will show you a few more details than Kruschke did in the text. But just has he did, we’ll cover this workflow in much more detail in the chapters to come. In order to recreate Figure 2.5, we need to generate the data and fit the model. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height/weight data of the kind in his text. Here is the code in full: HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values to generate and how probable we want the values to be based on those from men. These are controlled by the nSubj and maleProb parameters. # set your seed to make the data generation reproducible set.seed(57) d &lt;- HtWtDataGenerator(nSubj = 57, maleProb = .5) %&gt;% as_tibble() d %&gt;% head() ## # A tibble: 6 x 3 ## male height weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 68.8 133. ## 2 1 70 187. ## 3 0 63.2 154 ## 4 0 61.4 145. ## 5 0 66.1 130. ## 6 1 71.5 271 We’re about ready for the model. We will fit it with HMC via the brms package. library(brms) The traditional use of diffuse and noninformative priors is discouraged with HMC, as is the uniform distribution for sigma. Instead, we’ll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for \\(\\sigma\\). fit1 &lt;- brm(data = d, family = gaussian, weight ~ 1 + height, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 2) If you wanted a quick model summary, you could execute print(fit1). We’ll walk through that and other diagnostics in greater detail starting in Chapter 8. But for now, here’s how we might make Figure 2.5.a. # extract the posterior draws post &lt;- posterior_samples(fit1) # this will streamline some of the code, below n_lines &lt;- 150 # plot! d %&gt;% ggplot(aes(x = height, y = weight)) + geom_abline(intercept = post[1:n_lines, 1], slope = post[1:n_lines, 2], color = &quot;grey50&quot;, size = 1/4, alpha = .3) + geom_point(alpha = 2/3) + coord_cartesian(xlim = 55:80, ylim = 50:250) + # the `eval(substitute(paste()))` trick came from: https://www.r-bloggers.com/value-of-an-r-object-in-an-expression/ labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;Height in inches&quot;, y = &quot;Weight in pounds&quot;) + theme(panel.grid = element_blank()) For Figure 2.5.b., we’ll use the handy stat_pointintervalh() function from the tidybayes package to mark off the mode and 95% HDIs. library(tidybayes) post %&gt;% ggplot(aes(x = b_height)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, binwidth = .2, size = .2) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:8) + labs(title = &quot;The posterior distribution&quot;, subtitle = &quot;The mode and 95% HPD intervals are\\nthe dot and horizontal line at the bottom.&quot;, x = expression(paste(beta[1], &quot; (slope)&quot;))) + theme(panel.grid = element_blank()) Here’s Figure 2.6. We’ll go over the brms::predict() function later. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 20)) predict(fit1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), color = &quot;grey67&quot;, shape = 20) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) The posterior predictions might be easier to depict with a ribbon and line, instead. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 30)) predict(fit1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + geom_line(aes(y = Estimate), color = &quot;grey92&quot;) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 brms_2.10.3 Rcpp_1.0.2 forcats_0.4.0 ## [5] stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 readr_1.3.1 ## [9] tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.15 ggstance_0.3.2 ## [5] markdown_1.1 base64enc_0.1-3 ## [7] rstudioapi_0.10 rstan_2.19.2 ## [9] svUnit_0.7-12 DT_0.9 ## [11] fansi_0.4.0 lubridate_1.7.4 ## [13] xml2_1.2.0 bridgesampling_0.7-2 ## [15] knitr_1.23 shinythemes_1.1.2 ## [17] zeallot_0.1.0 bayesplot_1.7.0 ## [19] jsonlite_1.6 broom_0.5.2 ## [21] shiny_1.3.2 compiler_3.6.0 ## [23] httr_1.4.0 backports_1.1.5 ## [25] assertthat_0.2.1 Matrix_1.2-17 ## [27] lazyeval_0.2.2 cli_1.1.0 ## [29] later_1.0.0 htmltools_0.4.0 ## [31] prettyunits_1.0.2 tools_3.6.0 ## [33] igraph_1.2.4.1 coda_0.19-3 ## [35] gtable_0.3.0 glue_1.3.1 ## [37] reshape2_1.4.3 cellranger_1.1.0 ## [39] vctrs_0.2.0 nlme_3.1-139 ## [41] crosstalk_1.0.0 xfun_0.10 ## [43] ps_1.3.0 rvest_0.3.4 ## [45] mime_0.7 miniUI_0.1.1.1 ## [47] lifecycle_0.1.0 gtools_3.8.1 ## [49] MASS_7.3-51.4 zoo_1.8-6 ## [51] scales_1.0.0 colourpicker_1.0 ## [53] hms_0.4.2 promises_1.1.0 ## [55] Brobdingnag_1.2-6 parallel_3.6.0 ## [57] inline_0.3.15 shinystan_2.5.0 ## [59] yaml_2.2.0 gridExtra_2.3 ## [61] loo_2.1.0 StanHeaders_2.19.0 ## [63] stringi_1.4.3 dygraphs_1.1.1.6 ## [65] pkgbuild_1.0.5 rlang_0.4.0 ## [67] pkgconfig_2.0.3 matrixStats_0.55.0 ## [69] HDInterval_0.2.0 evaluate_0.14 ## [71] lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5 labeling_0.3 ## [75] tidyselect_0.2.5 processx_3.4.1 ## [77] plyr_1.8.4 magrittr_1.5 ## [79] R6_2.4.0 generics_0.0.2 ## [81] pillar_1.4.2 haven_2.1.0 ## [83] withr_2.1.2 xts_0.11-2 ## [85] abind_1.4-5 modelr_0.1.4 ## [87] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [89] utf8_1.1.4 rmarkdown_1.13 ## [91] grid_3.6.0 readxl_1.3.1 ## [93] callr_3.3.2 threejs_0.3.1 ## [95] digest_0.6.21 xtable_1.8-4 ## [97] httpuv_1.5.2 stats4_3.6.0 ## [99] munsell_0.5.0 shinyjs_1.0 "],
["the-r-programming-language.html", "3 The R Programming Language 3.1 Get the software 3.2 A simple example of R in action 3.3 Basic commands and operators in R 3.4 Variable types 3.5 Loading and saving data 3.6 Some utility functions 3.7 Programming in R 3.8 Graphical plots: Opening and saving Reference Session info", " 3 The R Programming Language The material in this chapter is rather dull reading because it basically amounts to a list (although a carefully scaffolded list) of basic commands in R along with illustrative examples. After reading the first few pages and nodding off, you may be tempted to skip ahead, and I wouldn’t blame you. But much of the material in this chapter is crucial, and all of it will eventually be useful, so you should at least skim it all so you know where to return when the topics arise later. (p. 35) Most, but not all, of this part of my project will mirror what’s in the text. However, I do add tidyverse-oriented content, such as a small walk through of plotting in ggplot2. 3.1 Get the software In addition to R and RStudio, I make use of a variety of R packages in this project. You can get the heaviest hitters by executing this code block. install.packages(&quot;devtools&quot;) install.packages(&quot;tidyverse&quot;, dependencies = T) install.packages(&quot;brms&quot;, dependencies = T) install.packages(&quot;tidybayes&quot;, dependencies = T) 3.1.1 A look at RStudio. The R programming language comes with its own basic user interface that is adequate for modest applications. But larger applications become unwieldy in the basic R user interface, and therefore it helps to install a more sophisticated R-friendly editor. There are a number of useful editors available, many of which are free, and they are constantly evolving. At the time of this writing, I recommend RStudio, which can be obtained from http://www.rstudio.com/ (p. 35). I completely agree. R programing is easier with RStudio. 3.2 A simple example of R in action Basic arithmetic is straightforward in R. 2 + 3 ## [1] 5 Algebra is simple, too. x &lt;- 2 x + x ## [1] 4 Behold Figure 3.1. library(tidyverse) d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) If you’re new to the tidyverse and/or making figures with ggplot2, it’s worthwhile to walk that code out. With the first line, library(tidyverse), we opened up the core packages within the tidyverse, which are: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats. With the next block d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) we made our tibble. In R, data frames are one of the primary types of data objects (see subsection 3.4.4., below). We’ll make extensive use of data frames in this project. Tibbles are a particular type of data frame, which you might learn more about here. With those first two lines, we determined what the name of our tibble would be, d, and made the first column, x. Note the %&gt;% operator at the end of the second line. In prose, we call that the pipe. As explained in chapter 5 of R4DS, “a good way to pronounce %&gt;% when reading code is ‘then.’” So in words, the those first two lines indicate “Make an object, d, which is a tibble with a variable, x, defined by the seq() function, then…” In the portion after then (i.e., the %&gt;%), we changed d. The mutate() function let us add another variable, y, which is a function of our first variable, x. With the next 4 lines of code, we made our plot. When plotting with ggplot2, the first line is always with the ggplot() function. This is where you typically tell ggplot2 what data object you’re using–which must be a data frame or tibble–and what variables you want on your axes. The interesting thing about ggplot2 is that the code is modular. So if we only coded the ggplot() portion, we’d get: ggplot(data = d, aes(x = x, y = y)) Although ggplot2 knows which variables to put on which axes, it has no idea how we’d like to express the data. The result is an empty coordinate system. The next line of code is the main event. With geom_line() we told ggplot2 to connect the data points with a line. With the color argument, we made that line skyblue. [Here’s a great list of the named colors available in ggplot2.] Also, notice the + operator at the end of the ggplot() function. With ggplot2, you add functions by placing the + operator on the right of the end of one function, which will then append the next function. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) Personally, I’m not a fan of gridlines. They occasionally have their place and I do use them from time to time. But on the while, I prefer to omit them from my plots. The final theme() function allowed me to do so. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) Chapter 3 of R4DS is a great introduction to plotting with ggplot2. If you want to dive deeper, see the references at the bottom of this page. 3.2.1 Get the programs used with this book. This subtitle has a double meaning, here. Yes, you should probably get Kruschke’s scripts from the book’s website. You may have noticed this already, but unlike in Kruschke’s text, I will usually show all my code. Indeed, the purpose of my project is to make coding these kinds of models and visualizations easier. But if you’re ever curious, you can always find my script files in their naked form, here. Later in this subsection, Kruschke mentioned working directories. If you don’t know what your current working directory is, just execute getwd(). I’ll have more to say on this topic later on when I make my pitch for RStudio projects. 3.3 Basic commands and operators in R In addition to the resource link Kruschke provided in the text, Grolemund and Wickham’s R4DS is an excellent general introduction to the kinds of R functions you’ll want to succeed with your data analysis. Other than that, I’ve learned the most when I had a specific data problem to solve and then sought out the specific code/techniques required to solve it. If already have your own data or can get your hands on some sexy data, learn these techniques by playing around with them. This isn’t the time to worry about rigor, preregistration, or all of that. This is time to play. 3.3.1 Getting help in R. As with plot() you can learn more about the ggplot() function with ?. ?ggplot help.start() can be nice, too. help.start() ??geom_line() can help us learn more about the geom_line() function. ??geom_line() 3.3.2 Arithmetic and logical operators. With arithmetic, the order of operations is: power first, then multiplication, then addition. 1 + 2 * 3^2 ## [1] 19 With parentheses, you can force addition before multiplication. (1 + 2) * 3^2 ## [1] 27 Operations inside parentheses get done before power operations. (1 + 2 * 3)^2 ## [1] 49 One can nest parentheses. ((1 + 2) * 3)^2 ## [1] 81 ?Syntax We can use R to perform a variety of logical tests, such as negation. !TRUE ## [1] FALSE We can do conjunction. TRUE &amp; FALSE ## [1] FALSE And we can do disjunction. TRUE | FALSE ## [1] TRUE Conjunction has precedence over disjunction. TRUE | TRUE &amp; FALSE ## [1] TRUE However, with parentheses we can force disjunction first. (TRUE | TRUE) &amp; FALSE ## [1] FALSE 3.3.3 Assignment, relational operators, and tests of equality. In contrast to Kruschke’s preference, I will use the arrow operator, &lt;-, to assign values to named variables. x = 1 x &lt;- 1 Yep, this ain’t normal math. (x = 1) ## [1] 1 (x = x + 1) ## [1] 2 Here we use == to test for equality. (x = 2) ## [1] 2 x == 2 ## [1] TRUE Using !=, we can check whether the value of x is NOT equal to 3. x != 3 ## [1] TRUE We can use &lt; to check whether the value of x is less than 3. x &lt; 3 ## [1] TRUE Similarly, we can use &gt; to check whether the value of x is greater than 3. x &gt; 3 ## [1] FALSE This normal use of the &lt;- operator x &lt;- 3 is not the same as x &lt; - 3 ## [1] FALSE The limited precision of a computer’s memory can lead to odd results. x &lt;- 0.5 - 0.3 y &lt;- 0.3 - 0.1 Although mathematically TRUE, this is FALSE for limited precision. x == y ## [1] FALSE However, they are equal up to the precision of a computer. all.equal(x, y) ## [1] TRUE 3.4 Variable types If you’d like to learn more about the differences among vectors, matrices, lists, data frames and so on, you might check out Roger Peng’s R Programming for Data Science, chapter 4. 3.4.1 Vector. 3.4.1.1 The combine function. The combine function is c(). c(2.718, 3.14, 1.414) ## [1] 2.718 3.140 1.414 x &lt;- c(2.718, 3.14, 1.414) You’ll note the equivalence. x == c(2.718, 3.14, 1.414) ## [1] TRUE TRUE TRUE This leads to the next subsection. 3.4.1.2 Component-by-component vector operations. We can multiply two vectors, component by component. c(1, 2, 3) * c(7, 6, 5) ## [1] 7 12 15 If you have a sole number, a scaler, you can multiply an entire vector by it like: 2 * c(1, 2, 3) ## [1] 2 4 6 which is a more compact way to perform this. c(2, 2, 2) * c(1, 2, 3) ## [1] 2 4 6 The same sensibilities hold for other operations, such as addition. 2 + c(1, 2, 3) ## [1] 3 4 5 3.4.1.3 The colon operator and sequence function. The colon operator has precedence over addition. 2 + 3:6 ## [1] 5 6 7 8 Parentheses override default precedence. (2 + 3):6 ## [1] 5 6 The power operator has precedence over the colon operator. 1:3^2 ## [1] 1 2 3 4 5 6 7 8 9 And parentheses override default precedence. (1:3)^2 ## [1] 1 4 9 The seq() function is quite handy. If you don’t specify the length of the output, it will figure that out the logical consequence of the other arguments. seq(from = 0, to = 3, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This sequence won’t exceed to = 3. seq(from = 0, to = 3, by = 0.5001) ## [1] 0.0000 0.5001 1.0002 1.5003 2.0004 2.5005 In each of the following examples, we’ll omit one of the core seq() arguments: from, to, by, and length.out. Here we do not define the end point. seq(from = 0, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This time we fail to define the increment. seq(from = 0, to = 3, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 And this time we omit a starting point. seq(to = 3, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.4.1.4 The replicate function. We’ll define our pre-replication vector with the &lt;- operator. abc &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) With times, we repeat the vector as a unit. rep(abc, times = 2) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; But if we mix times with c(), we can repeat individual components of abc differently. rep(abc, times = c(4, 2, 1)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; With the each argument, we repeat the individual components of abc one at a time. rep(abc, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; And you can even combine each and length, repeating each element until the length requirement has been fulfilled. rep(abc, each = 2, length = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; You can also combine each and times. rep(abc, each = 2, times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; ## [18] &quot;C&quot; I tend to do things like the above as two separate steps. One way to do so is by nesting one rep() function within another. rep(rep(abc, each = 2), times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; ## [18] &quot;C&quot; As Kruschke points out, this can look confusing. rep(abc, each = 2, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; But breaking the results up into two steps might be easier to understand, rep(rep(abc, each = 2), times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; And especially earlier in my R career, it helped quite a bit to break operation sequences like this up by saving and assessing the intermediary steps. step_1 &lt;- rep(abc, each = 2) step_1 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; rep(step_1, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; 3.4.1.5 Getting at elements of a vector. Behold our exemplar vector, x. x &lt;- c(2.718, 3.14, 1.414, 47405) The straightforward way to extract the second and fourth elements is x[c(2, 4)] ## [1] 3.14 47405.00 Or you might use reverse logic and omit the first and third elements. x[c(-1, -3 )] ## [1] 3.14 47405.00 It’s handy to know that T is a stand in for TRUE and F is a stand in for FALSE. You’ll probably notice I tend to use the abbreviations most of the time. x[c(F, T, F, T)] ## [1] 3.14 47405.00 The names() function makes it easy to name the components of a vector. names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) x ## e pi sqrt2 zipcode ## 2.718 3.140 1.414 47405.000 Now we can call the components with their names. x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 Here’s Kruschke’s review: # define a vector x &lt;- c(2.718, 3.14, 1.414, 47405) # name the components names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) # you can indicate which elements you&#39;d like to include x[c(2, 4)] ## pi zipcode ## 3.14 47405.00 # you can decide which to exclude x[c(-1, -3)] ## pi zipcode ## 3.14 47405.00 # or you can use logical tests x[c(F, T, F, T)] ## pi zipcode ## 3.14 47405.00 # and you can use the names themselves x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 3.4.2 Factor. Here are our five-person SES status data. x &lt;- c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;high&quot;, &quot;medium&quot;) x ## [1] &quot;high&quot; &quot;medium&quot; &quot;low&quot; &quot;high&quot; &quot;medium&quot; The factor() function turns them into a factor, which will return the levels when called. xf &lt;- factor(x) xf ## [1] high medium low high medium ## Levels: high low medium Here are the factor levels as numerals. as.numeric(xf) ## [1] 1 3 2 1 3 With the levels and ordered arguments, we can order the factor elements. xfo &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T) xfo ## [1] high medium low high medium ## Levels: low &lt; medium &lt; high Now “high” is a larger integer. as.numeric(xfo) ## [1] 3 2 1 3 2 We’ve already specified xf. xf ## [1] high medium low high medium ## Levels: high low medium And we know how it’s been coded numerically. as.numeric(xf) ## [1] 1 3 2 1 3 We can have levels and labels. xfol &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T, labels = c(&quot;Bottom SES&quot;, &quot;Middle SES&quot;, &quot;Top SES&quot;)) xfol ## [1] Top SES Middle SES Bottom SES Top SES Middle SES ## Levels: Bottom SES &lt; Middle SES &lt; Top SES 3.4.3 Matrix and array. Kruschke uses these more often than I do. I’m more of a vector and data frame kinda guy. Even so, here’s an example of a matrix. matrix(1:6, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 We can get the same thing using nrow. matrix(1:6, nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Note how the numbers got ordered by rows within each column? We can specify them to be ordered across columns, first. matrix(1:6, nrow = 2, byrow = T) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 We can name the dimensions. I’m not completely consistent, but I’ve been moving in the direction of following The Tidyverse Style Guide for naming my R objects and their elements. From the guide, we read Variable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name. By those sensibilities, we’ll name our rows and columns as matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) ## TheColDimName ## TheRowDimName col_1_name col_2_name col_3_name ## row_1_name 1 3 5 ## row_2_name 2 4 6 You’ve also probably noticed that I “always put a space after a comma, never before, just like in regular English,” as well as “put a space before and after = when naming arguments in function calls.” IMO, this makes code easier to read. You do you. We’ll name our matrix x. x &lt;- matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) Since there are 2 dimensions, we’ll subset with two dimensions. Numerical indices work. x[2, 3] ## [1] 6 Row and column names work, too. Just make sure to use quotation marks, &quot;&quot;, for those. x[&quot;row_2_name&quot;, &quot;col_3_name&quot;] ## [1] 6 Here we specify the range of columns to include. x[2, 1:3] ## col_1_name col_2_name col_3_name ## 2 4 6 Leaving that argument blank returns them all. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 And leaving the row index blank returns all row values within the specified column(s). x[, 3] ## row_1_name row_2_name ## 5 6 Mind your commas! This produces the second row, returned as a vector. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 This returns both rows of the 2nd column. x[, 2] ## row_1_name row_2_name ## 3 4 Leaving out the comma will return the numbered element. x[2] ## [1] 2 It’ll be important in your brms career to have a sense of 3-dimensional arrays. Several brms convenience functions often return them (e.g., ranef() in multilevel models). a &lt;- array(1:24, dim = c(3, 4, 2), # 3 rows, 4 columns, 2 layers dimnames = list(RowDimName = c(&quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;), ColDimName = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;), LayDimName = c(&quot;l1&quot;, &quot;l2&quot;))) a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 Since these have 3 dimensions, you have to use 3-dimensional indexing. As with 2-dimensional objects, leaving the indices for a dimension blank will return all elements within that dimension. For example, this code returns all columns of r3 and l2, as a vector. a[&quot;r3&quot;, , &quot;l2&quot;] ## c1 c2 c3 c4 ## 15 18 21 24 And this code returns all layers of r3 and c4, as a vector. a[&quot;r3&quot;, &quot;c4&quot;, ] ## l1 l2 ## 12 24 3.4.4 List and data frame. Here’s my_list. my_list &lt;- list(&quot;a&quot; = 1:3, &quot;b&quot; = matrix(1:6, nrow = 2), &quot;c&quot; = &quot;Hello, world.&quot;) my_list ## $a ## [1] 1 2 3 ## ## $b ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## $c ## [1] &quot;Hello, world.&quot; To return the contents of the a portion of my_list, just execute this. my_list$a ## [1] 1 2 3 We can index further within a. my_list$a[2] ## [1] 2 To return the contents of the first item in our list with the double bracket, [[]], do: my_list[[1]] ## [1] 1 2 3 You can index further to return only the second element of the first list item. my_list[[1]][2] ## [1] 2 But double brackets, [][], are no good, here. my_list[1][2] ## $&lt;NA&gt; ## NULL To learn more, Jenny Bryan has a great talk discussing the role of lists within data wrangling. But here’s a data frame. d &lt;- data.frame(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) d ## integers number_names ## 1 1 one ## 2 2 two ## 3 3 three With data frames, we can continue indexing with the $ operator. d$number_names ## [1] one two three ## Levels: one three two We can also use the double bracket. d[[2]] ## [1] one two three ## Levels: one three two Notice how the single bracket with no comma indexes columns rather than rows. d[2] ## number_names ## 1 one ## 2 two ## 3 three But adding the comma returns the factor-level information when indexing columns. d[, 2] ## [1] one two three ## Levels: one three two It works a touch differently when indexing by row. d[2, ] ## integers number_names ## 2 2 two Let’s try with a tibble, instead. t &lt;- tibble(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) t ## # A tibble: 3 x 2 ## integers number_names ## &lt;int&gt; &lt;chr&gt; ## 1 1 one ## 2 2 two ## 3 3 three One difference is that tibbles default to assigning text columns as character strings rather than factors. Another difference occurs when printing large data frames versus large tibbles. Tibbles yield more compact glimpses. For more, check out R4DS Chapter 10. It’s also worthwhile pointing out that within the tidyverse, you can pull out a specific column with the select() function. Here we select number_names. t %&gt;% select(number_names) ## # A tibble: 3 x 1 ## number_names ## &lt;chr&gt; ## 1 one ## 2 two ## 3 three Go here learn more about select(). 3.5 Loading and saving data 3.5.1 The read.csv read_csv() and read.table read_table() functions. Although read.csv() is the default CSV reader in R, the read_csv() function from the readr package (i.e., one of the core tidyverse packages) is a new alternative. In comparison to base R’s read.csv(), readr::read_csv() is faster and returns tibbles (as opposed to data frames with read.csv()). The same general points hold for base R’s read.table() versus readr::read_table(). Using Kruschke’s HGN.csv example, we’d load the CSV with read_csv() like this: hgn &lt;- read_csv(&quot;data.R/HGN.csv&quot;) Note again that read_csv() defaults to returning columns with character information as characters, not factors. hgn$Hair ## [1] &quot;black&quot; &quot;brown&quot; &quot;blond&quot; &quot;black&quot; &quot;black&quot; &quot;red&quot; &quot;brown&quot; See? As a character variable, Hair no longer has factor level information. But if you knew you wanted to treat Hair as a factor, you could easily convert it with dplyr::mutate(). hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair)) hgn$Hair ## [1] black brown blond black black red brown ## Levels: black blond brown red And here’s a tidyverse way to reorder the levels for the Hair factor. hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair, levels = c(&quot;red&quot;, &quot;blond&quot;, &quot;brown&quot;, &quot;black&quot;))) hgn$Hair ## [1] black brown blond black black red brown ## Levels: red blond brown black as.numeric(hgn$Hair) ## [1] 4 3 2 4 4 1 3 Since we imported hgn with read_csv(), the Name column is already a character vector, which we can verify with the str() function. hgn$Name %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; Note how using as.vector() did nothing in our case. Name was already a character vector. hgn$Name %&gt;% as.vector() %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; The Group column was imported as composed of integers. hgn$Group %&gt;% str() ## num [1:7] 1 1 1 2 2 2 2 Switching Group to a factor is easy enough. hgn &lt;- hgn %&gt;% mutate(Group = factor(Group)) hgn$Group ## [1] 1 1 1 2 2 2 2 ## Levels: 1 2 3.5.2 Saving data from R. Yeah you guessed, readr has a write_csv() function, too. The arguments are as follows: write_csv(x, path, na = &quot;NA&quot;, append = FALSE, col_names = !append). Saving hgn in your working directory is as easy as: write_csv(hgn, &quot;hgn.csv&quot;) You could also use save(). save(hgn, file = &quot;hgn.Rdata&quot; ) Once we start fitting Bayesian models, this method will be an important way to save the results of those models. The load() function is simple. load(&quot;hgn.Rdata&quot; ) The ls() function works very much the same way as the more verbosely-named objects() function. ls() ## [1] &quot;a&quot; &quot;abc&quot; &quot;d&quot; &quot;hgn&quot; &quot;my_list&quot; &quot;step_1&quot; &quot;t&quot; ## [8] &quot;x&quot; &quot;xf&quot; &quot;xfo&quot; &quot;xfol&quot; &quot;y&quot; 3.6 Some utility functions # this is a more compact way to replicate 100 1&#39;s, 200 2&#39;s, and 300 3&#39;s x &lt;- rep(1:3, times = c(100, 200, 300)) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.500 2.333 3.000 3.000 We can use the pipe to convert and then summarize x. x %&gt;% factor() %&gt;% summary() ## 1 2 3 ## 100 200 300 head() and tail() are quite useful. head(x) ## [1] 1 1 1 1 1 1 tail(x) ## [1] 3 3 3 3 3 3 Within the tidyverse, the slice() function serves a similar role. In order to use slice(), we’ll want to convert x, which is just a vector of integers, into a data frame. Then we’ll use slice() to return a subset of the rows. x &lt;- x %&gt;% as_tibble() ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. x %&gt;% slice(1:6) ## # A tibble: 6 x 1 ## value ## &lt;int&gt; ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 So that was analogous to what we accomplished with head(). Here’s the analogue to tail(). x %&gt;% slice(595:600) ## # A tibble: 6 x 1 ## value ## &lt;int&gt; ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 The downside of that code was we had to do the math to determine that \\(600 - 6 = 595\\) in order to get the last six rows, as returned by tail(). A more general approach is to use n(), which will return the total number of rows in the tibble. x %&gt;% slice((n() - 6):n()) ## # A tibble: 7 x 1 ## value ## &lt;int&gt; ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 ## 7 3 To unpack (n() - 6):n(), because n() = 600, (n() - 6) = 600 - 6 = 595. Therefore (n() - 6):n() was equivalent to having coded 595:600. Instead of having to do the math ourselves, n() did it for us. It’s often easier to just go with head() or tail(). But the advantage of this more general approach is that it allows one take more complicated slices of the data, such as returning the first three and last three rows. x %&gt;% slice(c(1:3, (n() - 3):n())) ## # A tibble: 7 x 1 ## value ## &lt;int&gt; ## 1 1 ## 2 1 ## 3 1 ## 4 3 ## 5 3 ## 6 3 ## 7 3 We’ve already used the handy str() function a bit. It’s also nice to know that tidyverse::glimpse() performs a similar function. x %&gt;% str() ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 600 obs. of 1 variable: ## $ value: int 1 1 1 1 1 1 1 1 1 1 ... x %&gt;% glimpse() ## Observations: 600 ## Variables: 1 ## $ value &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… Within the tidyverse, we’d use group_by() and then summarize() as alternatives to the aggregate() function. With group_by() we group the observations first by Hair and then by Gender within Hair. After that, we summarize the groups by taking the median() values of their Number. hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(median = median(Number)) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender median ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 red M 7 ## 2 blond F 3 ## 3 brown F 7 ## 4 black F 7 ## 5 black M 1.5 One of the nice things about this workflow is that the code reads somewhat like how we’d explain what we were doing. We, in effect, told R to Take hgn, then group the data by Hair and Gender within Hair, and then summarize() those groups by their median() Number values. There’s also the nice quality that we don’t have to continually tell R where the data are coming from the way the aggregate() function required Kruschke to prefix each of his variables with HGNdf$. We also didn’t have to explicitly rename the output columns the way Kruschke had to. I’m not aware that our group_by() %&gt;% summarize() workflow has a formula format the way aggregate() does. To count how many levels we had in a grouping factor, we’d use the n() function in summarize(). hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(n = n()) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 Alternatively, we could switch out the summary(n = n()) line with count(). hgn %&gt;% group_by(Hair, Gender) %&gt;% count() ## # A tibble: 5 x 3 ## # Groups: Hair, Gender [5] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 We could then use spread() to convert that output to a format similar to Kruschke’s table of counts. hgn %&gt;% group_by(Hair, Gender) %&gt;% count() %&gt;% spread(key = Hair, value = n) ## # A tibble: 2 x 5 ## # Groups: Gender [2] ## Gender red blond brown black ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 F NA 1 2 1 ## 2 M 1 NA NA 2 With this method, the NAs are stand-ins for 0s. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 apply() is part of a family of functions that offer a wide array of uses. You can learn more about the apply() family here or here. apply(a, MARGIN = c(2, 3), FUN = sum) ## LayDimName ## ColDimName l1 l2 ## c1 6 42 ## c2 15 51 ## c3 24 60 ## c4 33 69 Here’s a. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 The reshape2 package is a precursor to the tidyr package (i.e., one of the core tidyverse packages). The reshape2::melt() function is a quick way to transform the 3-dimensional a matrix into a tidy data frame. a %&gt;% reshape2::melt() ## RowDimName ColDimName LayDimName value ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## 11 r2 c4 l1 11 ## 12 r3 c4 l1 12 ## 13 r1 c1 l2 13 ## 14 r2 c1 l2 14 ## 15 r3 c1 l2 15 ## 16 r1 c2 l2 16 ## 17 r2 c2 l2 17 ## 18 r3 c2 l2 18 ## 19 r1 c3 l2 19 ## 20 r2 c3 l2 20 ## 21 r3 c3 l2 21 ## 22 r1 c4 l2 22 ## 23 r2 c4 l2 23 ## 24 r3 c4 l2 24 We have an alternative if you wanted to stay within the tidyverse. To my knowledge, the fastest way to make the transformation is to first use as.tbl_cube() and follow that up with as_tibble(). The as.tbl_cube() function will convert the a matrix into a tbl_cube. We will use the met_name argument to determine the name of the measure assessed in the data. Since the default is for as.tbl_cube() to name the measure name as ., it seemed value was a more descriptive choice. We’ll then use the as_tibble() function to convert our tbl_cube object into a tidy tibble. a %&gt;% as.tbl_cube(met_name = &quot;value&quot;) %&gt;% as_tibble() ## # A tibble: 24 x 4 ## RowDimName ColDimName LayDimName value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## # … with 14 more rows Notice how the first three columns are returned as characters instead of factors. If you really wanted those to be factors, you could always follow up the code with mutate_if(is.character, as.factor). 3.7 Programming in R It’s worthy to note that this project was done with R Markdown, which is an alternative to an R script. As Grolemund and Wickham point out R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at http://rstudio.com/cheatsheets. I also strongly recommend checking out R Notebooks, which is a kind of R Markdown document but with a few bells a whistles that make it more useful for working scientists. You can learn more about it here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. 3.7.1 Variable names in R. Kruschke prefers to use camelBack notation for his variable and function names. Though I initially hated it, I’ve been moving in the direction of snake_case. It seems easier to read_prose_in_snake_case than it is to readProseInCamelBack. To each their own. 3.7.2 Running a program. See R4DS Chapter 8 for a nice overview on working directories within the context of an RStudio project. 3.7.3 Programming a function. Here’s our simple a_sq_plus_b function. a_sq_plus_b &lt;- function(a, b = 1) { c &lt;- a^2 + b return(c) } If you explicitly denote your arguments, everything works fine. a_sq_plus_b(a = 3, b = 2) ## [1] 11 Keep things explicit and you can switch up the order of the arguments. a_sq_plus_b(b = 2, a = 3) ## [1] 11 But here’s what happens when you are less explicit. # this a_sq_plus_b(3, 2) ## [1] 11 # is not the same as this a_sq_plus_b(2, 3) ## [1] 7 Since we gave b a default value, we can be really lazy. a_sq_plus_b(a = 2) ## [1] 5 But we can’t be lazy with a. This a_sq_plus_b(b = 1) yielded this warning on my computer: “Error in a_sq_plus_b(b = 1) : argument”a&quot; is missing, with no default&quot;. If we’re completely lazy, a_sq_plus_b() presumes our sole input value is for the a argument and it uses the default value of 1 for b. a_sq_plus_b(2) ## [1] 5 The lesson is important because it’s good practice to familiarize yourself with the defaults of the functions you use in statistics and data analysis, more generally. 3.7.4 Conditions and loops. Here’s our starting point for if() and else(). if(x &lt;= 3){ # if x is less than or equal to 3 show(&quot;small&quot;) # display the word &quot;small&quot; } else { # otherwise show(&quot;big&quot;) # display the word &quot;big&quot; } # end of ’else’ clause ## Warning in if (x &lt;= 3) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;small&quot; Yep, this is no good. if (x &lt;= 3) {show(&quot;small&quot;)} else {show(&quot;big&quot;)} On my computer, it returned this message: “the condition has length &gt; 1 and only the first element will be used[1]”small&quot; Error: unexpected ‘else’ in “else”&quot;. Here we use the loop. for (count_down in 5:1) { show(count_down) } ## [1] 5 ## [1] 4 ## [1] 3 ## [1] 2 ## [1] 1 for (note in c(&quot;do&quot;, &quot;re&quot;, &quot;mi&quot;)) { show(note) } ## [1] &quot;do&quot; ## [1] &quot;re&quot; ## [1] &quot;mi&quot; It’s also useful to understand how to use the ifelse() function within the context of a data frame. Recall hos x is a data frame. x &lt;- tibble(x = 1:5) x ## # A tibble: 5 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 We can use the mutate() function to make a new variable, size, which is itself a function of the original variable, x. We’ll use the ifelse() function to return “small” if x &lt;= 3, but to return “big” otherwise. x %&gt;% mutate(size = ifelse(x &lt;= 3, &quot;small&quot;, &quot;big&quot;)) ## # A tibble: 5 x 2 ## x size ## &lt;int&gt; &lt;chr&gt; ## 1 1 small ## 2 2 small ## 3 3 small ## 4 4 big ## 5 5 big You should also know there’s a dplyr alternative, called if_else(). It works quite similarily, but is stricter about type consistency. If you ever get into a situation where you need to do many ifelse() statements or a many-layered ifelse() statement, you might check out dplyr::case_when(). 3.7.5 Measuring processing time. This will be nontrivial to consider in your Bayesian career. Here’s the loop. start_time &lt;- proc.time() y &lt;- vector(mode = &quot;numeric&quot;, length = 1.0E6) for (i in 1:1.0E6) {y[i] &lt;- log(i)} stop_time &lt;- proc.time() elapsed_time_loop &lt;- stop_time - start_time show(elapsed_time_loop) ## user system elapsed ## 0.123 0.009 0.153 Now we use a vector. start_time &lt;- proc.time() y &lt;- log(1:1.0E6) stop_time &lt;- proc.time() elapsed_time_vector &lt;- stop_time - start_time show(elapsed_time_vector) ## user system elapsed ## 0.023 0.009 0.040 Here we compare the two times. elapsed_time_vector[1] / elapsed_time_loop[1] ## user.self ## 0.1869919 For my computer, the vectorized approach took about 18.7% the time the loop approach did. When using R, avoid loops for vectorized approaches whenever possible. As an alternative, when I’m doing analyses like these, I tend to just use Sys.time(). I’m not going to walk them out, here. But as we go along, you might notice I sometimes use functions from the purrr::map() family in places where Kruschke used loops. I think they’re pretty great. 3.7.6 Debugging. This should be no surprise, by now, but in addition to Kruschke’s good advice, I also recommend checking out R4DS. I reference it often. 3.8 Graphical plots: Opening and saving For making and saving plots with ggplot2, I recommend reviewing R4DS Chapters 3 and 28. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.10 reshape2_1.4.3 haven_2.1.0 ## [5] lattice_0.20-38 colorspace_1.4-1 vctrs_0.2.0 generics_0.0.2 ## [9] htmltools_0.4.0 yaml_2.2.0 utf8_1.1.4 rlang_0.4.0 ## [13] pillar_1.4.2 glue_1.3.1 withr_2.1.2 modelr_0.1.4 ## [17] readxl_1.3.1 plyr_1.8.4 lifecycle_0.1.0 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.4 evaluate_0.14 ## [25] labeling_0.3 knitr_1.23 fansi_0.4.0 broom_0.5.2 ## [29] Rcpp_1.0.2 scales_1.0.0 backports_1.1.5 jsonlite_1.6 ## [33] hms_0.4.2 digest_0.6.21 stringi_1.4.3 grid_3.6.0 ## [37] cli_1.1.0 tools_3.6.0 magrittr_1.5 lazyeval_0.2.2 ## [41] crayon_1.3.4 pkgconfig_2.0.3 zeallot_0.1.0 xml2_1.2.0 ## [45] lubridate_1.7.4 assertthat_0.2.1 rmarkdown_1.13 httr_1.4.0 ## [49] rstudioapi_0.10 R6_2.4.0 nlme_3.1-139 compiler_3.6.0 "],
["what-is-this-stuff-called-probability.html", "4 What is This Stuff Called Probability? 4.1 The set of all possible events 4.2 Probability: Outside or inside the head 4.3 Probability distributions 4.4 Two-way distributions Reference Session info", " 4 What is This Stuff Called Probability? Inferential statistical techniques assign precise measures to our uncertainty about possibilities. Uncertainty is measured in terms of probability, and therefore we must establish the properties of probability before we can make inferences about it. This chapter introduces the basic ideas of probability. (p. 71, emphasis in the original) 4.1 The set of all possible events This snip from page 72 is important (emphasis in the original): Whenever we ask about how likely an outcome is, we always ask with a set of possible outcomes in mind. This set exhausts all possible outcomes, and the outcomes are all mutually exclusive. This set is called the sample space. 4.2 Probability: Outside or inside the head It’s worthwhile to quote this section in full. Sometimes we talk about probabilities of outcomes that are “out there” in the world. The face of a flipped coin is such an outcome: We can observe the flip, and the probability of coming up heads can be estimated by observing several flips. But sometimes we talk about probabilities of things that are not so clearly “out there,” and instead are just possible beliefs “inside the head.” Our belief about the fairness of a coin is an example of something inside the head. The coin may have an intrinsic physical bias, but now I am referring to our belief about the bias. Our beliefs refer to a space of mutually exclusive and exhaustive possibilities. It might be strange to say that we randomly sample from our beliefs, like we randomly sample from a sack of coins. Nevertheless, the mathematical properties of probabilities outside the head and beliefs inside the head are the same in their essentials, as we will see. (pp. 73–74, emphasis in the original) 4.2.1 Outside the head: Long-run relative frequency. For events outside the head, it’s intuitive to think of probability as being the long-run relative frequency of each possible outcome… We can determine the long-run relative frequency by two different ways. One way is to approximate it by actually sampling from the space many times and tallying the number of times each event happens. A second way is by deriving it mathematically. These two methods are now explored in turn. (p. 74) 4.2.1.1 Simulating a long-run relative frequency. Before we try coding the simulation, we’ll first load the tidyverse. library(tidyverse) Now run the simulation. n &lt;- 500 # specify the total number of flips p_heads &lt;- 0.5 # specify underlying probability of heads # Kruschke reported this was the seed he used at the top of page 94 set.seed(47405) # here we use that seed to flip a coin n times and compute the running proportion of heads at each flip. # we generate a random sample of n flips (heads = 1, tails = 0) d &lt;- tibble(flip_sequence = sample(x = c(0, 1), prob = c(1 - p_heads, p_heads), size = n, replace = T), r = cumsum(flip_sequence), n = 1:n) %&gt;% mutate(run_prop = r / n) end_prop &lt;- d %&gt;% select(run_prop) %&gt;% slice(n()) %&gt;% round(digits = 3) %&gt;% pull() Now we’re ready to make Figure 4.1. d %&gt;% filter(n &lt; 1000) %&gt;% # this step cuts down on the time it takes to make the plot ggplot(aes(x = n, y = run_prop)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line(color = &quot;grey50&quot;) + geom_point(color = &quot;grey50&quot;, alpha = 1/4) + scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 200, 500)) + coord_cartesian(xlim = 1:500, ylim = 0:1) + labs(title = &quot;Running proportion of heads&quot;, subtitle = paste(&quot;Our end proportion =&quot;, end_prop), x = &quot;Flip number&quot;, y = &quot;Proportion of heads&quot;) + theme(panel.grid = element_blank()) 4.2.1.2 Deriving a long-run relative frequency. Sometimes, when the situation is simple enough mathematically, we can derive the exact long-run relative frequency. The case of the fair coin is one such simple situation. The sample space of the coin consists of two possible outcomes, head and tail. By the assumption of fairness, we know that each outcome is equally likely. Therefore, the long-run relative frequency of heads should be exactly one out of two, i.e., 1/2, and the long-run relative frequency of tails should also be exactly 1/2. (p. 76) 4.2.2 Inside the head: Subjective belief. To specify our subjective beliefs, we have to specify how likely we think each possible outcome is. It can be hard to pin down mushy intuitive beliefs. In the next section, we explore one way to “calibrate” subjective beliefs, and in the subsequent section we discuss ways to mathematically describe degrees of belief. (p. 76) 4.2.3 Probabilities assign numbers to possibilities. In general, a probability, whether it’s outside the head or inside the head, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called “probabilities,” merely need to satisfy three properties (Kolmogorov, 1956): A probability value must be nonnegative (i.e., zero or positive). The sum of the probabilities across all events in the entire sample space must be 1.0 (i.e., one of the events in the space must happen, otherwise the space does not exhaust all possibilities). For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities. For example, the probability that a fair six-sided die comes up 3-dots or 4-dots is 1/6 + 1/6 = 2/6. Any assignment of numbers to events that respects those three properties will also have all the properties of probabilities that we will discuss below. (pp. 77–78, emphasis in the original) 4.3 Probability distributions “A probability distribution is simply a list of all possible outcomes and their corresponding probabilities” (p. 78, emphasis in the original) 4.3.1 Discrete distributions: Probability mass. When the sample space consists of discrete outcomes, then we can talk about the probability of each distinct outcome. For example, the sample space of a flipped coin has two discrete outcomes, and we talk about the probability of head or tail… For continuous outcome spaces, we can discretize the space into a finite set of mutually exclusive and exhaustive “bins.” (p. 78, emphasis in the original) In order to recreate Figure 4.2, we need to generate the heights data. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height data of the kind in his text. Here is the code: HtWtDataGenerator &lt;- function(n_subj, rndsd = NULL, male_prob = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} data_matrix &lt;- matrix(0, nrow = n_subj, ncol = 3) colnames(data_matrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:n_subj) { # Flip coin to decide sex sex = sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(male_prob, 1 - male_prob)) if (sex == maleval) {datum &lt;- MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } data_matrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(data_matrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values are generated and how probable we want the values to be based on those from men. These are controlled by the n_subj and male_prob parameters. set.seed(4) d &lt;- HtWtDataGenerator(n_subj = 10000, male_prob = .5) %&gt;% as_tibble() %&gt;% mutate(person = 1:n()) d %&gt;% head() ## # A tibble: 6 x 4 ## male height weight person ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 76 222. 1 ## 2 0 59.5 190 2 ## 3 0 60.2 118. 3 ## 4 1 64.1 138. 4 ## 5 1 69.3 148. 5 ## 6 1 69.1 166. 6 For Figure 4.2, we’ll make extensive use of the case_when() syntax, which you can learn more about from hrbrmstr’s Making a Case for case_when. d_bin &lt;- d %&gt;% mutate(bin = case_when( height &lt; 51 ~ 51, between(height, 51, 53) ~ 53, between(height, 53, 55) ~ 55, between(height, 55, 57) ~ 57, between(height, 57, 59) ~ 59, between(height, 59, 61) ~ 61, between(height, 61, 63) ~ 63, between(height, 63, 65) ~ 65, between(height, 65, 67) ~ 67, between(height, 67, 69) ~ 69, between(height, 69, 71) ~ 71, between(height, 71, 73) ~ 73, between(height, 73, 75) ~ 75, between(height, 75, 77) ~ 77, between(height, 77, 79) ~ 79, between(height, 79, 81) ~ 71, between(height, 81, 83) ~ 83, height &gt; 83 ~ 85) ) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 1) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 2), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) Because we’re simulating and we don’t know what seed number Kruschke used for his plot, ours will differ a little from his. But the overall pattern is the same. It’s a little less work to make Figure 4.2.b. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), binwidth = 2, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = c(0, .04, .08)) + coord_cartesian(xlim = 51:83) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) Our data binning approach for Figure 4.2.c will be a little different than what we did, above. Here we’ll make our bins with the round() function. d_bin &lt;- d %&gt;% mutate(bin = round(height, digits = 0)) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 0.5) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 1), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) However, our method for Figure 4.2.d will be like what we did, before. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = c(0, .04, .08)) + coord_cartesian(xlim = 51:83) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) The probability of a discrete outcome, such as the probability of falling into an interval on a continuous scale, is referred to as a probability mass. Loosely speaking, the term “mass” refers the amount of stuff in an object. When the stuff is probability and the object is an interval of a scale, then the mass is the proportion of the outcomes in the interval. (p. 80, emphasis in the original) 4.3.2 Continuous distributions: Rendezvous with density. If you think carefully about a continuous outcome space, you realize that it becomes problematic to talk about the probability of a specific value on the continuum, as opposed to an interval on the continuum… Therefore, what we will do is make the intervals infinitesimally narrow, and instead of talking about the infinitesimal probability mass of each infinitesimal interval, we will talk about the ratio of the probability mass to the interval width. That ratio is called the probability density. Loosely speaking, density is the amount of stuff per unit of space it takes up. Because we are measuring amount of stuff by its mass, then density is the mass divided by the amount space it occupies. (p. 80, emphasis in the original) To make Figure 4.3, we’ll need new data. set.seed(4) d &lt;- tibble(height = rnorm(1e4, mean = 84, sd = .1)) %&gt;% mutate(door = 1:n()) d %&gt;% head() ## # A tibble: 6 x 2 ## height door ## &lt;dbl&gt; &lt;int&gt; ## 1 84.0 1 ## 2 83.9 2 ## 3 84.1 3 ## 4 84.1 4 ## 5 84.2 5 ## 6 84.1 6 To make the bins for our version of Figure 4.3.a, we could use the case_when() approach from above. However, that would require some tedious code. Happily, we have an alternative in the santoku package. We can use the santoku::chop() function to discretize our height values. Here we’ll walk through the first part. # devtools::install_github(&quot;hughjonesd/santoku&quot;) library(santoku) d_bin &lt;- d %&gt;% mutate(bin = chop(height, breaks = seq(from = 83.6, to = 84.4, length.out = 31), labels = seq(from = 83.6, to = 84.4, length.out = 31)[-1])) head(d_bin) ## # A tibble: 6 x 3 ## height door bin ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 84.0 1 84.0266666666667 ## 2 83.9 2 83.9466666666667 ## 3 84.1 3 84.1066666666667 ## 4 84.1 4 84.08 ## 5 84.2 5 84.1866666666667 ## 6 84.1 6 84.08 We’ve labeled our bin levels by their upper bounds. Note how they are saved as factors. To make use of those values in our plot, we’ll need to convert them to numerals. Here we make that conversion and complete the data wrangling. d_bin &lt;- d_bin %&gt;% mutate(bin = as.character(bin) %&gt;% as.double()) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - (83.62667 - 83.6) / 2) head(d_bin) ## # A tibble: 6 x 3 ## bin n height ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83.7 5 83.6 ## 2 83.7 5 83.7 ## 3 83.7 7 83.7 ## 4 83.7 24 83.7 ## 5 83.8 37 83.7 ## 6 83.8 86 83.8 Now we plot. d %&gt;% ggplot(aes(x = height, y = door)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 83.6, to = 84.4, length.out = 31), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Door #&quot;) + theme(panel.grid = element_blank()) Figure 4.3.b is a breeze. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = .025, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = 0:4) + coord_cartesian(xlim = c(83.6, 84.4)) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) 4.3.2.1 Properties of probability density functions. 4.3.2.2 The normal probability density function. We’ll use dnorm() again to make our version of Figure 4.4. tibble(x = seq(from = -.8, to = .8, by = .02)) %&gt;% mutate(p = dnorm(x, mean = 0, sd = .2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = p), color = &quot;grey50&quot;, size = 1.25) + geom_linerange(aes(ymin = 0, ymax = p), size = 1/3) + coord_cartesian(xlim = c(-.61, .61)) + labs(title = &quot;Normal probability density&quot;, subtitle = expression(paste(mu, &quot; = 0 and &quot;, sigma, &quot; = 0.2&quot;)), y = &quot;p(x)&quot;) + theme(panel.grid = element_blank()) The equation for the normal probability density follows the form \\[ p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\Bigg ( - \\frac{1}{2} \\bigg [ \\frac{x - \\mu}{\\sigma}^2 \\bigg ] \\Bigg ), \\] where \\(\\mu\\) governs the mean and \\(\\sigma\\) governs the standard deviation. 4.3.3 Mean and variance of a distribution. The mean of a probability distribution is also called the expected value, which follow the form \\[E[x] = \\sum_x p(x) x\\] when \\(x\\) is discrete. For continuous \\(x\\) values, the formula is \\[E[x] = \\int \\text d x \\; p(x) x.\\] The variance is defined as the mean squared deviation from the mean, \\[\\text{var}_x = \\int \\text d x \\; p(x) (x - E[x])^2.\\] If you take the square root of the variance, you get the standard deviation. 4.3.4 Highest density interval (HDI). The HDI indicates which points of a distribution are most credible, and which cover most of the distribution. Thus, the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say 95% of it, such that every point inside the interval has higher credibility than any point outside the interval. (p. 87) In chapter 10 (p. 294), Kruschke briefly mentions his HDIofICDF() function, the code for which you can find in his DBDA2E-utilities.R file. It’s a handy function which we’ll put to use from time to time. Here’s a mild reworking of his code. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Now we already know from the text, and perhaps from prior experience, what the 95% HDIs for the unit normal. But it’s nice to be able to confirm that with a function. h &lt;- hdi_of_icdf(name = qnorm, mean = 0, sd = 1) h ## [1] -1.959964 1.959964 Now we’ve saved those values in h, we can use then to make our version of Figure 4.5.a. tibble(x = seq(from = -3.5, to = 3.5, by = .05)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;=h[1] &amp; x &lt;= h[2]), aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .02, h[2] - .02), y = c(.059, .059)), aes(y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 0, y = .09, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(-3.1, 3.1) + labs(y = &quot;p(x)&quot;) + theme(panel.grid = element_blank()) As far as I could tell, Figure 4.5.b is of a beta distribution, which Kruschke covers in greater detail starting in chapter 6. I got the shape1 and shape2 values from playing around. If you have a more principled approach, do share. But anyway, we can use our hdi_of_icdf() funciton to ge the correct values. h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 4) h ## [1] 0.6103498 0.9507510 Let’s put those h values to work. tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x, shape1 = 15, shape2 = 4)), fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;=h[1] &amp; x &lt;= h[2]), aes(ymin = 0, ymax = dbeta(x, shape1 = 15, shape2 = 4)), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .01, h[2] - .002), y = c(.75, .75)), aes(x = x, y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = .8, y = 1.1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(.4, 1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) Figure 4.5.c was also a lot of trial and error. It seemed the easiest way to reproduce the shape was to mash two Gaussians together. After playing around with rnorm(), I ended up with this. set.seed(4) d &lt;- tibble(x = c(rnorm(6e5, mean = 1.50, sd = .5), rnorm(4e5, mean = 4.75, sd = .5))) glimpse(d) ## Observations: 1,000,000 ## Variables: 1 ## $ x &lt;dbl&gt; 1.6083774, 1.2287537, 1.9455723, 1.7979903, 2.3178090, 1.84463… As you’ll see, it’s not exactly right. But it’s close enough to give you a sense of what’s going on. But anyway, since we’re working with simulated data rather than an analytic solution, we’ll want to use a powerful convenience function from the tidybayes package. library(tidybayes) Kay’s tidybayes package provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme [median|mean|mode]_[qi|hdi], for example, median_qi, mean_qi, mode_hdi, and so on. The first name (before the _) indicates the type of point summary, and the second name indicates the type of interval. qi yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and hdi yields a highest (posterior) density interval. (from here) Here we’ll use mode_hdi() to compute the HDIs and put them in a tibble. We’ll be using a lot of mode_hdi() in this project. h &lt;- d %&gt;% mode_hdi() h ## # A tibble: 2 x 6 ## x .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.50 0.458 2.55 0.95 mode hdi ## 2 1.50 3.82 5.68 0.95 mode hdi Usually, mode_hdi() will return a tibble with just one row. But in this case, since we had a bimodal distribution, it returned two rows—one for each of the two distinct regions. Oh, and in case it wasn’t clear, that first column x is the measure of central tendency—the mode, in this case. Though I acknowledge, it’s a little odd to speak of central tendency in a bimodal distribution. Again, this won’t happen much. In order to fill the bimodal density with the split HDIs, you need to use the density() function to transform the d data to a tibble with the values for the x-axis in an x vector and the corresponding density values in a y vector. dens &lt;- d$x %&gt;% density() %&gt;% with(tibble(x, y)) head(dens) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.0000000503 ## 2 -1.09 0.0000000822 ## 3 -1.08 0.000000131 ## 4 -1.06 0.000000201 ## 5 -1.04 0.000000304 ## 6 -1.03 0.000000449 We’re finally ready to plot. Forgive me. It’s a monster. ggplot(data = dens, aes(x = x, y = y)) + geom_ribbon(aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey75&quot;) + # note the use of `pull()`, which extracts the values, rather than return a tibble geom_ribbon(data = dens %&gt;% filter(x &gt; h[1, 2] %&gt;% pull() &amp; x &lt; h[1, 3] %&gt;% pull()), aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey50&quot;) + geom_ribbon(data = dens %&gt;% filter(x &gt; h[2, 2] %&gt;% pull() &amp; x &lt; h[2, 3] %&gt;% pull()), aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1, 2] %&gt;% pull(), h[1, 3] %&gt;% pull()), y = c(.06, .06)), aes(x = x, y = y), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_line(data = tibble(x = c(h[2, 2] %&gt;% pull(), h[2, 3] %&gt;% pull()), y = c(.06, .06)), aes(x = x, y = y), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 1.5, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 4.75, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = 0:6, limits = c(0, 6.3)) + scale_y_continuous(&quot;p(x)&quot;, breaks = c(0, .1, .2, .3, .4, .5)) + theme(panel.grid = element_blank()) 4.4 Two-way distributions Here’s a way to compute the marginals from the inner cells of Table 4.1. d1 &lt;- tibble(eye_color = c(&quot;brown&quot;, &quot;blue&quot;, &quot;hazel&quot;, &quot;green&quot;), black = c(.11, .03, .03, .01), brunette = c(.2, .14, .09, .05), red = c(.04, .03, .02, .02), blond = c(.01, .16, .02, .03)) %&gt;% mutate(marginal_eye_color = black + brunette + red + blond) d2 &lt;- d1 %&gt;% summarise_if(is.double, sum) %&gt;% mutate(eye_color = &quot;marginal_hair_color&quot;) %&gt;% select(eye_color, everything()) bind_rows(d1, d2) ## # A tibble: 5 x 6 ## eye_color black brunette red blond marginal_eye_color ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 brown 0.11 0.2 0.04 0.01 0.36 ## 2 blue 0.03 0.14 0.03 0.16 0.36 ## 3 hazel 0.03 0.09 0.02 0.02 0.160 ## 4 green 0.01 0.05 0.02 0.03 0.11 ## 5 marginal_hair_color 0.18 0.48 0.11 0.22 0.99 Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 santoku_0.2.0.9000 forcats_0.4.0 ## [4] stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [7] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ## [10] ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] ggstance_0.3.2 tidyselect_0.2.5 ## [3] xfun_0.10 haven_2.1.0 ## [5] lattice_0.20-38 colorspace_1.4-1 ## [7] vctrs_0.2.0 generics_0.0.2 ## [9] htmltools_0.4.0 yaml_2.2.0 ## [11] utf8_1.1.4 rlang_0.4.0 ## [13] pillar_1.4.2 glue_1.3.1 ## [15] withr_2.1.2 HDInterval_0.2.0 ## [17] modelr_0.1.4 readxl_1.3.1 ## [19] plyr_1.8.4 lifecycle_0.1.0 ## [21] munsell_0.5.0 gtable_0.3.0 ## [23] cellranger_1.1.0 rvest_0.3.4 ## [25] coda_0.19-3 evaluate_0.14 ## [27] labeling_0.3 knitr_1.23 ## [29] fansi_0.4.0 broom_0.5.2 ## [31] Rcpp_1.0.2 arrayhelpers_1.0-20160527 ## [33] scales_1.0.0 backports_1.1.5 ## [35] jsonlite_1.6 svUnit_0.7-12 ## [37] hms_0.4.2 digest_0.6.21 ## [39] stringi_1.4.3 grid_3.6.0 ## [41] cli_1.1.0 tools_3.6.0 ## [43] magrittr_1.5 lazyeval_0.2.2 ## [45] pacman_0.5.1 crayon_1.3.4 ## [47] pkgconfig_2.0.3 zeallot_0.1.0 ## [49] MASS_7.3-51.4 xml2_1.2.0 ## [51] lubridate_1.7.4 assertthat_0.2.1 ## [53] rmarkdown_1.13 httr_1.4.0 ## [55] rstudioapi_0.10 R6_2.4.0 ## [57] nlme_3.1-139 compiler_3.6.0 "],
["bayes-rule.html", "5 Bayes’ Rule 5.1 Bayes’ rule 5.2 Applied to parameters and data 5.3 Complete examples: Estimating bias in a coin 5.4 Why Bayesian inference can be difficult Reference Session info", " 5 Bayes’ Rule “Bayes’ rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data” (pp. 99–100). 5.1 Bayes’ rule Thomas Bayes (1702-1761) was a mathematician and Presbyterian minister in England. His famous theorem was published posthumously in 1763, thanks to the extensive editorial efforts of his friend, Richard Price (Bayes &amp; Price, 1763). The simple rule has vast ramifications for statistical inference, and therefore as long as his name is attached to the rule, we’ll continue to see his name in textbooks. But Bayes himself probably was not fully aware of these ramifications, and many historians argue that it is Bayes’ successor, Pierre-Simon Laplace (1749-1827), whose name should really label this type of analysis, because it was Laplace who independently rediscovered and extensively developed the methods (e.g., Dale, 1999; McGrayne, 2011). (p. 100) I do recommend checking out McGrayne’s book It’s an easy and entertaining read. For a sneak preview, why not listen to her discuss the main themes she covered in her book? 5.1.1 Derived from definitions of conditional probability. With equations 5.5 and 5.6, Kruschke gave us Bayes’ rule in terms of \\(c\\) and \\(r\\). Equation 5.5 was \\[p(c|r) = \\frac{p(r|c)p(c)}{p(r)}.\\] Since \\(p(r) = \\sum_{c^*}p(r|c^*)p(c^*)\\), we can re-express that as Equation 5.6: \\[p(c|r) = \\frac{p(r|c)p(c)}{\\sum_{c^*}p(r|c^*)p(c^*)},\\] where \\(c^*\\) “in the denominator is a variable that takes on all possible values” of \\(c\\) (p. 101). 5.2 Applied to parameters and data Here we get those equations re-expressed in the terms data analysts tend to think with, parameters (i.e., \\(\\theta\\)) and data (i.e., \\(D\\)). \\[ \\begin{align*} p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\;\\; \\text{and since} \\\\ p(D) &amp; = \\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*) \\;\\; \\text{it&#39;s also the case that} \\\\ p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{\\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*)}. \\end{align*} \\] As in the previous section where we spoke in terms of \\(r\\) and \\(c\\), our updated \\(\\theta^*\\) notation is meant to indicate all possible values of \\(\\theta\\). For practice, it’s worth repeating how Kruschke broke this down with Equation 5.7, \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; = \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; / \\; \\underbrace{p(D)}_\\text{evidence}. \\] The “prior,” \\(p(\\theta)\\), is the credibility of the \\(\\theta\\) values without the data \\(D\\). The “posterior,” \\(p(\\theta|D)\\), is the credibility of \\(\\theta\\) values with the data \\(D\\) taken into account. The “likelihood,” \\(p(D|\\theta)\\), is the probability that the data could be generated by the model with parameter value \\(\\theta\\). The “evidence” for the model, \\(p(D)\\), is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106–107) And don’t forget, “evidence” is short “marginal likelihood,” which is the term we’ll use in some of our code, below. 5.3 Complete examples: Estimating bias in a coin Here’s a way to make Figure 5.1.a. library(tidyverse) tibble(theta = seq(from = 0, to = 1, by = .1), prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Prior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) If you were curious, it is indeed the case that those prior values sum to 1. tibble(prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% summarise(s = sum(prior)) ## # A tibble: 1 x 1 ## s ## &lt;dbl&gt; ## 1 1 If we follow Kruschke’s equation 5.10 (i.e., the Bernoulli function) closely, we can express it as a function in R. bernoulli &lt;- function(theta, y){ return(theta^y * (1 - theta)^(1 - y)) } To get a sense of how it works, consider a single coin flip of heads when heads is considered a successful trial. We’ll call the single successful trial y = 1. We can use our custom bernoulli() function to compute the likelihood of different values of \\(\\theta\\). We’ll look at 11 candidate \\(\\theta\\) values, which we’ll call theta_sequence. theta_sequence &lt;- seq(from = 0, to = 1, by = .1) bernoulli(theta = theta_sequence, y = 1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Notice how our theta_sequence corresponds nicely with the sequence of \\(\\theta\\) values on the x-axes of Figure 5.1. We can combine theta_sequence and our bernoulli() function to make the middle panel of Figure 5.1 tibble(x = theta_sequence) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% ggplot(aes(x = x, y = likelihood)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Likelihood&quot;, x = expression(theta), y = expression(paste(&quot;p(D|&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) In order to compute \\(p(D)\\) (i.e., the evidence or the marginal likelihood), we’ll need to multiply our respective prior and likelihood values for each point in our theta sequence and then sum all that up. That sum will be our marginal likelihood. With that cleared up, we can make Figure 5.1.c. tibble(theta = theta_sequence, prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Posterior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) 5.3.1 Influence of sample size on the posterior. In order to follow along with this section, we’re going to have to update our Bernoulli likelihood function so it can accommodate more than a single trial. We’ll anticipate chapter 6 and call our more general function the bernoulli_likelihood(). bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) return(theta^sum(data) * (1 - theta)^(n - sum(data))) } Here’s the work required to make our version of the left portion of Figure 5.2. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) We’ll follow the same procedure to make the right portion of Figure 5.2. The only difference is how we switched from small_data to large_data. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) With just an \\(N = 40\\), the likelihood already dominated the posterior. But this is also a function of our fairly gentle prior. 5.3.2 Influence of prior on the posterior. It’s not immediately obvious how Kruschke made his prior distributions for Figure 5.3. However, hidden away in his BernGridExample.R file he indicated that to get the distribution for the left side of Figure 5.3, you simply raise the prior from the left of Figure 5.2 to the 0.1 power. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^0.1) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) The trick is similar for the right half of Figure 5.3. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^10) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) Bayesian inference is intuitively rational: With a strongly informed prior that uses a lot of previous data to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. But with a weakly informed prior that spreads credibility over a wide range of parameter values, it takes relatively little data to shift the peak of the posterior distribution toward the data (although the posterior will be relatively wide and uncertain). (p. 114) 5.4 Why Bayesian inference can be difficult Determining the posterior distribution directly from Bayes, rule involves computing the evidence (a.k.a. marginal likelihood) in Equations 5.8 and 5.9. In the usual case of continuous parameters, the integral in Equation 5.9 can be impossible to solve analytically. Historically, the difficulty of the integration was addressed by restricting models to relatively simple likelihood functions with corresponding formulas for prior distributions, called conjugate priors, that “played nice” with the likelihood function to produce a tractable integral. (p. 115, emphasis in the original) However, the simple model + conjugate prior approach has its limitations. As we’ll see, we often want to fit complex models without shackling ourselves with conjugate priors—which can be quite a pain to work with. Happily, another kind of approximation involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution. In recent decades, many such algorithms have been developed, generally referred to as Markov chain Monte Carlo (MCMC) methods. What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models without computing the integral in Bayes’ rule. It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. (pp. 115–116, emphasis in the original) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.10 haven_2.1.0 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.0 generics_0.0.2 htmltools_0.4.0 ## [9] yaml_2.2.0 utf8_1.1.4 rlang_0.4.0 pillar_1.4.2 ## [13] glue_1.3.1 withr_2.1.2 modelr_0.1.4 readxl_1.3.1 ## [17] lifecycle_0.1.0 munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 ## [21] rvest_0.3.4 evaluate_0.14 labeling_0.3 knitr_1.23 ## [25] fansi_0.4.0 broom_0.5.2 Rcpp_1.0.2 scales_1.0.0 ## [29] backports_1.1.5 jsonlite_1.6 hms_0.4.2 digest_0.6.21 ## [33] stringi_1.4.3 grid_3.6.0 cli_1.1.0 tools_3.6.0 ## [37] magrittr_1.5 lazyeval_0.2.2 crayon_1.3.4 pkgconfig_2.0.3 ## [41] zeallot_0.1.0 ellipsis_0.3.0 xml2_1.2.0 lubridate_1.7.4 ## [45] assertthat_0.2.1 rmarkdown_1.13 httr_1.4.0 rstudioapi_0.10 ## [49] R6_2.4.0 nlme_3.1-139 compiler_3.6.0 In the next few chapters, we will develop all the foundational concepts and methods of Bayesian data analysis, which are applied to the simplest type of data. Because of the simplicity of the data, we can focus on the Bayesian methods and scaffold the concepts clearly and efficiently. The subsequent part of the book applies the methods developed in this part to more complex data structures. (p. 121) "],
["inferring-a-binomial-probability-via-exact-mathematical-analysis.html", "6 Inferring a Binomial Probability via Exact Mathematical Analysis 6.1 The likelihood function: The Bernoulli distribution 6.2 A description of credibilities: The beta distribution 6.3 The posterior beta 6.4 Examples 6.5 Summary Reference Session info", " 6 Inferring a Binomial Probability via Exact Mathematical Analysis This chapter presents an example of how to do Bayesian inference using pure analytical mathematics without any approximations. Ultimately, we will not use the pure analytical approach for complex applications, but this chapter is important for two reasons. First, the relatively simple mathematics in this chapter nicely reveal the underlying concepts of Bayesian inference on a continuous parameter. The simple formulas show how the continuous allocation of credibility changes systematically as data accumulate. The examples provide an important conceptual foundation for subsequent approximation methods, because the examples give you a clear sense of what is being approximated. Second, the distributions introduced in this chapter, especially the beta distribution, will be used repeatedly in subsequent chapters. (p. 123, emphasis added) 6.1 The likelihood function: The Bernoulli distribution If we denote a set of possible outcomes as \\(\\{y_i\\}\\) Kruschke’s Bernoulli likelihood function for a set of \\(N\\) trials follows the form \\[p(\\{y_i\\} | \\theta) = \\theta^z \\cdot (1 - \\theta) ^ {N - z},\\] where \\(z\\) is the number of 1s in the data (i.e., heads in a series of coin flips) and the sole parameter a given observation will be a 1 is \\(\\theta\\) (i.e., the probability; \\(p(y_i = 1 | \\theta)\\)). If you follow that equation closely, here is how we might express it in R. bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } This will come in handy in just a bit. 6.2 A description of credibilities: The beta distribution In this chapter, we use purely mathematical analysis, with no numerical approximation, to derive the mathematical form of the posterior credibilities of parameter values. To do this, we need a mathematical description of the prior allocation of credibilities… In principle, we could use any probability density function supported on the interval [0, 1]. When we intend to apply Bayes’ rule (Equation 5.7, p. 106), however, there are two desiderata for mathematical tractability. First, it would be convenient if the product of \\(p(y | \\theta)\\) and \\(p(\\theta)\\), which is in the numerator of Bayes’ rule, results in a function of the same form as \\(p(\\theta)\\)… Second, we desire the denominator of Bayes’ rule (Equation 5.9, p. 107), namely \\(\\int \\text d \\; \\theta p(y | \\theta) p(\\theta)\\), to be solvable analytically. This quality also depends on how the form of the function \\(p(\\theta)\\) relates to the form of the function \\(p(y | \\theta)\\). When the forms of \\(p(y | \\theta)\\) and \\(p(\\theta)\\) combine so that the posterior distribution has the same form as the prior distribution, then \\(p(\\theta)\\) is called a conjugate prior for \\(p(y | \\theta)\\). (p. 127 emphasis in the original) When we want a conjugate prior for \\(\\theta\\) of the Bernoulli likelihood, the beta distribution is a handy choice. Beta has two parameters, \\(a\\) and \\(b\\) (also sometimes called \\(\\alpha\\) and \\(\\beta\\)), and the density is defined as \\[\\begin{align*} p(\\theta | a, b) &amp; = \\operatorname{beta} (\\theta | a, b) \\\\ &amp; = \\frac{\\theta^{(a - 1)} (1 - \\theta)^{(b - 1)}}{B(a, b)}, \\end{align*}\\] where \\(B(a, b)\\) is a normalizing constant, keeping the results in a probability metric. Kruschke then clarifies that the beta distribution and the Beta function are not the same. In R, we use the beta density with the dbeta() function, whereas we use the Beta function with beta(). In this project, we’ll primarily use dbeta(). But to give a sense, notice that when given the same input for \\(a\\) and \\(b\\), the two functions return very different values. theta &lt;- .5 a &lt;- 3 b &lt;- 3 dbeta(theta, a, b) ## [1] 1.875 beta(a, b) ## [1] 0.03333333 The \\(a\\) and \\(b\\) parameters are also called shape parameters. And indeed, if we look at the parameters of the dbeta() function in R, we’ll see that \\(a\\) is called shape1 and \\(b\\) is called shape2. print(dbeta) ## function (x, shape1, shape2, ncp = 0, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_dbeta, x, shape1, shape2, log) ## else .Call(C_dnbeta, x, shape1, shape2, ncp, log) ## } ## &lt;bytecode: 0x7feee4218d48&gt; ## &lt;environment: namespace:stats&gt; You can learn more about the dbeta() function here. Before we make Figure 6.1, we’ll need some data. library(tidyverse) length &lt;- 1e4 d &lt;- tibble(shape1 = c(.1, 1:4)) %&gt;% expand(shape1, shape2 = c(.1, 1:4)) %&gt;% expand(nesting(shape1, shape2), x = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(a = str_c(&quot;a =&quot;, shape1), b = str_c(&quot;b =&quot;, shape2), group = rep(1:length, each = 25)) head(d) ## # A tibble: 6 x 6 ## shape1 shape2 x a b group ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 0.1 0.1 0 a =0.1 b =0.1 1 ## 2 0.1 0.1 0.000100 a =0.1 b =0.1 1 ## 3 0.1 0.1 0.000200 a =0.1 b =0.1 1 ## 4 0.1 0.1 0.000300 a =0.1 b =0.1 1 ## 5 0.1 0.1 0.000400 a =0.1 b =0.1 1 ## 6 0.1 0.1 0.000500 a =0.1 b =0.1 1 Now we’re ready for our Figure 6.1. d %&gt;% ggplot(aes(x = x, group = group)) + geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)), color = &quot;grey50&quot;, size = 1.25) + scale_x_continuous(breaks = c(0, .5, 1)) + coord_cartesian(ylim = 0:3) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|a, b)&quot;))) + theme(panel.grid = element_blank()) + facet_grid(b~a) Notice that as \\(a\\) gets bigger (left to right across columns of Figure 6.1), the bulk of the distribution moves rightward over higher values of \\(\\theta\\), but as \\(b\\) gets bigger (top to bottom across rows of Figure 6.1), the bulk of the distribution moves leftward over lower values of \\(\\theta\\). Notice that as \\(a\\) and \\(b\\) get bigger together, the beta distribution gets narrower. (p. 127). 6.2.1 Specifying a beta prior. It is useful to know the central tendency and spread of the beta distribution expressed in terms of \\(a\\) and \\(b\\). It turns out that the mean of the \\(\\text{beta} (\\theta | a, b)\\) distribution is \\(\\mu = a / (a + b)\\) and the mode is \\(\\omega = (a − 1) / (a + b − 2)\\) for \\(a &gt; 1\\) and \\(b &gt; 1\\) (\\(\\mu\\) is Greek letter mu and \\(\\omega\\) is Greek letter omega)… The spread of the beta distribution is related to the “concentration” \\(\\kappa = a + b\\) (\\(\\kappa\\) is Greek letter kappa). You can see from Figure 6.1 that as \\(\\kappa = a + b\\) gets larger, the beta distribution gets narrower or more concentrated. (p. 129) As such, if you’d like to specify a beta distribution in terms of \\(\\omega\\) and \\(\\kappa\\), it’d follow the form \\[\\operatorname{beta} (\\alpha = \\omega (\\kappa - 2) + 1, \\beta = (1 - \\omega) \\cdot (\\kappa - 2) + 1),\\] as long as \\(\\kappa &gt; 2\\). Kruschke further clarified: The value we choose for the prior \\(\\kappa\\) can be thought of this way: It is the number of new flips of the coin that we would need to make us teeter between the new data and the prior belief about \\(\\mu\\). If we would only need a few new flips to sway our beliefs, then our prior beliefs should be represented by a small \\(\\kappa\\). If we would need a large number of new flips to sway us away from our prior beliefs about \\(\\mu\\), then our prior beliefs are worth a very large \\(\\kappa\\). (p. 129) He went on to clarify why we might prefer the mode to the mean when discussing the central tendency of a beta distribution. The mode can be more intuitive than the mean, especially for skewed distributions, because the mode is where the distribution reaches its tallest height, which is easy to visualize. The mean in a skewed distribution is somewhere away from the mode, in the direction of the longer tail. (pp. 129–130) Figure 6.2 helped contrast the mean and mode for beta. We’ll use the same process for Figure 6.2 and create the data, first. d &lt;- tibble(shape1 = c(5.6, 17.6, 5, 17), shape2 = c(1.4, 4.4, 2, 5)) %&gt;% mutate(a = str_c(&quot;a =&quot;, shape1), b = str_c(&quot;b =&quot;, shape2), kappa = rep(c(&quot;kappa = 7&quot;, &quot;kappa = 22&quot;), times = 2), mu_omega = rep(c(&quot;mu = 0.8&quot;, &quot;omega = 0.8&quot;), each = 2)) %&gt;% mutate(kappa = factor(kappa, levels = c(&quot;kappa = 7&quot;, &quot;kappa = 22&quot;))) %&gt;% expand(nesting(shape1, shape2, a, b, kappa, mu_omega), x = seq(from = 0, to = 1, length.out = length)) head(d) ## # A tibble: 6 x 7 ## shape1 shape2 a b kappa mu_omega x ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5 2 a =5 b =2 kappa = 7 omega = 0.8 0 ## 2 5 2 a =5 b =2 kappa = 7 omega = 0.8 0.000100 ## 3 5 2 a =5 b =2 kappa = 7 omega = 0.8 0.000200 ## 4 5 2 a =5 b =2 kappa = 7 omega = 0.8 0.000300 ## 5 5 2 a =5 b =2 kappa = 7 omega = 0.8 0.000400 ## 6 5 2 a =5 b =2 kappa = 7 omega = 0.8 0.000500 Here’s Figure 6.2. d %&gt;% ggplot(aes(x = x)) + geom_vline(xintercept = .8, color = &quot;white&quot;) + geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)), color = &quot;grey50&quot;, size = 1.25) + scale_x_continuous(breaks = c(0, .8, 1)) + coord_cartesian(ylim = 0:5) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|a, b)&quot;))) + theme(panel.grid = element_blank()) + facet_grid(mu_omega~kappa) In lines 264 to 290 in his DBDA2E-utilities.R file, Kruschke provided a series of betaABfrom...() functions that will allow us to compute the \\(a\\) and \\(b\\) parameters from measures of central tendency (i.e., mean and mode) and of spread (i.e., \\(\\kappa\\) and \\(\\sigma\\)). Here are those bits of his code. # Shape parameters from central tendency and scale: betaABfromMeanKappa &lt;- function(mean, kappa) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (kappa &lt;= 0) stop(&quot;kappa must be &gt; 0&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } betaABfromModeKappa &lt;- function(mode, kappa) { if (mode &lt;= 0 | mode &gt;= 1) stop(&quot;must have 0 &lt; mode &lt; 1&quot;) if (kappa &lt;= 2) stop(&quot;kappa must be &gt; 2 for mode parameterization&quot;) a &lt;- mode * (kappa - 2) + 1 b &lt;- (1.0 - mode) * (kappa - 2) + 1 return(list(a = a, b = b)) } betaABfromMeanSD &lt;- function(mean, sd) { if (mean &lt;= 0 | mean &gt;= 1) stop(&quot;must have 0 &lt; mean &lt; 1&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) kappa &lt;- mean * (1 - mean)/sd^2 - 1 if (kappa &lt;= 0) stop(&quot;invalid combination of mean and sd&quot;) a &lt;- mean * kappa b &lt;- (1.0 - mean) * kappa return(list(a = a, b = b)) } You can use them like so. betaABfromMeanKappa(mean = .25, kappa = 4) ## $a ## [1] 1 ## ## $b ## [1] 3 betaABfromModeKappa(mode = .25, kappa = 4) ## $a ## [1] 1.5 ## ## $b ## [1] 2.5 betaABfromMeanSD(mean = .5, sd = .1) ## $a ## [1] 12 ## ## $b ## [1] 12 You can also save the results as an object, which can then be indexed by parameter. beta_param &lt;- betaABfromModeKappa(mode = .25, kappa = 4) beta_param$a ## [1] 1.5 beta_param$b ## [1] 2.5 6.3 The posterior beta I’m not going to reproduce all of Formula 6.8. But this a fine opportunity to re-express Bayes’ rule in terms of \\(z\\) and \\(N\\), \\[p(\\theta | z, N) = \\frac{p(z, N | \\theta) p(\\theta)}{p(z, N)}.\\] 6.3.1 Posterior is compromise of prior and likelihood. You might wonder how Kruschke computed the HDI values for Figure 6.3. Remember our hdi_of_icdf() function from back in Chapter 4? Yep, that’s how. Here’s that code, again. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Recall it’s based off of the HDIofICDF() function from Kruschke’s DBDA2E-utilities.R file. I’ve altered Kruschke’s formatting a little bit, but the guts of the code are unchanged. Our hdi_of_icdf() function will take the name of an “inverse cumulative density function” and its parameters and then return an HDI range, as defined by the width parameter. Since the prior at the top of Figure 6.3 is \\(\\text{beta} (5, 5)\\), we can use hdi_of_icdf() to calculate the HDI like so. hdi_of_icdf(name = qbeta, shape1 = 5, shape2 = 5, width = .95) ## [1] 0.2120085 0.7879915 Here they are for the posterior distribution at the bottom of the figure. hdi_of_icdf(name = qbeta, shape1 = 6, shape2 = 14) ## [1] 0.1142339 0.4967144 Note that since we set width = .95 as the default, we can leave it out if we want to stick with the conventional 95% intervals. Here are the mean calculations from the last paragraph on page 134. n &lt;- 10 z &lt;- 1 a &lt;- 5 b &lt;- 5 (proportion_heads &lt;- z / n) ## [1] 0.1 (prior_mean &lt;- a / (a + b)) ## [1] 0.5 (posterior_mean &lt;- (z + a) / (n + a + b)) ## [1] 0.3 In order to make the plots for Figure 6.3, we’ll want to compute the prior, likelihood, and posterior density values across a densely-packed range of \\(\\theta\\) values. trial_data &lt;- c(rep(0, 9), 1) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(`Prior (beta)` = dbeta(theta, shape1 = a, shape2 = b), `Likelihood (Bernoulli)` = bernoulli_likelihood(theta = theta, data = trial_data), `Posterior (beta)` = dbeta(theta, shape1 = 6, shape2 = 14)) glimpse(d) ## Observations: 100 ## Variables: 4 ## $ theta &lt;dbl&gt; 0.00000000, 0.01010101, 0.02020202, 0.0… ## $ `Prior (beta)` &lt;dbl&gt; 0.000000e+00, 6.297429e-06, 9.670878e-0… ## $ `Likelihood (Bernoulli)` &lt;dbl&gt; 0.000000000, 0.009218977, 0.016812166, … ## $ `Posterior (beta)` &lt;dbl&gt; 0.000000e+00, 1.500163e-05, 4.201284e-0… To make things easier on ourselves, we’ll also make two additional data objects to annotate the plots with lines and text. # the data for the in-plot lines d_line &lt;- tibble(theta = c(.212 + .008, .788 - .008, .114 + .004, .497 - .005), value = rep(c(.51, .66), each = 2), xintercept = c(.212, .788, .114, .497), key = rep(c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;), each = 2)) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) # the data for the annotation d_text &lt;- tibble(theta = c(.5, .3), value = c(.8, 1.125), label = &quot;95% HDI&quot;, key = c(&quot;Prior (beta)&quot;, &quot;Posterior (beta)&quot;)) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) Finally, here’s our Figure 6.3. d %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Bernoulli)&quot;, &quot;Posterior (beta)&quot;))) %&gt;% ggplot(aes(x = theta)) + # densities geom_ribbon(aes(ymin = 0, ymax = value), fill = &quot;grey67&quot;) + # dashed vertical lines geom_vline(data = d_line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + # arrows geom_line(data = d_line, aes(y = value), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + # text geom_text(data = d_text, aes(y = value, label = label), color = &quot;grey92&quot;) + labs(x = expression(theta), y = NULL) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) + theme(panel.grid = element_blank()) 6.4 Examples 6.4.1 Prior knowledge expressed as a beta distribution. Here are the results Kruschke reported in the first paragraph of this subsection. beta_param &lt;- betaABfromModeKappa(mode = .5, kappa = 500) beta_param$a ## [1] 250 beta_param$b ## [1] 250 Confusingly, it appears Kruschke switched from dbeta(250, 250) in the prose to dbeta(100, 100) in Figure 6.4.a. So it goes… beta_param &lt;- betaABfromModeKappa(mode = .5, kappa = 200) beta_param$a ## [1] 100 beta_param$b ## [1] 100 ggplot(data = tibble(x = seq(from = 0, to = 1, by = .001)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b)), fill = &quot;grey75&quot;) + geom_ribbon(data = tibble(x = seq(from = .5 - .069, to = .5 + .069, by = .001)), aes(ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b)), fill = &quot;grey67&quot;) + geom_line(data = tibble(x = c(.5 - .069 + .005, .5 + .069 - .005), y = 1.7), aes(y = y), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = .5, y = 3.5, label = &quot;95% HDI&quot;, color = &quot;grey0&quot;) + coord_cartesian(ylim = 0:12) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(paste(&quot;dbeta(&quot;, theta, &quot;|100, 100)&quot;))) + theme(panel.grid = element_blank()) Here are those HDI values. hdi_of_icdf(name = qbeta, shape1 = 100, shape2 = 100, width = .95) ## [1] 0.4309509 0.5690491 Here are our \\(a\\) and \\(b\\) parameters for Kruschke’s free throw example beta_param &lt;- betaABfromModeKappa(mode = .75, kappa = 25) beta_param$a ## [1] 18.25 beta_param$b ## [1] 6.75 Behold Figure 6.4.b. ggplot(data = tibble(x = seq(from = 0, to = 1, by = .001)), aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b)), fill = &quot;grey75&quot;) + geom_ribbon(data = tibble(x = seq(from = .558, to = .892, by = .001)), aes(ymin = 0, ymax = dbeta(x, shape1 = beta_param$a, shape2 = beta_param$b)), fill = &quot;grey67&quot;) + geom_line(data = tibble(x = c(.558 + .005, .892 - .005), y = .75), aes(y = y), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = .73, y = 1.5, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + coord_cartesian(ylim = 0:6) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(paste(&quot;dbeta(&quot;, theta, &quot;|18.25, 6.75)&quot;))) + theme(panel.grid = element_blank()) Here are those HDI values. hdi_of_icdf(name = qbeta, shape1 = beta_param$a, shape2 = beta_param$b, width = .95) ## [1] 0.5581935 0.8915815 But we can be a little more organized and plot the top row of Figure 6.4 all at once. # the data for the in-plot lines d_line &lt;- tibble(theta = c(.431 + .005, .569 - .005, .558 + .005, .892 - .005), value = rep(c(1.9, .8), each = 2), xintercept = c(.431, .569, .558, .892), exemplar = rep(c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;), each = 2)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) # the data for the annotation d_text &lt;- tibble(theta = c(.5, .735), value = c(3.6, 1.45), label = &quot;95% HDI&quot;, exemplar = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) # our main data tibble tibble(theta = seq(from = 0, to = 1, length.out = 100) %&gt;% rep(., times = 3), a = rep(c(100, 18.25, 1), each = 100), b = rep(c(100, 6.75, 1), each = 100)) %&gt;% mutate(exemplar = ifelse(a == 18.25, str_c(&quot;dbeta(theta, &quot;, a, &quot;, &quot;, b, &quot;)&quot;), str_c(&quot;dbeta(theta, &quot;, a %&gt;% round(0), &quot;, &quot;, b %&gt;% round(0), &quot;)&quot;)), density = dbeta(theta, shape1 = a, shape2 = b)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 100, 100)&quot;, &quot;dbeta(theta, 18.25, 6.75)&quot;, &quot;dbeta(theta, 1, 1)&quot;))) %&gt;% # finally, the plot code! ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = density), size = 0, fill = &quot;grey67&quot;) + geom_vline(data = d_line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + geom_line(data = d_line, aes(y = value), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_text(data = d_text, aes(y = value, label = label), color = &quot;grey0&quot;) + labs(title = &quot;Prior (beta)&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~exemplar, scales = &quot;free_y&quot;, ncol = 3) If you look closely, you’ll notice the middle row is the same for each column. So we’ll just plot it once. n &lt;- 20 z &lt;- 17 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = likelihood), size = 0, fill = &quot;grey67&quot;) + labs(title = &quot;Likelihood (Bernoulli)&quot;, subtitle = &quot;This is the same for all 3 exemplars, so\\nthere&#39;s no need to plot this thrice.&quot;, x = expression(theta), y = expression(paste(&quot;p(D|&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) The bottom row: # the data for the in-plot lines d_line &lt;- tibble(theta = c(.466 + .005, .597 - .005, .663 + .004, .897 - .005, .660 + .005, .959 - .003), value = rep(c(1.9, 1.1, .85), each = 2), xintercept = c(.466, .597, .663, .897, .660, .959), exemplar = rep(c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;), each = 2)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) # the data for the annotation d_text &lt;- tibble(theta = c(.532, .78, .82), value = c(3.5, 2, 1.6), label = &quot;95% HDI&quot;, exemplar = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) # our main data tibble tibble(theta = seq(from = 0, to = 1, length.out = 100) %&gt;% rep(., times = 3), a = rep(c(117, 35.25, 18), each = 100), b = rep(c(103, 9.75, 4), each = 100)) %&gt;% mutate(exemplar = ifelse(a == 35.25, str_c(&quot;dbeta(theta, &quot;, a, &quot;, &quot;, b, &quot;)&quot;), str_c(&quot;dbeta(theta, &quot;, a %&gt;% round(0), &quot;, &quot;, b %&gt;% round(0), &quot;)&quot;)), density = dbeta(theta, shape1 = a, shape2 = b)) %&gt;% mutate(exemplar = factor(exemplar, levels = c(&quot;dbeta(theta, 117, 103)&quot;, &quot;dbeta(theta, 35.25, 9.75)&quot;, &quot;dbeta(theta, 18, 4)&quot;))) %&gt;% # the plot ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = density), size = 0, fill = &quot;grey67&quot;) + geom_vline(data = d_line, aes(xintercept = xintercept), linetype = 2, color = &quot;grey92&quot;) + geom_line(data = d_line, aes(y = value), arrow = arrow(length = unit(.15,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_text(data = d_text, aes(y = value, label = label), color = &quot;grey0&quot;) + labs(title = &quot;Posterior (beta)&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~exemplar, scales = &quot;free_y&quot;, ncol = 3) And if you wanted those HDI values in bulk, you might wrap the hdi_of_icdf() into another function for use in purrr::map2(). # new function get_hdi &lt;- function(a, b) { hdi_of_icdf(name = qbeta, shape1 = a, shape2 = b, width = .95) %&gt;% as_tibble() %&gt;% mutate(key = c(&quot;lower level&quot;, &quot;upper level&quot;)) %&gt;% spread(key = key, value = value) } # put it to work tibble(`figure position` = c(&quot;left&quot;, &quot;middle&quot;, &quot;right&quot;), a = c(117, 35.25, 18), b = c(103, 9.75, 4)) %&gt;% mutate(hdi = purrr::map2(a, b, get_hdi)) %&gt;% unnest() ## # A tibble: 3 x 5 ## `figure position` a b `lower level` `upper level` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 left 117 103 0.466 0.597 ## 2 middle 35.2 9.75 0.663 0.897 ## 3 right 18 4 0.660 0.959 6.4.2 Prior knowledge that cannot be expressed as a beta distribution. The beauty of using a beta distribution to express prior knowledge is that the posterior distribution is again exactly a beta distribution, and therefore, no matter how much data we include, we always have an exact representation of the posterior distribution and a simple way of computing it. But not all prior knowledge can be expressed by a beta distribution, because the beta distribution can only be in the forms illustrated by Figure 6.1. If the prior knowledge cannot be expressed as a beta distribution, then we must use a different method to derive the posterior. In particular, we might revert to grid approximation as was explained in Section 5.5 (p. 116). For such a small section in the text, the underlying code is a bit of a beast. Fir kicks, we’ll practice two ways. First we’ll follow the code Kruschke used in the text. Our second attempt will be in a more tidyverse sort of way. 6.4.2.1 Figure 6.5 in Kruschke style. # Fine teeth for Theta theta &lt;- seq(0, 1, length = 1000) # Two triangular peaks on a small non-zero floor p_theta &lt;- c(rep(1, 200), seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2) # Make p_theta sum to 1.0 p_theta &lt;- p_theta / sum(p_theta) Here’s Kruschke’s BernGrid() code in all its glory. BernGrid = function( Theta , pTheta , Data , plotType=c(&quot;Points&quot;,&quot;Bars&quot;)[2] , showCentTend=c(&quot;Mean&quot;,&quot;Mode&quot;,&quot;None&quot;)[3] , showHDI=c(TRUE,FALSE)[2] , HDImass=0.95 , showpD=c(TRUE,FALSE)[2] , nToPlot=length(Theta) ) { # Theta is vector of values between 0 and 1. # pTheta is prior probability mass at each value of Theta # Data is vector of 0&#39;s and 1&#39;s. # Check for input errors: if ( any( Theta &gt; 1 | Theta &lt; 0 ) ) { stop(&quot;Theta values must be between 0 and 1&quot;) } if ( any( pTheta &lt; 0 ) ) { stop(&quot;pTheta values must be non-negative&quot;) } if ( !isTRUE(all.equal( sum(pTheta) , 1.0 )) ) { stop(&quot;pTheta values must sum to 1.0&quot;) } if ( !all( Data == 1 | Data == 0 ) ) { stop(&quot;Data values must be 0 or 1&quot;) } # Create summary values of Data z = sum( Data ) # number of 1&#39;s in Data N = length( Data ) # Compute the Bernoulli likelihood at each value of Theta: pDataGivenTheta = Theta^z * (1-Theta)^(N-z) # Compute the evidence and the posterior via Bayes&#39; rule: pData = sum( pDataGivenTheta * pTheta ) pThetaGivenData = pDataGivenTheta * pTheta / pData # Plot the results. layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins cexAxis = 1.33 cexLab = 1.75 # convert plotType to notation used by plot: if ( plotType==&quot;Points&quot; ) { plotType=&quot;p&quot; } if ( plotType==&quot;Bars&quot; ) { plotType=&quot;h&quot; } dotsize = 5 # how big to make the plotted dots barsize = 5 # how wide to make the bar lines # If the comb has a zillion teeth, it&#39;s too many to plot, so plot only a # thinned out subset of the teeth. nteeth = length(Theta) if ( nteeth &gt; nToPlot ) { thinIdx = round( seq( 1, nteeth , length=nteeth ) ) } else { thinIdx = 1:nteeth } # Plot the prior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote(p(theta)) , cex.lab=cexLab , main=&quot;Prior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pTheta , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # Plot the likelihood: p(Data|Theta) plot( Theta[thinIdx] , pDataGivenTheta[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(D|&quot; * theta * &quot;)&quot; ) , cex.lab=cexLab , main=&quot;Likelihood&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( z &gt; .5*N ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx ,1.0*max(pDataGivenTheta) ,cex=2.0 ,bquote( &quot;Data: z=&quot; * .(z) * &quot;,N=&quot; * .(N) ) ,adj=textadj ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pDataGivenTheta ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pDataGivenTheta ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.7*max(pDataGivenTheta) , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot the posterior. yLim = c(0,1.1*max(c(pTheta,pThetaGivenData))) plot( Theta[thinIdx] , pThetaGivenData[thinIdx] , type=plotType , pch=&quot;.&quot; , cex=dotsize , lwd=barsize , xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis , xlab=bquote(theta) , ylab=bquote( &quot;p(&quot; * theta * &quot;|D)&quot; ) , cex.lab=cexLab , main=&quot;Posterior&quot; , cex.main=1.5 , col=&quot;skyblue&quot; ) if ( showCentTend != &quot;None&quot; ) { if ( showCentTend == &quot;Mean&quot; ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mean=&quot; * .(signif(meanTheta,3)) ) , cex=2.0 , adj=textadj ) } if ( showCentTend == &quot;Mode&quot; ) { modeTheta = Theta[ which.max( pThetaGivenData ) ] if ( modeTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , yLim[2] , bquote( &quot;mode=&quot; * .(signif(modeTheta,3)) ) , cex=2.0 , adj=textadj ) } } # Plot marginal likelihood pData: if ( showpD ) { meanTheta = sum( Theta * pThetaGivenData ) if ( meanTheta &gt; .5 ) { textx = 0 ; textadj = c(0,1) } else { textx = 1 ; textadj = c(1,1) } text( textx , 0.75*max(pThetaGivenData) , cex=2.0 , bquote( &quot;p(D)=&quot; * .(signif(pData,3)) ) ,adj=textadj ) } # Mark the highest density interval. HDI points are not thinned in the plot. if ( showHDI ) { HDIinfo = HDIofGrid( pThetaGivenData , credMass=HDImass ) points( Theta[ HDIinfo$indices ] , rep( HDIinfo$height , length( HDIinfo$indices ) ) , pch=&quot;-&quot; , cex=1.0 ) text( mean( Theta[ HDIinfo$indices ] ) , HDIinfo$height , bquote( .(100*signif(HDIinfo$mass,3)) * &quot;% HDI&quot; ) , adj=c(0.5,-1.5) , cex=1.5 ) # Mark the left and right ends of the waterline. # Find indices at ends of sub-intervals: inLim = HDIinfo$indices[1] # first point for ( idx in 2:(length(HDIinfo$indices)-1) ) { if ( ( HDIinfo$indices[idx] != HDIinfo$indices[idx-1]+1 ) | # jumps on left, OR ( HDIinfo$indices[idx] != HDIinfo$indices[idx+1]-1 ) ) { # jumps on right inLim = c(inLim,HDIinfo$indices[idx]) # include idx } } inLim = c(inLim,HDIinfo$indices[length(HDIinfo$indices)]) # last point # Mark vertical lines at ends of sub-intervals: for ( idx in inLim ) { lines( c(Theta[idx],Theta[idx]) , c(-0.5,HDIinfo$height) , type=&quot;l&quot; , lty=2 , lwd=1.5 ) text( Theta[idx] , HDIinfo$height , bquote(.(round(Theta[idx],3))) , adj=c(0.5,-0.1) , cex=1.2 ) } } # return( pThetaGivenData ) } # end of function You plot using Kruschke’s method, like so. Data &lt;- c(rep(0, 13), rep(1, 14)) BernGrid(theta, p_theta, Data, plotType = &quot;Bars&quot;, showCentTend = &quot;None&quot;, showHDI = FALSE, showpD = FALSE) The method works fine. But, I’m not a fan. It’s clear Kruschke put a lot of thought into the BernGrid() function. However, its inner workings are too opaque, for me, which leads to our next section… 6.4.2.2 Figure 6.5 in tidyverse style. Here we’ll be plotting with ggplot2. But let’s first get the data into a tibble. # we need these to compute the likelihood n &lt;- 27 z &lt;- 14 trial_data &lt;- c(rep(0, times = n - z), rep(1, times = z)) # (i.e., Data) d &lt;- tibble(theta = seq(from = 0, to = 1, length.out = 1000), # (i.e., Theta) Prior = c(rep(1, 200), # (i.e., pTheta) seq(1, 100, length = 50), seq(100, 1, length = 50), rep(1, 200)) %&gt;% rep(., times = 2)) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, # (i.e., pDataGivenTheta) data = trial_data)) %&gt;% mutate(evidence = sum(Likelihood * Prior)) %&gt;% # (i.e., pData) mutate(Posterior = Likelihood * Prior / evidence) # (i.e., pThetaGivenData) glimpse(d) ## Observations: 1,000 ## Variables: 5 ## $ theta &lt;dbl&gt; 0.000000000, 0.001001001, 0.002002002, 0.003003003, 0… ## $ Prior &lt;dbl&gt; 9.174312e-05, 9.174312e-05, 9.174312e-05, 9.174312e-0… ## $ Likelihood &lt;dbl&gt; 0.000000e+00, 1.000988e-42, 1.618784e-38, 4.664454e-3… ## $ evidence &lt;dbl&gt; 3.546798e-10, 3.546798e-10, 3.546798e-10, 3.546798e-1… ## $ Posterior &lt;dbl&gt; 0.000000e+00, 2.589202e-37, 4.187221e-33, 1.206529e-3… With our nice tibble in hand, we’ll plot the prior, likelihood, and posterior one at a time. # prior (p1 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Prior), fill = &quot;grey50&quot;) + labs(title = &quot;Prior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) ) # likelihood (p2 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Likelihood), fill = &quot;grey50&quot;) + labs(title = &quot;Likelihood&quot;, x = expression(theta), y = expression(paste(&quot;p(D|&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) ) # posterior (p3 &lt;- d %&gt;% ggplot(aes(x = theta)) + geom_ribbon(aes(ymin = 0, ymax = Posterior), fill = &quot;grey50&quot;) + labs(title = &quot;Posterior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) ) Note how we saved each the plots as objects. If we want to stack the plots one atop another like they’re presented in Figure 6.5, we can use the grid.arrange() function from the gridExtra package. library(gridExtra) grid.arrange(p1, p2, p3) 6.5 Summary The main point of this chapter was to demonstrate how Bayesian inference works when Bayes’ rule can be solved analytically, using mathematics alone, without numerical approximation… Unfortunately, there are two severe limitations with this approach… Thus, although it is interesting and educational to see how Bayes’ rule can be solved analytically, we will have to abandon exact mathematical solutions when doing complex applications. We will instead use Markov chain Monte Carlo (MCMC) methods. (p. 139) And if you’re looking at this project, I imagine that’s exactly what you’re looking for. We want to use the power of a particular kind of MCMC, Hamiltonian Monte Carlo, through the interface of the brms package. Get excited. It’s coming. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 ## [5] purrr_0.3.2 readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ## [9] ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.10 reshape2_1.4.3 haven_2.1.0 ## [5] lattice_0.20-38 colorspace_1.4-1 vctrs_0.2.0 generics_0.0.2 ## [9] htmltools_0.4.0 yaml_2.2.0 utf8_1.1.4 rlang_0.4.0 ## [13] pillar_1.4.2 glue_1.3.1 withr_2.1.2 modelr_0.1.4 ## [17] readxl_1.3.1 lifecycle_0.1.0 plyr_1.8.4 munsell_0.5.0 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.4 evaluate_0.14 ## [25] labeling_0.3 knitr_1.23 fansi_0.4.0 broom_0.5.2 ## [29] Rcpp_1.0.2 scales_1.0.0 backports_1.1.5 jsonlite_1.6 ## [33] hms_0.4.2 digest_0.6.21 stringi_1.4.3 grid_3.6.0 ## [37] cli_1.1.0 tools_3.6.0 magrittr_1.5 lazyeval_0.2.2 ## [41] crayon_1.3.4 pkgconfig_2.0.3 zeallot_0.1.0 ellipsis_0.3.0 ## [45] xml2_1.2.0 lubridate_1.7.4 assertthat_0.2.1 rmarkdown_1.13 ## [49] httr_1.4.0 rstudioapi_0.10 R6_2.4.0 nlme_3.1-139 ## [53] compiler_3.6.0 "],
["markov-chain-monte-carlo.html", "7 Markov Chain Monte Carlo 7.1 Approximating a distribution with a large sample 7.2 A simple case of the Metropolis algorithm 7.3 The Metropolis algorithm more generally 7.4 Toward Gibbs sampling: Estimating two coin biases 7.5 MCMC representativeness, accuracy, and efficiency Reference Session info", " 7 Markov Chain Monte Carlo This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications. The class of methods is called Markov chain Monte Carlo (MCMC), for reasons that will be explained later in the chapter. It is MCMC algorithms and software, along with fast computer hardware, that allow us to do Bayesian data analysis for realistic applications that would have been effectively impossible 30 years ago. (p. 144) Statistician David Draper covered some of the history of MCMC in his lecture, Bayesian Statistical Reasoning. 7.1 Approximating a distribution with a large sample The concept of representing a distribution by a large representative sample is foundational for the approach we take to Bayesian analysis of complex models. The idea is applied intuitively and routinely in everyday life and in science. For example, polls and surveys are founded on this concept: By randomly sampling a subset of people from a population, we estimate the underlying tendencies in the entire population. The larger the sample, the better the estimation. What is new in the present application is that the population from which we are sampling is a mathematically defined distribution, such as a posterior probability distribution. (p. 145) Like in Chapters 4 and 6, we need to define our hdi_of_icdf() function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Our hdi_of_icdf() function will compute the analytic 95% HDIs for the distribution under consideration in Figure 7.1, \\(\\text{beta} (\\theta | 15, 7)\\). (h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 7) ) ## [1] 0.4907001 0.8639305 Using an equation from Chapter 6, \\(\\omega = (a − 1) / (a + b − 2)\\), we can compute the corresponding mode. (omega &lt;- (15 - 1) / (15 + 7 - 2)) ## [1] 0.7 To get the density in the upper left panel of Figure 7.1, we’ll make use of dbeta(). And we’ll also make use of our h[1:2] and omega values. library(tidyverse) tibble(theta = seq(from = 0, to = 1, length.out = 100)) %&gt;% ggplot() + geom_ribbon(aes(x = theta, ymin = 0, ymax = dbeta(theta, shape1 = 15, shape2 = 7)), fill = &quot;grey67&quot;) + geom_segment(aes(x = h[1], xend = h[2], y = 0, yend = 0), size = .75) + geom_point(aes(x = omega, y = 0), size = 1.5, shape = 19) + annotate(x = .675, y = .4, label = &quot;95% HDI&quot;, geom = &quot;text&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = c(0, h, omega, 1), labels = c(&quot;0&quot;, h %&gt;% round(2), omega, &quot;1&quot;)) + coord_cartesian(ylim = c(-.125, 4)) + labs(title = &quot;Exact distribution&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) The remaining panels in Figure 7.1 require we simulate the data. set.seed(7) d &lt;- tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) head(d) ## # A tibble: 6 x 3 ## n theta key ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 500 0.806 Sample N = 500 ## 2 500 0.756 Sample N = 500 ## 3 500 0.727 Sample N = 500 ## 4 500 0.784 Sample N = 500 ## 5 500 0.782 Sample N = 500 ## 6 500 0.590 Sample N = 500 With the data in hand, we’re ready to plot the remaining panels for Figure 7.1. This time, we’ll use the handy stat_pointintervalh() function from the tidybayes package to mark off the mode and 95% HDIs. library(tidybayes) d %&gt;% ggplot(aes(x = theta)) + geom_histogram(size = .2, color = &quot;grey92&quot;, fill = &quot;grey67&quot;, binwidth = .02) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + coord_cartesian(xlim = 0:1) + theme(panel.grid = element_blank()) + facet_wrap(~key, ncol = 3, scales = &quot;free&quot;) If we want the exact values for the mode and 95% HDIs, we can use the tidybayes::mode_hdi() function. d %&gt;% group_by(key) %&gt;% mode_hdi(theta) ## Warning: unnest() has a new interface. See ?unnest for details. ## Try `df %&gt;% unnest(c(.lower, .upper))`, with `mutate()` if needed ## # A tibble: 3 x 7 ## key theta .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Sample N = 500 0.695 0.511 0.868 0.95 mode hdi ## 2 Sample N = 5000 0.688 0.497 0.870 0.95 mode hdi ## 3 Sample N = 50000 0.711 0.490 0.863 0.95 mode hdi If you wanted a better sense of the phenomena, you could do a simulation. We’ll make a custom simulation function to compute the modes from many random draws from our \\(\\text{beta} (\\theta | 15, 7)\\) distribution, with varying \\(N\\) values. my_mode_simulation &lt;- function(seed){ set.seed(seed) tibble(n = c(500, 5000, 50000)) %&gt;% mutate(theta = map(n, ~rbeta(., shape1 = 15, shape2 = 7))) %&gt;% unnest(theta) %&gt;% mutate(key = str_c(&quot;Sample N = &quot;, n)) %&gt;% group_by(key) %&gt;% mode_hdi(theta) } Here we put our my_mode_simulation() function to work. # we need an index of the values we set our seed with in our `my_mode_simulation()` function sim &lt;- tibble(seed = 1:1e3) %&gt;% group_by(seed) %&gt;% # inserting our subsamples mutate(modes = map(seed, my_mode_simulation)) %&gt;% # unnesting allows us to access our model results unnest(modes) sim %&gt;% ggplot(aes(x = theta, y = key)) + geom_vline(xintercept = .7, color = &quot;white&quot;) + geom_halfeyeh(.width = c(.95, .5)) + labs(title = expression(paste(&quot;Variability of the mode for simulations of &quot;, beta, &quot;(&quot;, theta, &quot;|15, 7), the true mode of which is .7&quot;)), subtitle = &quot;For each sample size, the dot is the median, the inner thick line is the percentile-based 50% interval,\\nand the outer thin line the percentile-based 95% interval. Although the central tendency\\napproximates the true value for all three conditions, the variability of the mode estimate is inversely\\nrelated to the sample size.&quot;, x = &quot;mode&quot;, y = NULL) + coord_cartesian(xlim = c(.6, .8), ylim = c(1.25, 3.5)) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) 7.2 A simple case of the Metropolis algorithm Our goal in Bayesian inference is to get an accurate representation of the posterior distribution. One way to do that is to sample a large number of representative points from the posterior. The question then becomes this: How can we sample a large number of representative values from a distribution? (p. 146). The answer, my friends, is MCMC. 7.2.1 A politician stumbles upon the Metropolis algorithm. If we denote \\(P_\\text{proposed}\\) as the population of the proposed island and \\(P_\\text{current}\\) as the population of the current island, then \\[p_\\text{move} = \\frac{P_\\text{proposed}}{P_\\text{current}}.\\] “What’s amazing about this heuristic is that it works: In the long run, the probability that the politician is on any one of the islands exactly matches the relative population of the island” (p. 147)! 7.2.2 A random walk. The code below will allow us to reproduce Kruschke’s random walk. To give credit where it’s due, this is a mild amendment to the code from Chapter 8 of McElreath’s Statistical Rethinking text. set.seed(7) num_days &lt;- 5e4 positions &lt;- rep(0, num_days) current &lt;- 4 for (i in 1:num_days) { # record current position positions[i] &lt;- current # flip coin to generate proposal proposal &lt;- current + sample(c(-1, 1), size = 1) # now make sure he loops around from 7 back to 1 if (proposal &lt; 1) proposal &lt;- 7 if (proposal &gt; 7) proposal &lt;- 1 # move? prob_accept_the_proposal &lt;- proposal/current current &lt;- ifelse(runif(1) &lt; prob_accept_the_proposal, proposal, current) } If you missed it, positions is the main product of our simulation. Here we’ll put positions in a tibble and reproduce the top portion of Figure 7.2. tibble(theta = positions) %&gt;% ggplot(aes(x = theta)) + geom_bar() + scale_x_continuous(expression(theta), breaks = 1:7) + theme(panel.grid = element_blank()) Here’s the middle portion of Figure 7.2. tibble(t = 1:5e4, theta = positions) %&gt;% slice(1:500) %&gt;% ggplot(aes(x = theta, y = t)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + scale_x_continuous(expression(theta), breaks = 1:7) + scale_y_log10(&quot;Time Step&quot;, breaks = c(1, 2, 5, 20, 100, 500)) + theme(panel.grid = element_blank()) And now we make the bottom. tibble(x = 1:7, y = 1:7) %&gt;% ggplot(aes(x = x, y = y)) + geom_col(width = .2) + scale_x_continuous(expression(theta), breaks = 1:7) + ylab(expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) 7.2.3 General properties of a random walk. Unfortunately, the computations behind Figure 7.3 are beyond my math + programming capabilities. If you’ve got the code, hit me up. 7.2.4 Why we care. Through the simple magic of the random walk procedure, we are able to do indirectly something we could not necessarily do directly: We can generate random samples from the target distribution. Moreover, we can generate those random samples from the target distribution even when the target distribution is not normalized. This technique is profoundly useful when the target distribution \\(P(\\theta)\\) is a posterior proportional to \\(p(D | \\theta) p(\\theta)\\). Merely by evaluating \\(p(D | \\theta) p(\\theta)\\), without normalizing it by \\(p(D)\\), we can generate random representative values from the posterior distribution. This result is wonderful because the method obviates direct computation of the evidence \\(p(D)\\), which, as you’ll recall, is one of the most difficult aspects of Bayesian inference. By using MCMC techniques, we can do Bayesian inference in rich and complex models. It has only been with the development of MCMC algorithms and software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience. (p. 152, emphasis in the original) 7.3 The Metropolis algorithm more generally “The procedure described in the previous section was just a special case of a more general procedure known as the Metropolis algorithm, named after the first author of a famous article (Metropolis, Rosenbluth, Rosenbluth, Teller, &amp; Teller, 1953)” (p. 156). Here’s how to generate a proposed jump from a zero-mean normal distribuiton with a standard deviation of 0.2. rnorm(1, mean = 0, sd = 0.2) ## [1] -0.1985524 To get a sense of what draws from rnorm() looks like in the long run, we might plot. mu &lt;- 0 sigma &lt;- 0.2 # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(proposed_jump = rnorm(n, mean = mu, sd = sigma)) %&gt;% ggplot(aes(x = proposed_jump, y = 0)) + geom_jitter(width = 0, height = .1, size = 1/2, alpha = 1/2) + # this is the idealized distribution stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = -0.6, to = 0.6, length.out = 7)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Jump proposals&quot;, subtitle = &quot;The gray line shows the data generating distribution.&quot;) + theme(panel.grid = element_blank()) Anyway, having generated a proposed new position, the algorithm then decides whether or not to accept the proposal. The decision rule is exactly what was already specified in Equation 7.1. In detail, this is accomplished by computing the ratio \\(p_\\text{move} = P(\\theta_\\text{proposed}) / P(\\theta_\\text{current})\\). Then a random number from the uniform interval \\([0, 1]\\) is generated; in R, this can be accomplished with the command runif(1). If the random number is between 0 and pmove, then the move is accepted. (p. 157) We’ll see what that might look like in the next section. In the meantime, here’s how to use runif(). runif(1) ## [1] 0.2783186 Just for kicks, here’s what that looks like in bulk. # how many proposals would you like? n &lt;- 500 set.seed(7) tibble(draw = runif(n)) %&gt;% ggplot(aes(x = draw, y = 0)) + geom_jitter(width = 0, height = 1/4, size = 1/2, alpha = 1/2) + stat_function(fun = dunif, color = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL, limits = c(-1/3, 5/3)) + labs(title = &quot;Uniform draws&quot;, subtitle = &quot;The gray line shows the data generating distribution.&quot;) + theme(panel.grid = element_blank()) We do not see a concentration towards the mean, this time. The draws are uniformly distributed across the parameter space. 7.3.1 Metropolis algorithm applied to Bernoulli likelihood and beta prior. You can find Kruschke’s code in the BernMetrop.R file. I’m going to break it up a little. # specify the data, to be used in the likelihood function. my_data &lt;- c(rep(0, 6), rep(1, 14)) # define the Bernoulli likelihood function, p(D|theta). # the argument theta could be a vector, not just a scalar likelihood &lt;- function(theta, data) { z &lt;- sum(data) n &lt;- length(data) p_data_given_theta &lt;- theta^z * (1 - theta)^(n - z) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the likelihood for theta &gt; 1 or for theta &lt; 0 is zero p_data_given_theta[theta &gt; 1 | theta &lt; 0] &lt;- 0 return(p_data_given_theta) } # define the prior density function. prior_d &lt;- function(theta) { p_theta &lt;- dbeta(theta, 1, 1) # the theta values passed into this function are generated at random, # and therefore might be inadvertently greater than 1 or less than 0. # the prior for theta &gt; 1 or for theta &lt; 0 is zero p_theta[theta &gt; 1 | theta &lt; 0] = 0 return(p_theta) } # define the relative probability of the target distribution, # as a function of vector theta. for our application, this # target distribution is the unnormalized posterior distribution target_rel_prob &lt;- function(theta, data) { target_rel_prob &lt;- likelihood(theta, data) * prior_d(theta) return(target_rel_prob) } # specify the length of the trajectory, i.e., the number of jumps to try: traj_length &lt;- 50000 # this is just an arbitrary large number # initialize the vector that will store the results trajectory &lt;- rep(0, traj_length) # specify where to start the trajectory: trajectory[1] &lt;- 0.01 # another arbitrary value # specify the burn-in period burn_in &lt;- ceiling(0.0 * traj_length) # arbitrary number, less than `traj_length` # initialize accepted, rejected counters, just to monitor performance: n_accepted &lt;- 0 n_rejected &lt;- 0 That first part follows what Kruschke put in his script. I’m going to bundel the next large potion in a fucntion, my_metropolis() which will make it easier to plug the code into the purrr::map() function. my_metropolis &lt;- function(proposal_sd) { # now generate the random walk. the &#39;t&#39; index is time or trial in the walk. # specify seed to reproduce same random walk set.seed(47405) ## I&#39;m taking this section out and will replace it # # specify standard deviation of proposal distribution # proposal_sd &lt;- c(0.02, 0.2, 2.0)[2] ## end of the section I took out for (t in 1:(traj_length - 1)) { current_position &lt;- trajectory[t] # use the proposal distribution to generate a proposed jump proposed_jump &lt;- rnorm(1, mean = 0, sd = proposal_sd) # compute the probability of accepting the proposed jump prob_accept &lt;- min(1, target_rel_prob(current_position + proposed_jump, my_data) / target_rel_prob(current_position, my_data)) # generate a random uniform value from the interval [0, 1] to # decide whether or not to accept the proposed jump if (runif(1) &lt; prob_accept) { # accept the proposed jump trajectory[t + 1] &lt;- current_position + proposed_jump # increment the accepted counter, just to monitor performance if (t &gt; burn_in) {n_accepted &lt;- n_accepted + 1} } else { # reject the proposed jump, stay at current position trajectory[t + 1] &lt;- current_position # increment the rejected counter, just to monitor performance if (t &gt; burn_in) {n_rejected &lt;- n_rejected + 1} } } # extract the post-burn_in portion of the trajectory accepted_traj &lt;- trajectory[(burn_in + 1) : length(trajectory)] tibble(accepted_traj = accepted_traj, n_accepted = n_accepted, n_rejected = n_rejected) # end of Metropolis algorithm } Now we have my_metropolis(), we can run the analysis based on the three proposal_sd values, nesting the results in a tibble. d &lt;- tibble(proposal_sd = c(0.02, 0.2, 2.0)) %&gt;% mutate(accepted_traj = map(proposal_sd, my_metropolis)) %&gt;% unnest(accepted_traj) glimpse(d) ## Observations: 150,000 ## Variables: 4 ## $ proposal_sd &lt;dbl&gt; 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.0… ## $ accepted_traj &lt;dbl&gt; 0.01000000, 0.01000000, 0.01000000, 0.01000000, 0.01149173, 0.02550380, 0.0… ## $ n_accepted &lt;dbl&gt; 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801, 46801… ## $ n_rejected &lt;dbl&gt; 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 3198, 319… Now we have d in hand, here’s the top portion of Figure 7.4. d &lt;- d %&gt;% mutate(proposal_sd = str_c(&quot;Proposal SD = &quot;, proposal_sd), iter = rep(1:50000, times = 3)) d %&gt;% ggplot(aes(x = accepted_traj)) + geom_histogram(boundary = 0, binwidth = .02, size = .2, color = &quot;grey92&quot;, fill = &quot;grey67&quot;) + geom_halfeyeh(aes(y = 0), point.interval = mode_hdi, .width = .95) + scale_x_continuous(breaks = seq(from = 0, to = 1, length.out = 6)) + scale_y_continuous(NULL, breaks = NULL) + labs(x = expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~ proposal_sd, ncol = 3) The modes are the points and the lines depict the 95% HDIs. Here’s the middle of Figure 7.4. d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + coord_cartesian(xlim = 0:1, ylim = 49900:50000) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, length.out = 6)) + labs(title = &quot;End of Chain&quot;, y = &quot;Step in Chain&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~proposal_sd, ncol = 3) The bottom: d %&gt;% ggplot(aes(x = accepted_traj, y = iter)) + geom_path(size = 1/4, color = &quot;grey50&quot;) + geom_point(size = 1/2, alpha = 1/2) + coord_cartesian(xlim = 0:1, ylim = 1:100) + scale_x_continuous(expression(theta), breaks = seq(from = 0, to = 1, length.out = 6)) + labs(title = &quot;End of Chain&quot;, y = &quot;Step in Chain&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~proposal_sd, ncol = 3) 7.3.2 Summary of Metropolis algorithm. The motivation for methods like the Metropolis algorithm is that they provide a high-resolution picture of the posterior distribution, even though in complex models we cannot explicitly solve the mathematical integral in Bayes’ rule. The idea is that we get a handle on the posterior distribution by generating a large sample of representative values. The larger the sample, the more accurate is our approximation. As emphasized previously, this is a sample of representative credible parameter values from the posterior distribution; it is not a resampling of data (there is a fixed data set). The cleverness of the method is that representative parameter values can be randomly sampled from complicated posterior distributions without solving the integral in Bayes’ rule, and by using only simple proposal distributions for which efficient random number generators already exist. (p. 161) 7.4 Toward Gibbs sampling: Estimating two coin biases “The Metropolis method is very useful, but it can be inefficient. Other methods can be more efficient in some situations” (p. 162). 7.4.1 Prior, likelihood and posterior for two biases. We are considering situations in which there are two underlying biases, namely \\(\\theta_1\\) and \\(\\theta_2\\), for the two coins. We are trying to determine what we should believe about these biases after we have observed some data from the two coins. Recall that [Kruschke used] the term “bias” as the name of the parameter \\(\\theta\\), and not to indicate that the value of \\(\\theta\\) deviates from 0.5… What we have to do next is specify a particular mathematical form for the prior distribution. We will work through the mathematics of a particular case for two reasons: First, it will allow us to explore graphical displays of two-dimensional parameter spaces, which will inform our intuitions about Bayes’ rule and sampling from the posterior distribution. Second, the mathematics will set the stage for a specific example of Gibbs sampling. Later in the book when we do applied Bayesian analysis, we will not be doing any of this sort of mathematics. We are doing the math now, for simple cases, to understand how the methods work so we can properly interpret their outputs in realistically complex cases. (pp. 163–165, emphasis in the original) 7.4.2 The posterior via exact formal analysis. The plots in the left column of Figure 7.5 are outside of my skill set. I believe they are referred to as wireframe plots and it’s my understanding that ggplot2 does not support wireframe plots at this time. However, I can reproduce versions of the right hand column. For our initial attempt for the upper right corner, we’ll simulate. set.seed(7) betas &lt;- tibble(theta_1 = rbeta(1e5, shape1 = 2, shape2 = 2), theta_2 = rbeta(1e5, shape1 = 2, shape2 = 2)) betas %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + stat_density_2d() + coord_equal() + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) Instead of the contour lines, one might use color to depict the density variable, instead. betas %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + stat_density_2d(aes(fill = stat(density)), geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;A&quot;) + coord_equal() + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) With careful use of dbeta(), we can be more precise. theta_sequence &lt;- seq(from = 0, to = 1, by = .01) tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = prior_1 * prior_2)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + coord_equal() + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) We’ll need the bernoulli_likelihood() function from back in Chapter 6 for the middle right of Figure 7.5. bernoulli_likelihood &lt;- function(theta, data) { # theta = success probability parameter ranging from 0 to 1 # data = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } With our trusty bernoulli_likelihood() function in hand, we can now make a version of the middle right panel of Figure 7.5. theta_1_data &lt;- rep(0:1, times = c(8 - 6, 6)) theta_2_data &lt;- rep(0:1, times = c(7 - 2, 2)) tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) %&gt;% ggplot(aes(x = theta_1, y = theta_2, fill = likelihood_1 * likelihood_2)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + coord_equal() + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) Here’s the two-dimensional posterior, the lower right panel of Figure 7.5. # we&#39;ve already defined these, but here they are again theta_sequence &lt;- seq(from = 0, to = 1, by = .01) theta_1_data &lt;- rep(0:1, times = c(8 - 6, 6)) theta_2_data &lt;- rep(0:1, times = c(7 - 2, 2)) # this is a redo from two plots up, but saved as `d_prior` d_prior &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(prior_1 = dbeta(x = theta_1, shape1 = 2, shape2 = 2), prior_2 = dbeta(x = theta_2, shape1 = 2, shape2 = 2)) %&gt;% expand(nesting(theta_1, prior_1), nesting(theta_2, prior_2)) # this is a redo from one plot up, but saved as `d_likelihood` d_likelihood &lt;- tibble(theta_1 = theta_sequence, theta_2 = theta_sequence) %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_sequence, data = theta_1_data), likelihood_2 = bernoulli_likelihood(theta = theta_sequence, data = theta_2_data)) %&gt;% expand(nesting(theta_1, likelihood_1), nesting(theta_2, likelihood_2)) # here we cobine `d_prior` and `d_likelihood` d_prior %&gt;% left_join(d_likelihood, by = c(&quot;theta_1&quot;, &quot;theta_2&quot;)) %&gt;% # we need the marginal likelihood, the denominator in Bayes&#39; rule mutate(marginal_likelihood = sum(prior_1 * prior_2 * likelihood_1 * likelihood_2)) %&gt;% # finally, the two-dimensional posterior mutate(posterior = (prior_1 * prior_2 * likelihood_1 * likelihood_2) / marginal_likelihood) %&gt;% # plot! ggplot(aes(x = theta_1, y = theta_2, fill = posterior)) + geom_tile() + scale_fill_viridis_c(option = &quot;A&quot;) + coord_equal() + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) That last plot, my friends, is a depiction of \\[ p(\\theta_1, \\theta_2 | D) = \\frac{p(D | \\theta_1, \\theta_2) p(\\theta_1, \\theta_2)}{p(D)}. \\] 7.4.3 The posterior via the Metropolis algorithm. I’ve got nothing on this. But we’re here to learn HMC anyways. Read on. 7.4.4 Gibbs Hamiltonian Monte Carlo sampling. Figure 7.7 is still out of my skill set. But let’s fit the model with our primary package, brms. First we need to laod brms. library(brms) These, recall, are the data. d &lt;- tibble(z1 = 6, z2 = 2, n1 = 8, n2 = 7) d ## # A tibble: 1 x 4 ## z1 z2 n1 n2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 2 8 7 Kruschke said he was starting us out simply. But within the brms context, this is an intercepts-only multivariate model, which isn’t the simplest of things to code into brms. There are a couple ways to code a multivariate model in brms. With this one, it makes sense to specify the model for each sequence of flips separately. This results in two models, which we’ll call model_1 and model_2. model_1 &lt;- bf(z1 | trials(n1) ~ 1) model_2 &lt;- bf(z2 | trials(n2) ~ 1) Before we fit, we’ll have to address a technicality. The brms package does allow for multivariate Bernoulli models. However, it does not support such models with different numbers of trials across the variables. Since our first variable is of 8 trials and the second is of 7, brms will not support this model using the Bernoulli likelihood. However, we can fit the model in brms as an aggregated binomial model. The main difficulty is that the regularizing beta(2, 2) prior won’t make sense, here. So we’ll opt for the regularizing normal(0, 1), instead. fit1 &lt;- brm(data = d, family = binomial(), model_1 + model_2, prior(normal(0, 1), class = Intercept), iter = 25500, warmup = 500, cores = 1, chains = 1, seed = 7) Here are the results. print(fit1) ## Family: MV(binomial, binomial) ## Links: mu = logit ## mu = logit ## Formula: z1 | trials(n1) ~ 1 ## z2 | trials(n2) ~ 1 ## Data: d (Number of observations: 1) ## Samples: 1 chains, each with iter = 25500; warmup = 500; thin = 1; ## total post-warmup samples = 25000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## z1_Intercept 0.72 0.62 -0.46 1.97 1.00 25094 17061 ## z2_Intercept -0.59 0.63 -1.88 0.62 1.00 20552 16585 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As we’ll learn in later chapters, the parameters of a typical aggregated binomial model are in the log-odds scale. Over time, you will learn how to interpret them. But for now, just be happy that brms offers the inv_logit_scaled() function, which can convert our results back to the probability scale. fixef(fit1)[, 1] %&gt;% inv_logit_scaled() ## z1_Intercept z2_Intercept ## 0.6727986 0.3565870 Here we’ll use posterior_samples() to collect out posterior draws and save them as a data frame, which we’ll name post. post &lt;- posterior_samples(fit1, add_chain = T) With post in hand, we’re ready to make our version of Figure 7.8. To reduce the overplotting, we’re only looking at the first 500 post-warmup iterations. post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% filter(iter &lt; 1001) %&gt;% ggplot(aes(x = theta_1, y = theta_2)) + geom_point(alpha = 1/4) + geom_path(size = 1/10, alpha = 1/2) + coord_equal(xlim = 0:1, ylim = 0:1) + labs(x = expression(theta[1]), y = expression(theta[2])) + theme(panel.grid = element_blank()) Just for kicks and giggles, we’ll plot the marginal posterior densities. You’ll note that even though we didn’t use beta priors, the posteriors look quite beta like. post %&gt;% transmute(`theta[1]` = b_z1_Intercept %&gt;% inv_logit_scaled(), `theta[2]` = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% gather() %&gt;% ggplot(aes(x = value, y = key)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.5, .95)) + scale_y_discrete(NULL, labels = ggplot2:::parse_safe) + xlab(&quot;posterior&quot;) + theme(panel.grid = element_blank()) 7.4.5 Is there a difference between biases? The difference distribution from our brms-based multivariate aggregated binomial model, \\(\\theta_1 - \\theta_2\\), is pretty similar to the ones in Figure 7.9. post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% transmute(`theta_1 - theta_2` = theta_1 - theta_2) %&gt;% ggplot(aes(x = `theta_1 - theta_2`)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = c(-.5, .8)) + xlab(expression(paste(theta[1], &quot; - &quot;, theta[2]))) + theme(panel.grid = element_blank()) Here are the exact estimates of the mode and 95% HDIs for our difference distribution, \\(\\theta_1 - \\theta_2\\). post %&gt;% mutate(theta_1 = b_z1_Intercept %&gt;% inv_logit_scaled(), theta_2 = b_z2_Intercept %&gt;% inv_logit_scaled()) %&gt;% transmute(`theta_1 - theta_2` = theta_1 - theta_2) %&gt;% mode_hdi() ## # A tibble: 1 x 6 ## `theta_1 - theta_2` .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.311 -0.0719 0.641 0.95 mode hdi Given that we used both a different likelihood function, which necessitated a different prior, I think we did pretty good complimenting the results in the text. 7.4.6 Terminology: MCMC. Any simulation that samples a lot of random values from a distribution is called a Monte Carlo simulation, named after the dice and spinners and shufflings of the famous casino locale. The appellation “Monte Carlo” is attributed (Eckhardt, 1987) to the mathematicians Stanislaw Ulam (1909–1984) and John von Neumann (1903–1957). (p. 177) In case you didn’t know, brms is a user-friendly interface for the Stan probabilistic programing language and Stan is named after Stanislaw Ulam. 7.5 MCMC representativeness, accuracy, and efficiency We have three main goals in generating an MCMC sample from the posterior distribution: The values in the chain must be representative of the posterior distribution. They should not be unduly influenced by the arbitrary initial value of the chain, and they should fully explore the range of the posterior distribution without getting stuck. The chain should be of sufficient size so that estimates are accurate and stable. In particular, the estimates of the central tendency (such as median or mode), and the limits of the 95% HDI, should not be much different if the MCMC analysis is run again (using different seed states for the pseudorandom number generators). The chain should be generated efficiently, with as few steps as possible, so not to exceed our patience or computing power. (p. 178, emphasis in the original) 7.5.1 MCMC representativeness. Here are our data. z &lt;- 35 n &lt;- 50 d &lt;- tibble(y = rep(0:1, times = c(n - z, z))) Here we fit the model. Note how since we’re just univariate, it’s easy to switch back to directly modeling with the Bernoulli likelihood. fit2 &lt;- brm(data = d, family = bernoulli(link = identity), y ~ 1, prior(beta(2, 2), class = Intercept), iter = 10000, warmup = 500, cores = 3, chains = 3, seed = 7) On page 179, Kruschke discussed burn-in steps within the Gibbs framework: The preliminary steps, during which the chain moves from its unrepresentative initial value to the modal region of the posterior, is called the burn-in period. For realistic applications, it is routine to apply a burn-in period of several hundred to several thousand steps. For each HMC chain, the first \\(n\\) iterations are warmups. In this example, \\(n = 500\\) (i.e., warmup = 500). Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. For more on warmup, check out McElreath’s lecture, starting here or, for more detail, the HMC Algorithm Parameters section of the Stan Reference Manual, version 2.20. It appears that the upshot of all this is that many of the packages in the Stan ecosystem don’t make it easy to extract the warmup values. For example, the brms::plot() function excludes them from the trace plot without the option to include them. plot(fit2) Notice how the x-axis on the trace plot ranges from 0 to 9,500. Now recall that our model code included iter = 10000, warmup = 500. Those 9,500 iterations in the trace plot are excluding the first 500 warmup iterations. This code is a little janky, but if you really want those warmup iterations, you can extract them from the fit2 object like this: warmups &lt;- c(fit2$fit@sim$samples[[1]]$b_Intercept[1:500], fit2$fit@sim$samples[[2]]$b_Intercept[1:500], fit2$fit@sim$samples[[3]]$b_Intercept[1:500]) %&gt;% # since these come from lists, here we&#39;ll convert them to a data frame as.data.frame() %&gt;% rename(b_Intercept = &quot;.&quot;) %&gt;% # we&#39;ll need to recapture the iteration and chain information mutate(iter = rep(1:500, times = 3), chain = rep(1:3, each = 500)) %&gt;% mutate(chain = factor(chain, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;))) warmups %&gt;% head() ## b_Intercept iter chain ## 1 0.2981264 1 1 ## 2 0.2981264 2 1 ## 3 0.2981264 3 1 ## 4 0.2981264 4 1 ## 5 0.3129055 5 1 ## 6 0.3119297 6 1 The bayesplot package makes it easier to reproduce some of the plots in Figure 7.10. library(bayesplot) We’ll reproduce the upper left panel with mcmc_trace(). mcmc_trace(warmups, pars = &quot;b_Intercept&quot;) + theme(panel.grid = element_blank()) It appears our HMC warmup iterations found the posterior quite quickly. Here’s the autocorrelation plot. mcmc_acf(warmups, pars = &quot;b_Intercept&quot;, lags = 25) + theme(panel.grid = element_blank()) Our autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. If you were unhappy with the way mcmc_acf() defaults to faceting the plot by chain, you could always extract the data from the function and use them to make the plot the way you prefer. E.g., mcmc_acf(warmups)$data %&gt;% as_tibble() %&gt;% filter(Parameter == &quot;b_Intercept&quot;) %&gt;% ggplot(aes(x = Lag, y = AC, color = Chain %&gt;% as.factor())) + geom_hline(yintercept = 0, color = &quot;white&quot;) + geom_point(size = 2/3) + geom_line() + scale_color_viridis_d(end = .8) + ylab(&quot;Autocorrelation&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) Here are the overlaid densities. mcmc_dens_overlay(warmups, pars = c(&quot;b_Intercept&quot;)) + theme(panel.grid = element_blank()) The densities aren’t great, but they still appear nicer than those in for the burn-in iterations in the text. With our warmups in their current state, I’m not aware how we might conveniently make a shrink factor plot, as seen in the lower left of Figure 7.10. So it goes… Figure 7.11 examined the post-burn-in iterations. We’ll follow suit with our post-warmup iterations. post &lt;- posterior_samples(fit2, add_chain = T) mcmc_trace(post, pars = &quot;b_Intercept&quot;) + theme(panel.grid = element_blank()) The autocorrelation plots: mcmc_acf(post, pars = &quot;b_Intercept&quot;, lags = 40) + theme(panel.grid = element_blank()) As with the warmups, above, the post-warmup autocorrelation plots indicate substantially lower autocorrelations yielded by HMC as implemented by Stan than what Kruschke generated with the MH algorithm. This is one of the reasons folks using HMC tend to use fewer iterations than those using MH or Gibbs. Here are the overlaid densities. mcmc_dens_overlay(post, pars = c(&quot;b_Intercept&quot;)) + theme(panel.grid = element_blank()) And now that we’re focusing on the post-warmup iterations, we can make a shrink factor plot. We’ll do so with the coda::gelman.plot() function. But you can’t just dump your brm() fit object into coda::gelman.plot(). It’s the wrong object type. However, brms offers the as.mcmc() function which will convert brm() objects for use in coda package functions. fit2_c &lt;- as.mcmc(fit2) fit2_c %&gt;% glimpse() ## List of 3 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.695 0.685 0.717 0.728 0.755 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.694 0.655 0.766 0.631 0.65 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## $ : &#39;mcmc&#39; num [1:9500, 1:2] 0.676 0.697 0.754 0.737 0.847 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ iterations: NULL ## .. ..$ parameters: chr [1:2] &quot;b_Intercept&quot; &quot;lp__&quot; ## ..- attr(*, &quot;mcpar&quot;)= num [1:3] 501 10000 1 ## - attr(*, &quot;class&quot;)= chr &quot;mcmc.list&quot; With our freshly-converted fit2_c object in hand, we’re ready to plot. coda::gelman.plot(fit2_c[, &quot;b_Intercept&quot;, ]) Looks great. As Kruschke explained on page 181, that plot is based on the potential scale reduction factor, or \\(\\hat R\\) as it’s typically referred to in the Stan ecosystem. Happily, brms reports the \\(\\hat R\\) values for the major model parameters using print() or summary(). print(fit2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup samples = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9532 13554 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Instead of a running value, you get a single statistic in the ‘Rhat’ column. On page 181, Kruschke discussed how his overlaid density plots include the HDIs, by chain. The convenience functions from brms and bayesplot don’t easily get us there. But we can get those easy enough with a little help tidybayes::geom_halfeyeh(). post %&gt;% ggplot(aes(x = b_Intercept, y = chain, fill = chain)) + geom_halfeyeh(point.interval = mode_hdi, .width = .95) + scale_fill_viridis_d(begin = .35, end = .95) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 7.5.2 MCMC accuracy. We’ll wrangle our post object a bit to make it easier to reproduce Figure 7.12. lagged_post &lt;- post %&gt;% filter(chain == 1) %&gt;% select(b_Intercept, iter) %&gt;% rename(lag_0 = b_Intercept) %&gt;% mutate(lag_1 = lag(lag_0, 1), lag_5 = lag(lag_0, 5), lag_10 = lag(lag_0, 10)) %&gt;% gather(key, value, -iter) head(lagged_post) ## iter key value ## 1 501 lag_0 0.6945376 ## 2 502 lag_0 0.6850353 ## 3 503 lag_0 0.7166854 ## 4 504 lag_0 0.7276339 ## 5 505 lag_0 0.7546228 ## 6 506 lag_0 0.6638754 Here’s our version of the top row. p1 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_1&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 1&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p2 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_5&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 5&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) p3 &lt;- lagged_post %&gt;% filter(key %in% c(&quot;lag_0&quot;, &quot;lag_10&quot;), iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = iter, y = value, color = key)) + geom_point() + geom_line() + scale_color_manual(values = c(&quot;black&quot;, &quot;grey67&quot;)) + labs(x = &quot;Index 1001:1071&quot;, title = &quot;Lag 10&quot;) + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) Here’s the middle row for Figure 7.12. lagged_post_wide &lt;- lagged_post %&gt;% spread(key = key, value = value) p1 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_1, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() + theme(panel.grid = element_blank()) p2 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_5, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() + theme(panel.grid = element_blank()) p3 &lt;- lagged_post_wide %&gt;% filter(iter &gt; 1000 &amp; iter &lt; 1071) %&gt;% ggplot(aes(x = lag_10, y = lag_0)) + stat_smooth(method = &quot;lm&quot;) + geom_point() + theme(panel.grid = element_blank()) grid.arrange(p1, p2, p3, ncol = 3) For kicks and giggles, we used stat_smooth() to add an OLS regression line with its 95% confidence intervals to each plot. If you want the Pearson’s correlations among the lags, the lowerCor() function from the psych package can be handy. library(psych) lagged_post_wide %&gt;% select(-iter) %&gt;% filter(!is.na(lag_10)) %&gt;% lowerCor(digits = 3) ## lag_0 lag_1 lag_10 lag_5 ## lag_0 1.000 ## lag_1 0.510 1.000 ## lag_10 -0.017 -0.008 1.000 ## lag_5 0.033 0.081 0.033 1.000 For our version of the bottom of Figure 7.12, we’ll use the bayesplot::mcmc_acf_bar() function to get the autocorrelation bar plot, by chain. mcmc_acf_bar(post, pars = &quot;b_Intercept&quot;, lags = 20) + theme(panel.grid = element_blank()) All three rows of our versions for Figure 7.12 indicate in their own way how much lower our autocorrelations were than the ones in the text. If you’re curious of the effective sample sizes for the parameters in your brms models, just look at the model summary using either summary() or print(). print(fit2) ## Warning: There were 1 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. ## See http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: d (Number of observations: 50) ## Samples: 3 chains, each with iter = 10000; warmup = 500; thin = 1; ## total post-warmup samples = 28500 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.06 0.56 0.80 1.00 9532 13554 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I’m not quite sure how to reproduce Kruschke’s MCMC ESS simulation studies. If you’ve got it figured out, please share your code. If you’re interested in the Monte Carlo standard error (MCSE) for your brms parameters, the easiest way is to tack $fit onto your fit object. fit2$fit ## Inference for Stan model: 40d4a5c85526772aeb02dd0e607b20d7. ## 3 chains, each with iter=10000; warmup=500; thin=1; ## post-warmup draws per chain=9500, total post-warmup draws=28500. ## ## mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat ## b_Intercept 0.69 0.00 0.06 0.56 0.64 0.69 0.73 0.80 9387 1 ## lp__ -30.80 0.01 0.69 -32.80 -30.96 -30.53 -30.36 -30.31 14051 1 ## ## Samples were drawn using NUTS(diag_e) at Sun Oct 27 15:27:00 2019. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). This returns an rstan-like summary. The ‘se_mean’ column is the MCSE. 7.5.3 MCMC efficiency. Kruschke wrote: “It is often the case in realistic applications that there is strong autocorrelation for some parameters, and therefore, an extremely long chain is required to achieve an adequate ESS or MCSE” (p. 187). As we’ll see, this is generally less of a problem for HMC than for MH or Gibbs. But it does still crop up, particularly in complicated models. As he wrote on the following page, “one sampling method that can be relatively efficient is Hamiltonian Monte Carlo.” Indeed. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] psych_1.8.12 gridExtra_2.3 bayesplot_1.7.0 brms_2.10.3 Rcpp_1.0.2 tidybayes_1.1.0 ## [7] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 readr_1.3.1 tidyr_1.0.0 ## [13] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.1 ## [4] rsconnect_0.8.15 ggstance_0.3.2 markdown_1.1 ## [7] base64enc_0.1-3 rstudioapi_0.10 rstan_2.19.2 ## [10] svUnit_0.7-12 DT_0.9 fansi_0.4.0 ## [13] lubridate_1.7.4 xml2_1.2.0 bridgesampling_0.7-2 ## [16] mnormt_1.5-5 knitr_1.23 shinythemes_1.1.2 ## [19] zeallot_0.1.0 jsonlite_1.6 broom_0.5.2 ## [22] shiny_1.3.2 compiler_3.6.0 httr_1.4.0 ## [25] backports_1.1.5 assertthat_0.2.1 Matrix_1.2-17 ## [28] lazyeval_0.2.2 cli_1.1.0 later_1.0.0 ## [31] htmltools_0.4.0 prettyunits_1.0.2 tools_3.6.0 ## [34] igraph_1.2.4.1 coda_0.19-3 gtable_0.3.0 ## [37] glue_1.3.1 reshape2_1.4.3 cellranger_1.1.0 ## [40] vctrs_0.2.0 nlme_3.1-139 crosstalk_1.0.0 ## [43] xfun_0.10 ps_1.3.0 rvest_0.3.4 ## [46] mime_0.7 miniUI_0.1.1.1 lifecycle_0.1.0 ## [49] gtools_3.8.1 MASS_7.3-51.4 zoo_1.8-6 ## [52] scales_1.0.0 colourpicker_1.0 hms_0.4.2 ## [55] promises_1.1.0 Brobdingnag_1.2-6 parallel_3.6.0 ## [58] inline_0.3.15 shinystan_2.5.0 yaml_2.2.0 ## [61] StanHeaders_2.19.0 loo_2.1.0 stringi_1.4.3 ## [64] dygraphs_1.1.1.6 pkgbuild_1.0.5 rlang_0.4.0 ## [67] pkgconfig_2.0.3 matrixStats_0.55.0 HDInterval_0.2.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5 labeling_0.3 tidyselect_0.2.5 ## [76] processx_3.4.1 plyr_1.8.4 magrittr_1.5 ## [79] R6_2.4.0 generics_0.0.2 foreign_0.8-71 ## [82] pillar_1.4.2 haven_2.1.0 withr_2.1.2 ## [85] xts_0.11-2 abind_1.4-5 modelr_0.1.4 ## [88] crayon_1.3.4 arrayhelpers_1.0-20160527 utf8_1.1.4 ## [91] rmarkdown_1.13 grid_3.6.0 readxl_1.3.1 ## [94] callr_3.3.2 threejs_0.3.1 digest_0.6.21 ## [97] xtable_1.8-4 httpuv_1.5.2 stats4_3.6.0 ## [100] munsell_0.5.0 viridisLite_0.3.0 shinyjs_1.0 "],
["jags-brms.html", "8 JAGS brms 8.1 JAGS brms and its relation to R 8.2 A complete example 8.3 Simplified scripts for frequently used analyses 8.4 Example: Difference of biases 8.5 Sampling from the prior distribution in JAGS brms 8.6 Probability distributions available in JAGS brms 8.7 Faster sampling with parallel processing in runjags brms::brm() 8.8 Tips for expanding JAGS brms models Reference Session info", " 8 JAGS brms We, of course, will be using brms in place of JAGS. 8.1 JAGS brms and its relation to R In the opening prargraph in his GitHub repository for brms, Bürkner explained: The brms package provides an interface to fit Bayesian generalized (non-)linear multivariate multilevel models using Stan, which is a C++ package for performing full Bayesian inference (see http://mc-stan.org/). The formula syntax is very similar to that of the package lme4 to provide a familiar and simple interface for performing regression analyses. A wide range of response distributions are supported, allowing users to fit – among others – linear, robust linear, count data, survival, response times, ordinal, zero-inflated, and even self-defined mixture models all in a multilevel context. Further modeling options include non-linear and smooth terms, auto-correlation structures, censored data, missing value imputation, and quite a few more. In addition, all parameters of the response distribution can be predicted in order to perform distributional regression. Multivariate models (i.e., models with multiple response variables) can be fit, as well. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed and compared with posterior predictive checks, cross-validation, and Bayes factors. (emphasis in the original) Bürkner’s brms repository includes many helpful links, such as to where brms lives on CRAN, a list of blog posts highlighting brms, and a forum where users can ask questions about brms in specific or about Stan in general. You can install the current official version of brms in the same way you would any other R package (i.e., install.packages(&quot;brms&quot;, dependencies = T)). If you want the current developmental version, you could download it from GitHub by executing the following. if (!requireNamespace(&quot;devtools&quot;)) { install.packages(&quot;devtools&quot;) } devtools::install_github(&quot;paul-buerkner/brms&quot;) 8.2 A complete example We express the likelihood for our coin toss example as \\[y_{i} \\sim \\text{Bernoulli} (\\theta).\\] Our prior will be \\[\\theta \\sim \\text{Beta} (\\alpha, \\beta).\\] 8.2.1 Load data. “Logically, models of data start with the data. We must know their basic scale and structure to conceive of a descriptive model” (p. 197). Here we load the data with the readr::read_csv() function, the tidyverse version of base R `read.csv(). library(tidyverse) my_data &lt;- read_csv(&quot;data.R/z15N50.csv&quot;) Unlike what Kruschke wrote about JAGS, the brms package does not require us to convert the data into a list. It can handle data in lists or data frames, of which tibbles are a special case. Here are what the data look like. head(my_data) ## # A tibble: 6 x 1 ## y ## &lt;dbl&gt; ## 1 0 ## 2 1 ## 3 0 ## 4 0 ## 5 0 ## 6 0 We might visualize them in a bar plot. my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar() + theme(panel.grid = element_blank()) If you wanted to compute “Ntotal”, the number of rows in our tibble, one way is with count(). my_data %&gt;% count() ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 50 However, we’re not going to do anything with an “Ntotal” value. For brms, the data are fine in their current data frame form. No need for a dataList. 8.2.2 Specify model. Let’s open brms. library(brms) The brms package doesn’t have code blocks following the JAGS format or the sequence in Kurschke’s diagrams. Rather, its syntax is modeled in part after the popular frequentist mixed-effects package, lme4. To learn more about how brms compares to lme4, see Bürkner’s overview. The primary function in brms is brm(). Into this one function we will specify the data, the model, the likelihood function, the prior(s), and any technical settings such as the number of MCMC chains, iterations, and so forth. You can order the arguments in any way you like. My typical practice is to start with data, family (i.e., the likelihood function), the model formula, and my priors. If there are any technical specifications such as the number of MHC iterations I’d like to change from their default values, I usually do that last. Here’s how to fit the model. fit1 &lt;- brm(data = my_data, family = bernoulli(link = identity), formula = y ~ 1, prior(beta(2, 2), class = Intercept), iter = 500 + 3334, warmup = 500, chains = 3, seed = 8) For a more detailed explanation of the brms::brm() function, spend some time with the brm section of the brms reference manual. 8.2.3 Initialize chains. In Stan, and in brms by extension, the initial values have default settings. In Initialization section of the Program Execution chapter in the online Stan Reference Manual, Version 2.21 we read If there are no user-supplied initial values, the default initialization strategy is to initialize the unconstrained parameters directly with values drawn uniformly from the interval \\((−2, 2)\\). The bounds of this initialization can be changed but it is always symmetric around 0. The value of 0 is special in that it represents the median of the initialization. An unconstrained value of 0 corresponds to different parameter values depending on the constraints declared on the parameters.&quot; In general, I don’t recommend setting custom initial values in brms or Stan. Under the hood, Stan will transform the parameters to the unconstrained space in models where they are bounded. In our Bernoulli model, \\(\\theta\\) is bounded at 0 and 1. A little further down in the same section, we read For parameters bounded above and below, the initial value of 0 on the unconstrained scale corresponds to a value at the midpoint of the constraint interval. For probability parameters, bounded below by 0 and above by 1, the transform is the inverse logit, so that an initial unconstrained value of 0 corresponds to a constrained value of 0.5, -2 corresponds to 0.12 and 2 to 0.88. Bounds other than 0 and 1 are just scaled and translated. If you want to play around with this, have at it. In my experience, the only time it helps to set these manually is when you want to fix them to zero. You can do that by specifying inits = &quot;0&quot; within brm(). 8.2.4 Generate chains. By default, brms will use 4 chains of 2000 iterations each. The type of MCMC brms uses is Hamiltonian Monte Carlo (HMC). You can learn more about HMC at the Stan website, which includes the Stan User’s Guide, the Stan Reference Manual, and a list of tutorials. McElreath also has a nice intro lecture on MCMC in general and HMC in particular. Michael Bentacourt has some good lectures on Stan and HMC, such as here and here. And, of course, we will cover HMC with Kruschke in Chapter 14. Within each HMC chain, the first \\(n\\) iterations are warmups. Within the Stan-HMC paradigm, warmups are somewhat analogous to but not synonymous with burn-in iterations as done by the Gibbs sampling in JAGS. But HMC warmups are like Gibbs burn-ins in that both are discarded and not used to describe the posterior. As such, the brms default settings yield 1000 post-warmup iterations for each of the 4 HMC chains. However, we specified iter = 500 + 3334, warmup = 500, chains = 3. Thus instead of defaults, we have 3 HMC chains. Each chain has 500 + 3334 = 3834 total iterations, of which 500 were discarded warmup iterations. To learn more about the warmup stage in Stan, check out the HMC Algorithm Parameters section of the MCMC Sampling chapter of the Stan Reference Manual 8.2.5 Examine chains. The brms::plot() function returns a density and trace plot for each model parameter. plot(fit1) If you want to display each chain as its own density, you can use the handy mcmc_dens_overlay() function from the bayesplot package. library(bayesplot) But before we do so, we’ll need to export the posterior samples into a data frame, for which we’ll employ posterior_samples(). post &lt;- posterior_samples(fit1, add_chain = T) Note the add_chain = T argument, which will allow us to differentiate the draws by their chain of origin. But anyway, here are the overlaid densities. mcmc_dens_overlay(post, pars = c(&quot;b_Intercept&quot;)) + theme(panel.grid = element_blank()) The bayesplot::mcmc_acf() function will give us the autocorrelation plots. mcmc_acf(post, pars = &quot;b_Intercept&quot;, lags = 35) With brms functions, we get a sole \\(\\hat R\\) value for each parameter rather than a running vector. rhat(fit1)[&quot;b_Intercept&quot;] ## b_Intercept ## 1.00023 We’ll have to employ brms::as.mcmc() and coda::gelman.plot() to make our running \\(\\hat R\\) plot. fit1_c &lt;- as.mcmc(fit1) coda::gelman.plot(fit1_c[, &quot;b_Intercept&quot;, ]) 8.2.5.1 The plotPost function How to plot your brms posterior distributions. We’ll get into plotting in just a moment. But before we do, here’s a summary of the model. print(fit1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup samples = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.20 0.44 1.00 3678 4655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). To summarize a posterior in terms of central tendency, brms defaults to the mean value (i.e., the value in the ‘Estimate’ column of the print() output). In many of the other convenience functions, you can also request the median instead. For example, we can use the robust = T argument to get the ‘Estimate’ in terms of the median. posterior_summary(fit1, robust = T) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept 0.314162 0.06271794 0.2005124 0.4431792 ## lp__ -30.522541 0.29765456 -32.6877345 -30.3052625 Across functions, the intervals default to 95%. With print() and summary() you can adjust the level with a prob argument. For example, here we’ll use 50% intervals. print(fit1, prob = .5) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: my_data (Number of observations: 50) ## Samples: 3 chains, each with iter = 3834; warmup = 500; thin = 1; ## total post-warmup samples = 10002 ## ## Population-Level Effects: ## Estimate Est.Error l-50% CI u-50% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.32 0.06 0.27 0.36 1.00 3678 4655 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). But in many other brms convenience functions, you can use the probs argument to request specific percentile summaries. posterior_summary(fit1, probs = c(.025, .25, .75, .975)) ## Estimate Est.Error Q2.5 Q25 Q75 Q97.5 ## b_Intercept 0.3160697 0.06246065 0.2005124 0.2726817 0.357283 0.4431792 ## lp__ -30.7894587 0.68273263 -32.6877345 -30.9497898 -30.354684 -30.3052625 Regardless of what prob or probs levels you use, brms functions always return percentile-based estimates. All this central tendency and interval talk will be important in a moment… When plotting the posterior distribution of a parameter estimated with brms, you typically do so working with the results of an object returned by posterior_samples(). Recall we already saved those samples as post. head(post) ## b_Intercept lp__ chain iter ## 1 0.3885602 -31.04295 1 501 ## 2 0.4013501 -31.28580 1 502 ## 3 0.3733681 -30.79754 1 503 ## 4 0.3764648 -30.84373 1 504 ## 5 0.3862548 -31.00268 1 505 ## 6 0.2774596 -30.42087 1 506 With post in hand, we can use ggplot2 to do the typical distributional plots, such as with geom_histogram(). post %&gt;% ggplot(aes(x = b_Intercept)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) The bayesplot::mcmc_areas() function offers a nice way to depict the posterior densities, along with their percentile-based 50% and 95% ranges. mcmc_areas( post, pars = c(&quot;b_Intercept&quot;), prob = 0.5, prob_outer = 0.95, point_est = &quot;mean&quot; ) + scale_y_discrete(NULL, breaks = NULL) + labs(title = &quot;Theta&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) brms doesn’t have a convenient way to compute the posterior mode or HDIs. Base R is no help, either. But Matthew Kay’s tidybayes package makes it easy to compute posterior modes and HDIs. The tidybayes package gives us access to the handy geom_halfeyeh() function. library(tidybayes) post %&gt;% ggplot(aes(x = b_Intercept, y = 0)) + geom_halfeyeh(point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) The tidybayes::geom_halfeyeh() function returns a density with a measure of the posterior’s central tendency in a dot and one or multiple interval bands as horizontal lines at the base of the density. Since we used the point_interval = mode_hdi argument, we asked for the mode to be our measure of central tendency and the highest posterior density intervals to be our intervals. With .width = c(.95, .5), we requested our HDIs be at both the 95% and 50% levels. To be more congruent with Kruschke’s plotting sensibilities, we can combine geom_histogram() with tidybayes::stat_pointintervalh(). # this is unnecessary, but makes for nicer x-axis breaks my_breaks &lt;- mode_hdi(post$b_Intercept)[, 1:3] %&gt;% gather(key, breaks) %&gt;% mutate(labels = breaks %&gt;% round(digits = 3)) # here&#39;s the main plot code post %&gt;% ggplot(aes(x = b_Intercept)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 40) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_x_continuous(breaks = my_breaks$breaks, labels = my_breaks$labels) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;Theta&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) With the point_interval argument within the stat_pointintervalh() function, we can request different combinations of measures of central tendency (i.e., mean, median, mode) and interval types (i.e., percentile-based and HDIs). Although all of these are legitimate ways to summarize a posterior, they can yield somewhat different results. For example, here we’ll contrast our mode + HDI summary with a median + percentile-based interval summary. post %&gt;% ggplot(aes(x = b_Intercept)) + stat_pointintervalh(aes(y = 1), point_interval = median_qi, .width = c(.95, .5)) + stat_pointintervalh(aes(y = 2), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = 1:2, labels = c(&quot;median_qi&quot;, &quot;mode_hdi&quot;)) + coord_cartesian(ylim = 0:3) + labs(title = &quot;Theta&quot;, x = expression(theta)) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) Similar, yet distinct. 8.3 Simplified scripts for frequently used analyses A lot has happened in R for Bayesian analysis since Kruschke wrote his text. In addition to our use of the tidyverse, the brms, bayesplot, and tidybayes packages offer an array of useful convenience functions. We can and occasionally will write our own. But really, the rich R ecosystem already has us pretty much covered. 8.4 Example: Difference of biases Here are our new data. my_data &lt;- read_csv(&quot;data.R/z6N8z2N7.csv&quot;) glimpse(my_data) ## Observations: 15 ## Variables: 2 ## $ y &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0 ## $ s &lt;chr&gt; &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Reginald&quot;, &quot;Re… They look like this. my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar() + theme(panel.grid = element_blank()) + facet_wrap(~s) Here we fit the model with brms::brm(). fit2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior = c(prior(beta(2, 2), class = b, coef = sReginald), prior(beta(2, 2), class = b, coef = sTony)), iter = 6000, warmup = 5000, cores = 4, chains = 4, # this line isn&#39;t always necessary, but it will let us use `prior_samples()` later sample_prior = T, control = list(adapt_delta = .999), seed = 8) More typically, we’d parameterize the model as y ~ 1 + s. This form would yield an intercept and a slope. Behind the scenes, brms would treat the nominal s variable as an 0-1 coded dummy variable. One of the nominal levels would become the reverence category, depicted by the Intercept, and the difference between that and the other category would be the s slope. However, with our y ~ 0 + s syntax, we’ve suppressed the typical model intercept. The consequence is that each level of the nominal variable s gets its own intercept or [i] index, if you will. This is analogous to Kruschke’s y[i] ∼ dbern(theta[s[i]]) code. Also, notice our use of the control = list(adapt_delta = .999) argument. By default, adapt_delta = .8. Leaving it at its default for this model resulted in “divergent transitions after warmup” warnings, which urged me to increase “adapt_delta above 0.8.” The model fit well after raising it to .999 and increasing the number of warmup samples. See the brms Reference manual for more on adapt_delta. All that aside, here are the chains. plot(fit2) The model summary() is as follows: summary(fit2) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 0 + s ## Data: my_data (Number of observations: 15) ## Samples: 4 chains, each with iter = 6000; warmup = 5000; thin = 1; ## total post-warmup samples = 4000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sReginald 0.67 0.13 0.39 0.89 1.00 1760 1821 ## sTony 0.36 0.14 0.13 0.65 1.00 1837 1590 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). The brms::pairs() function gets us the bulk of Figure 8.6. pairs(fit2, off_diag_args = list(size = 1/3, alpha = 1/3)) But to get at that difference-score distribution, we’ll have extract the posterior iterations with posterior_samples(), make difference score with mutate(), and manually plot with ggplot2. post &lt;- posterior_samples(fit2) post &lt;- post %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) glimpse(post) ## Observations: 4,000 ## Variables: 6 ## $ theta_Reginald &lt;dbl&gt; 0.5287682, 0.6097360, 0.7972532, 0.7778351, 0.7044172, 0.40… ## $ theta_Tony &lt;dbl&gt; 0.3324897, 0.3057314, 0.2492853, 0.2449084, 0.3836122, 0.40… ## $ prior_b_sReginald &lt;dbl&gt; 0.34506299, 0.69256517, 0.44661952, 0.48778682, 0.54056976,… ## $ prior_b_sTony &lt;dbl&gt; 0.1271459, 0.4352422, 0.2385522, 0.6839308, 0.5565150, 0.54… ## $ lp__ &lt;dbl&gt; -8.862774, -8.446904, -8.677803, -8.594283, -8.303235, -10.… ## $ `theta_Reginald - theta_Tony` &lt;dbl&gt; 0.1962784909, 0.3040045478, 0.5479678103, 0.5329266827, 0.3… gathered_post &lt;- post %&gt;% select(starts_with(&quot;theta&quot;)) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) gathered_post %&gt;% ggplot(aes(x = value, group = key)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .50)) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_x&quot;) Here’s a way to get the numeric summaries out of post. gathered_post %&gt;% group_by(key) %&gt;% mode_hdi() ## Warning: unnest() has a new interface. See ?unnest for details. ## Try `df %&gt;% unnest(c(.lower, .upper))`, with `mutate()` if needed ## # A tibble: 3 x 7 ## key value .lower .upper .width .point .interval ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta_Reginald 0.707 0.411 0.907 0.95 mode hdi ## 2 theta_Tony 0.297 0.109 0.633 0.95 mode hdi ## 3 theta_Reginald - theta_Tony 0.336 -0.0612 0.680 0.95 mode hdi In this context, the mode_hdi() summary yields: key (i.e., the name we used to denote the parameters) value (i.e., the value of the measure of central tendency) .lower (i.e., the lower level of the 95% HDI) .upper (i.e., the upper level…) .width (i.e., what interval we used) .point (i.e., the type of measure of central tendency) .interval (i.e., the type of interval) 8.5 Sampling from the prior distribution in JAGS brms The sample_prior = T argument in our brm() code allowed us to extract prior samples with the well-named prior_samples() function. prior &lt;- prior_samples(fit2) head(prior) ## b_sReginald b_sTony ## 1 0.3450630 0.1271459 ## 2 0.6925652 0.4352422 ## 3 0.4466195 0.2385522 ## 4 0.4877868 0.6839308 ## 5 0.5405698 0.5565150 ## 6 0.2326884 0.5445573 With prior in hand, we’re almost ready to make the prior histograms of Figure 8.7. But first we’ll want to determine the \\(z/N\\) values in order to mark them off in the plots. [You’ll note Kruschke did so with gray plus marks in his.] my_data %&gt;% group_by(s) %&gt;% summarise(z = sum(y), N = n()) %&gt;% mutate(`z/N` = z / N) ## # A tibble: 2 x 4 ## s z N `z/N` ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Reginald 6 8 0.75 ## 2 Tony 2 7 0.286 d_line &lt;- tibble(value = c(.75, .286, .75 - .286), key = factor(c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;), levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) Behold the histograms of Figure 8.7. prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% mutate(`theta_Reginald - theta_Tony` = theta_Reginald - theta_Tony) %&gt;% gather() %&gt;% mutate(key = factor(key, levels = c(&quot;theta_Reginald&quot;, &quot;theta_Tony&quot;, &quot;theta_Reginald - theta_Tony&quot;))) %&gt;% ggplot(aes(x = value, group = key)) + geom_vline(data = d_line, aes(xintercept = value), color = &quot;white&quot;, size = 1) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .50)) + scale_y_continuous(NULL, breaks = NULL) + theme_grey() + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_x&quot;) Here’s how to make the scatter plot. prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + geom_point(alpha = 1/4, color = &quot;grey50&quot;) + coord_equal() + theme(panel.grid = element_blank()) Or you could always use a two-dimensional density plot with stat_density_2d(). prior %&gt;% rename(theta_Reginald = b_sReginald, theta_Tony = b_sTony) %&gt;% ggplot(aes(x = theta_Reginald, y = theta_Tony)) + stat_density_2d(aes(fill = stat(density)), geom = &quot;raster&quot;, contour = F) + scale_fill_viridis_c(option = &quot;B&quot;) + labs(x = expression(theta[1]), y = expression(theta[2])) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) 8.6 Probability distributions available in JAGS brms [brms] has a large collection of frequently used probability distributions that are built-in. These distributions include the beta, gamma, normal, Bernoulli, and binomial along with many others. A complete list of distributions, and their [brms] names, can be found in [Bürkner’s vignette Parameterization of Response Distributions in brms]. (pp. 213–214, emphasis added) 8.6.1 Defining new likelihood functions. In addition to all the likelihood functions listed in above mentioned vignette, Parameterization of Response Distributions in brms, you can also make your own likelihood functions. Bürkner explained the method in his vignette Define Custom Response Distributions with brms. 8.7 Faster sampling with parallel processing in runjags brms::brm() We don’t need to open another package to sample in parallel in brms. In fact, we’ve already been doing that. Take another look at the code use used for the last model, fit2. fit2 &lt;- brm(data = my_data, family = bernoulli(identity), y ~ 0 + s, prior = c(prior(beta(2, 2), class = b, coef = sReginald), prior(beta(2, 2), class = b, coef = sTony)), iter = 6000, warmup = 5000, cores = 4, chains = 4, sample_prior = T, control = list(adapt_delta = .999), seed = 8) See the cores = 4, chains = 4 arguments? With that bit of code, we told brms::brm() we wanted 4 chains, which we ran in parallel across 4 cores. 8.8 Tips for expanding JAGS brms models I’m in complete agreement with Kruschke, here: Often, the process of programming a model is done is stages, starting with a simple model and then incrementally incorporating complexifications. At each step, the model is checked for accuracy and efficiency. This procedure of incremental building is useful for creating a desired complex model from scratch, for expanding a previously created model for a new application, and for expanding a model that has been found to be inadequate in a posterior predictive check. (p. 218) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 bayesplot_1.7.0 brms_2.10.3 Rcpp_1.0.2 forcats_0.4.0 stringr_1.4.0 ## [7] dplyr_0.8.3 purrr_0.3.2 readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [13] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ggridges_0.5.1 ## [4] rsconnect_0.8.15 ggstance_0.3.2 markdown_1.1 ## [7] base64enc_0.1-3 rstudioapi_0.10 rstan_2.19.2 ## [10] svUnit_0.7-12 DT_0.9 fansi_0.4.0 ## [13] lubridate_1.7.4 xml2_1.2.0 bridgesampling_0.7-2 ## [16] knitr_1.23 shinythemes_1.1.2 zeallot_0.1.0 ## [19] jsonlite_1.6 broom_0.5.2 shiny_1.3.2 ## [22] compiler_3.6.0 httr_1.4.0 backports_1.1.5 ## [25] assertthat_0.2.1 Matrix_1.2-17 lazyeval_0.2.2 ## [28] cli_1.1.0 later_1.0.0 htmltools_0.4.0 ## [31] prettyunits_1.0.2 tools_3.6.0 igraph_1.2.4.1 ## [34] coda_0.19-3 gtable_0.3.0 glue_1.3.1 ## [37] reshape2_1.4.3 cellranger_1.1.0 vctrs_0.2.0 ## [40] nlme_3.1-139 crosstalk_1.0.0 xfun_0.10 ## [43] ps_1.3.0 rvest_0.3.4 mime_0.7 ## [46] miniUI_0.1.1.1 lifecycle_0.1.0 gtools_3.8.1 ## [49] MASS_7.3-51.4 zoo_1.8-6 scales_1.0.0 ## [52] colourpicker_1.0 hms_0.4.2 promises_1.1.0 ## [55] Brobdingnag_1.2-6 parallel_3.6.0 inline_0.3.15 ## [58] shinystan_2.5.0 yaml_2.2.0 gridExtra_2.3 ## [61] loo_2.1.0 StanHeaders_2.19.0 stringi_1.4.3 ## [64] dygraphs_1.1.1.6 pkgbuild_1.0.5 rlang_0.4.0 ## [67] pkgconfig_2.0.3 matrixStats_0.55.0 HDInterval_0.2.0 ## [70] evaluate_0.14 lattice_0.20-38 rstantools_2.0.0 ## [73] htmlwidgets_1.5 labeling_0.3 tidyselect_0.2.5 ## [76] processx_3.4.1 plyr_1.8.4 magrittr_1.5 ## [79] R6_2.4.0 generics_0.0.2 pillar_1.4.2 ## [82] haven_2.1.0 withr_2.1.2 xts_0.11-2 ## [85] abind_1.4-5 modelr_0.1.4 crayon_1.3.4 ## [88] arrayhelpers_1.0-20160527 utf8_1.1.4 rmarkdown_1.13 ## [91] grid_3.6.0 readxl_1.3.1 callr_3.3.2 ## [94] threejs_0.3.1 digest_0.6.21 xtable_1.8-4 ## [97] httpuv_1.5.2 stats4_3.6.0 munsell_0.5.0 ## [100] viridisLite_0.3.0 shinyjs_1.0 "],
["hierarchical-models.html", "9 Hierarchical Models 9.1 A single coin from a single mint 9.2 Multiple coins from a single mint 9.3 Shrinkage in hierarchical models 9.4 Speeding up JAGS brms 9.5 Extending the hierarchy: Subjects within categories Reference Session info", " 9 Hierarchical Models As Kruschke put it, “There are many realistic situations that involve meaningful hierarchical structure. Bayesian modeling software makes it straightforward to specify and analyze complex hierarchical models” (p. 221). IMO, brms makes it even easier than JAGS. 9.1 A single coin from a single mint Recall from the last chapter that our likelihood is the Bernoulli distribution, \\[y_i \\sim \\operatorname{Bernoulli} (\\theta).\\] We’ll use the Beta density for our prior distribution for \\(\\theta\\), \\[\\theta \\sim \\operatorname{Beta} (\\alpha, \\beta).\\] And we can re-express \\(\\alpha\\) and \\(\\beta\\) in terms of the mode \\(\\omega\\) and concentration \\(\\kappa\\), such that \\[\\alpha = \\omega(\\kappa - 2) + 1 \\textrm{ and } \\beta = (1 - \\omega)(\\kappa - 2) + 1.\\] The consequence of this is that we can re-express \\(\\theta\\) as \\[\\theta \\sim \\operatorname{Beta} (\\omega(\\kappa - 2) + 1, (1 - \\omega)(\\kappa - 2) + 1).\\] On page 224, Kruschke wrote: “The value of \\(\\kappa\\) governs how near \\(\\theta\\) is to \\(\\omega\\), with larger values of \\(\\kappa\\) generating values of \\(\\theta\\) more concentrated near \\(\\omega\\).” To give a sense of that, we’ll simulate 20 beta distributions, all with \\(\\omega = .25\\) but with \\(\\theta\\) increasing from 10 to 200, by 10. library(tidyverse) library(ggridges) beta_by_k &lt;- function(k){ w &lt;- .25 tibble(x = seq(from = 0, to = 1, length.out = 1000)) %&gt;% mutate(theta = dbeta(x = x, shape1 = w * (k - 2) + 1, shape2 = (1 - w) * (k - 2) + 1)) } tibble(k = seq(from = 10, to = 200, by = 10)) %&gt;% mutate(theta = map(k, beta_by_k)) %&gt;% unnest() %&gt;% ggplot(aes(x = x, y = k, height = theta, group = k, fill = k)) + geom_vline(xintercept = c(0, .25, .5), color = &quot;grey85&quot;, size = 1/2) + geom_ridgeline(size = 1/5, color = &quot;grey92&quot;, scale = 2) + scale_fill_viridis_c(expression(kappa), option = &quot;A&quot;) + scale_y_continuous(expression(kappa), breaks = seq(from = 10, to = 200, by = 10)) + xlab(expression(theta)) + theme(panel.grid = element_blank()) Holding \\(\\omega\\) constant, the density gets more concentrated around \\(\\omega\\) as \\(\\kappa\\) increases. 9.1.1 Posterior via grid approximation. Given \\(\\alpha\\) and \\(\\beta\\), we can compute the corresponding mode \\(\\omega\\). To foreshadow, consider \\(\\text{Beta} (2, 2)\\). alpha &lt;- 2 beta &lt;- 2 (alpha - 1) / (alpha + beta - 2) ## [1] 0.5 That is, the mode of \\(\\text{Beta} (2, 2) = .5\\). We won’t be able to make the wireframe plots on the left of Figure 9.2, but we can make the others. We’ll make the initial data following Kruschke’s (p. 226) formulas. \\[p(\\theta, \\omega) = p(\\theta | \\omega) p(\\omega) = \\operatorname{Beta} (\\theta | \\omega (100 - 2) + 1, (1 - \\omega) (100 - 2) + 1) \\operatorname{Beta} (\\omega | 2, 2)\\] First, we’ll make a custom function, make_prior() based on the formulas. make_prior &lt;- function(theta, omega, alpha, beta, kappa) { # p(theta | omega) t &lt;- dbeta(x = theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta, omega) = p(theta | omega) * p(omega) return(t * o) } Next we’ll define the parameter space as a tightly-spaced sequence of values ranging from 0 to 1. parameter_space &lt;- seq(from = 0, to = 1, by = .01) Now we’ll use parameter_space to define the ranges for the two variables, theta and omega, which we’ll save in a tibble. We’ll then sequentially feed those theta and omega values into our make_prior() while manually specifying the desired values for alpha, beta, and kappa. d &lt;- # these first two functions define the grid for our grid approximation tibble(theta = parameter_space, omega = parameter_space) %&gt;% expand(theta, omega) %&gt;% # compute the joint prior mutate(prior = make_prior(theta, omega, alpha = 2, beta = 2, kappa = 100)) %&gt;% # convert the prior from the density metric to the probability metric mutate(prior = prior / sum(prior)) head(d) ## # A tibble: 6 x 3 ## theta omega prior ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 ## 2 0 0.01 0 ## 3 0 0.02 0 ## 4 0 0.03 0 ## 5 0 0.04 0 ## 6 0 0.05 0 Now we’re ready to plot the top middle panel of Figure 9.2. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Note, you could also make this with geom_tile(). But geom_raster() with the interpolate = T argument smooths the color transitions. If we collapse “the joint prior across \\(\\theta\\)” (i.e., group_by(omega) and then sum(prior)), we plot the marginal distribution for \\(p(\\omega)\\) as seen in the top right panel. d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;)&quot;))) + coord_flip(ylim = c(0, .03)) + theme(panel.grid = element_blank()) We’ll follow a similar procedure to get the marginal probability distribution for theta. d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;)&quot;))) + coord_cartesian(ylim = c(0, .03)) + theme(panel.grid = element_blank()) We’ll use the filter() function to take the two slices from the posterior grid. Since we’re taking slices, we’re no longer working with the joint probability distribution. As such, our two marginal prior distributions for theta no longer sum to 1, which means they’re no longer in a probability metric. No worries. After we group by omega, we can simply divide prior by the sum() of prior which renormalizes the two slices “so that they are individually proper probability densities that sum to 1.0 over \\(\\theta\\)” (p. 226). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + coord_cartesian() + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed) As Kruschke pointed out at the top of page 228, these are indeed Beta densities. Here’s proof. # we&#39;ll want this for the annotation text &lt;- tibble(theta = c(.75, .25), y = 10.5, label = c(&quot;Beta(74.5, 25.5)&quot;, &quot;Beta(25.5, 74.5)&quot;), omega = letters[1:2]) # here&#39;s the primary data for the plot tibble(theta = rep(parameter_space, times = 2), alpha = rep(c(74.5, 25.5), each = 101), beta = rep(c(25.5, 74.5), each = 101), omega = rep(letters[1:2], each = 101)) %&gt;% # the plot ggplot(aes(x = theta, fill = omega)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_text(data = text, aes(y = y, label = label, color = omega)) + scale_fill_viridis_d(option = &quot;A&quot;, begin = .3, end = .7) + scale_color_viridis_d(option = &quot;A&quot;, begin = .3, end = .7) + labs(x = expression(theta), y = &quot;density&quot;) + coord_cartesian(ylim = 0:12) + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) But back on track, we need the Bernoulli likelihood function for the lower three rows of Figure 9.2. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed theta and our data into the bernoulli_likelihood() function, which will allow us to make the 2-dimensional density plot in the middle of Figure 9.2. # define the data n &lt;- 12 z &lt;- 9 trial_data &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihood d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Note how this plot demonstrates how the likelihood is solely dependent on \\(\\theta\\); it’s orthogonal to \\(\\omega\\). This is the visual consequence of Kruschke’s Formula 9.6, \\[\\begin{align*} p (\\theta, \\omega | y) &amp; = \\frac{p (y | \\theta, \\omega) \\; p (\\theta, \\omega)}{p (y)} \\\\ &amp; = \\frac{p (y | \\theta) \\; p (\\theta | \\omega) \\; p (\\omega)}{p (y)}. \\end{align*}\\] That is, in the second line of the equation, the probability of \\(y\\) was only conditional on \\(\\theta\\). But the reason we call this a hierarchical model is because the probability of \\(\\theta\\) itself is conditioned on \\(\\omega\\). The prior itself had a prior. From Formula 9.1, the posterior \\(p(\\theta, \\omega | D)\\) is proportional to \\(p(D | \\theta) \\; p(\\theta | \\omega) \\; p(\\omega)\\). Divide that by the normalizing constant and we’ll have it in a proper probability metric. Recall that we’ve already saved the results of \\(p(\\theta | \\omega) \\; p(\\omega)\\) in the prior column. So we just need to multiply prior by likelihood and divide by their sum. Our first depiction will be the middle panel of the second row from the bottom. d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Although the likelihood was orthogonal to \\(\\omega\\), conditioning the prior for \\(\\theta\\) on \\(\\omega\\) resulted in a posterior that was conditioned on both \\(\\theta\\) and \\(\\omega\\). Making the marginal plots for posterior is much like when making them for prior, above. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;|D)&quot;))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;|D)&quot;))) + coord_cartesian() + theme(panel.grid = element_blank()) Note that after we slice with filter(), the next two wrangling lines renormalize those posterior slices into probability metrics. That is, when we take a slice through the joint posterior at a particular value of \\(\\omega\\), and renormalize by dividing the sum of discrete probability masses in that slice, we get the conditional distribution \\(p(\\theta | \\omega, D)\\). d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) To repeat the process for Figure 9.3, we’ll first compute the new joint prior. d &lt;- tibble(theta = parameter_space, omega = parameter_space) %&gt;% expand(theta, omega) %&gt;% mutate(prior = make_prior(theta, omega, alpha = 20, beta = 20, kappa = 6)) %&gt;% mutate(prior = prior / sum(prior)) Here’s the initial data and the 2-dimensional density plot for the prior, the middle plot in the top row of Figure 9.3. d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) That higher certainty in \\(\\omega\\) resulted in a two-dimensional density plot where the values on the y-axis were concentrated near .5. This will have down-the-road consequences for the posterior. But before we get there, we’ll average over omega and theta to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;)&quot;))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;)&quot;))) + coord_cartesian(ylim = c(0, .03)) + theme(panel.grid = element_blank()) Here are the two short plots in the right panel of the second row from the top of Figure 9.3. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(prior = prior / sum(prior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + coord_cartesian(ylim = c(0, .0375)) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed) Now we’re ready for the likelihood. # compute d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Now on to the posterior. Our first depiction will be the middle panel of the second row from the bottom of Figure 9.3. This will be \\(p(\\theta, \\omega | y)\\). # compute the posterior d &lt;- d %&gt;% mutate(posterior = (likelihood * prior) / sum(likelihood * prior)) # plot d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Here are the marginal plots for the two dimensions in our posterior. # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;|D)&quot;))) + coord_flip() + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;|D)&quot;))) + coord_cartesian() + theme(panel.grid = element_blank()) And we’ll finish off with the plots of Figure 9.3’s lower right panel. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% mutate(posterior = posterior / sum(posterior)) %&gt;% mutate(label = factor(str_c(&quot;omega == &quot;, omega), levels = c(&quot;omega == 0.75&quot;, &quot;omega == 0.25&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + coord_cartesian(ylim = c(0, .0375)) + theme(panel.grid = element_blank()) + facet_wrap(~label, ncol = 1, labeller = label_parsed, scales = &quot;free&quot;) 9.2 Multiple coins from a single mint What if we collect data from more than one coin created by the mint? If each coin has its own distinct bias \\(\\theta_s\\), then we are estimating a distinct parameter value for each coin, and using all the data to estimate \\(\\omega\\). (p. 230) 9.2.1 Posterior via grid approximation. Now we have two coins, the full prior distribution is a joint distribution over three parameters: \\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\). In a grid approximation, the prior is specified as a three-dimensional (3D) array that holds the prior probability at various grid points in the 3D space. (p. 233) So we’re going to have to update our make_prior() function. It was originally designed to handle two dimensions, \\(\\theta\\) and \\(\\omega\\). But now we have to update it to handle our three dimensions. make_prior &lt;- function(theta1, theta2, omega, alpha, beta, kappa) { # p(theta_1 | omega) t1 &lt;- dbeta(x = theta1, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(theta_2 | omega) t2 &lt;- dbeta(x = theta2, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1) # p(omega) o &lt;- dbeta(x = omega, shape1 = alpha, shape2 = beta) # p(theta1, theta2, omega) = p(theta1 | omega) * p(theta2 | omega) * p(omega) return(t1 * t2 * o) } Let’s make our new data object, d. d &lt;- tibble(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% expand(theta_1, theta_2, omega) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 5)) %&gt;% # here we normalize mutate(prior = prior / sum(prior)) glimpse(d) ## Observations: 1,030,301 ## Variables: 4 ## $ theta_1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ theta_2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ omega &lt;dbl&gt; 0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.… ## $ prior &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Unlike what Kruschke said in the text, we’re not using a 3D data array. Rather, we’re just using a tibble with which prior has been expanded across all possible dimensions of the three indexing variables: theta_1, theta_2, and omega. As you can see from the ‘Observations’ count, above, this makes for a very long tibble. “Because the parameter space is 3D, a distribution on it cannot easily be displayed on a 2D page. Instead, Figure 9.5 shows various marginal distributions” (p. 234). The consequence of that is when we marginalize, we’ll have to group by the two variables we’d like to retain for the plot. For example, the plots in the left and middle columns of the top row are the same save for their indices. So let’s just do the plot for theta_1. In order to marginalize over theta_2, we’ll need to group_by(theta_1, omega) and then summarise(prior = sum(prior)). d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) But we just have to average over omega and theta_1 to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;p(&quot;, omega, &quot;)&quot;))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(paste(&quot;p(&quot;, theta[1], &quot;)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Before we make the plots in the middle row of Figure 9.5, we need to add the likelihoods. Recall that we’re presuming the coin flips contained in \\(D_1\\) and \\(D_2\\) are independent. Kruschke explained in Chapter 7, section 4.1, that independence of the data across the two coins means that the data from coin 1 depend only on the bias in coin 1, and the data from coin 2 depend only on the bias in coin 2, which can be expressed formally as \\(p(y_1 | \\theta_1, \\theta_2) = p(y_1 | \\theta_1)\\) and \\(p(y_2 | \\theta_1, \\theta_2) = p(y_2 | \\theta_2)\\). (p. 164) The likelihood function for our two series of coin flips is then \\[p(D | \\theta_1, \\theta_2) = \\Big ( \\theta_1^{z_1} (1 - \\theta_1) ^ {N_1 - z_1} \\Big ) \\Big ( \\theta_2^{z_2} (1 - \\theta_2) ^ {N_2 - z_2} \\Big ).\\] The upshot is we can compute the likelihoods for \\(D_1\\) and \\(D_2\\) separately and just multiply them together. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) head(d) ## # A tibble: 6 x 7 ## theta_1 theta_2 omega prior likelihood_1 likelihood_2 likelihood ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 0 ## 2 0 0 0.01 0 0 0 0 ## 3 0 0 0.02 0 0 0 0 ## 4 0 0 0.03 0 0 0 0 ## 5 0 0 0.04 0 0 0 0 ## 6 0 0 0.05 0 0 0 0 Now after a little group_by() followed by summarise() we can plot the two marginal likelihoods, the two plots in the middle row of Figure 9.5. # likelihood_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) # likelihood_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) The likelihoods look good. Next we compute the posterior in the same way we’ve done before: multiply the prior and the likelihood and then divide by their sum in order to convert the results to a probability metric. # compute d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) # posterior_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Here’s the right plot on the second row from the bottom, the posterior distribution for \\(\\omega\\). # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;p(&quot;, omega, &quot;|D)&quot;))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Now here are the marginal posterior plots on the bottom row of Figure 9.5. # for theta_1 d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(paste(&quot;p(&quot;, theta[1], &quot;|D)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_2 d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[2]), y = expression(paste(&quot;p(&quot;, theta[2], &quot;|D)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) We’ll do this dog and pony one more time for Figure 9.6, which uses different priors on the same data. First, we make our new data object, d. d &lt;- tibble(theta_1 = parameter_space, theta_2 = parameter_space, omega = parameter_space) %&gt;% expand(theta_1, theta_2, omega) %&gt;% mutate(prior = make_prior(theta_1, theta_2, omega, alpha = 2, beta = 2, kappa = 75)) %&gt;% mutate(prior = prior / sum(prior)) Note how the only thing we changed from the last time was increasubf kappa to 75. Also like last time, the plots in the left and middle columns of the top row are the same save for their indices. So we’ll just focus on the one on the left. d %&gt;% group_by(theta_1, omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Now we’ll average over omega and theta to plot their marginal prior distributions. # for omega d %&gt;% group_by(omega) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;p(&quot;, omega, &quot;)&quot;))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta d %&gt;% group_by(theta_1) %&gt;% summarise(prior = sum(prior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(paste(&quot;p(&quot;, theta[1], &quot;)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Let’s get those likelihoods in there and plot. # D1: 3 heads, 12 tails n &lt;- 15 z &lt;- 3 trial_data_1 &lt;- rep(0:1, times = c(n - z, z)) # D2: 4 heads, 1 tail n &lt;- 5 z &lt;- 4 trial_data_2 &lt;- rep(0:1, times = c(n - z, z)) # compute the likelihoods d &lt;- d %&gt;% mutate(likelihood_1 = bernoulli_likelihood(theta = theta_1, data = trial_data_1), likelihood_2 = bernoulli_likelihood(theta = theta_2, data = trial_data_2)) %&gt;% mutate(likelihood = likelihood_1 * likelihood_2) # plot likelihood_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) # plot likelihood_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(likelihood = sum(likelihood)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Compute the posterior and make the left and middle plots of the second row to the bottom of Figure 9.6. d &lt;- d %&gt;% mutate(posterior = (prior * likelihood) / sum(prior * likelihood)) # posterior_1 d %&gt;% group_by(theta_1, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[1]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) # posterior_2 d %&gt;% group_by(theta_2, omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta[2]), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Here’s the right plot on the same row, the posterior distribution for \\(\\omega\\). # for omega d %&gt;% group_by(omega) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(omega), y = expression(paste(&quot;p(&quot;, omega, &quot;|D)&quot;))) + coord_flip(ylim = c(0, .04)) + theme(panel.grid = element_blank()) Finally, here are the marginal posterior plots on the bottom row of Figure 9.6. # for theta_1 d %&gt;% group_by(theta_1) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_1, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[1]), y = expression(paste(&quot;p(&quot;, theta[1], &quot;|D)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) # for theta_2 d %&gt;% group_by(theta_2) %&gt;% summarise(posterior = sum(posterior)) %&gt;% ggplot(aes(x = theta_2, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta[2]), y = expression(paste(&quot;p(&quot;, theta[2], &quot;|D)&quot;))) + coord_cartesian(ylim = c(0, .04)) + theme(panel.grid = element_blank()) The grid approximation displayed in Figures 9.5 and 9.6 used combs of only [101] points on each parameter (\\(\\omega\\), \\(\\theta_1\\), and \\(\\theta_2\\)). This means that the 3D grid had [1013 = 1,030,301] points, which is a size that can be handled easily on an ordinary desktop computer of the early 21st century. It is interesting to remind ourselves that the grid approximation displayed in Figures 9.5 and 9.6 would have been on the edge of computability 50 years ago, and would have been impossible 100 years ago. The number of points in a grid approximation can get hefty in a hurry. If we were to expand the example by including a third coin, with its parameter \\(\\theta_3\\), then the grid would have [1014 = 104,060,401] points, which already strains small computers. Include a fourth coin, and the grid contains over [10 billion] points. Grid approximation is not a viable approach to even modestly large problems, which we encounter next. (p. 235) In case you didn’t catch it, we used different numbers of points to evaluate each parameter. Whereas Kruschke indicated in the text he only used 50, we used 101. That value of 101 came from how we defined our parameter_space with the code seq(from = 0, to = 1, by = .01). The reason we used a more densely-packed parameter space was to get smoother-looking 2D density plots. 9.2.2 A realistic model with MCMC. “Because the value of \\(\\kappa − 2\\) must be non-negative, the prior distribution on \\(\\kappa − 2\\) must not allow negative values” (p. 237). Gamma is one of the distributions with that property. The gamma distribution is defined by two parameters, its shape and rate. To get a sense of how those play out, here’ a look at the gamma densities of Figure 9.8. # how many points do you want in your sequence of x values? length &lt;- 100 # wrangle tibble(shape = c(.01, 1.56, 1, 6.25), rate = c(.01, .0312, .02, .125)) %&gt;% expand(nesting(shape, rate), x = seq(from = 0, to = 200, length.out = length)) %&gt;% mutate(mean = shape * 1 / rate, sd = sqrt(shape * (1 / rate)^2)) %&gt;% mutate(label = str_c(&quot;shape = &quot;, shape, &quot;, rate = &quot;, rate, &quot;\\nmean = &quot;, mean, &quot;, sd = &quot;, sd)) %&gt;% # plot ggplot(aes(x = x, ymin = 0, ymax = dgamma(x = x, shape = shape, rate = rate))) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(breaks = c(0, .01, .02)) + coord_cartesian(xlim = 0:150) + labs(x = expression(kappa), y = expression(paste(&quot;p(&quot;, kappa, &quot;|s, r)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~label) You can find the formulas for the mean and \\(SD\\) for a given gamma distribution here. We used those formulas in the second mutate() statement for the data-prep stage of that last figure. Using \\(s\\) for shape and \\(r\\) for rate, Kruschke’s equations 9.7 and 9.8 are as follows: \\[ s = \\frac{\\mu^2}{\\sigma^2} \\;\\; \\text{and} \\;\\; r = \\frac{\\mu}{\\sigma^2} \\;\\; \\text{for mean} \\;\\; \\mu &gt; 0 \\\\ s = 1 + \\omega r \\;\\; \\text{where} \\;\\; r = \\frac{\\omega + \\sqrt{\\omega^2 + 4\\sigma^2}}{2\\sigma^2} \\;\\; \\text{for mode} \\;\\; \\omega &gt; 0. \\] With those in hand, we can follow Kruschke’s DBDA2E-utilities.R file to make a couple convenience functions. gamma_s_and_r_from_mean_sd &lt;- function(mean, sd) { if (mean &lt;= 0) stop(&quot;mean must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) shape &lt;- mean^2 / sd^2 rate &lt;- mean / sd^2 return(list(shape = shape, rate = rate)) } gamma_s_and_r_from_mode_sd &lt;- function(mode, sd) { if (mode &lt;= 0) stop(&quot;mode must be &gt; 0&quot;) if (sd &lt;= 0) stop(&quot;sd must be &gt; 0&quot;) rate &lt;- (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2) shape &lt;- 1 + mode * rate return(list(shape = shape, rate = rate)) } They’re easy to put to use: gamma_s_and_r_from_mean_sd(mean = 10, sd = 100) ## $shape ## [1] 0.01 ## ## $rate ## [1] 0.001 gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) ## $shape ## [1] 1.105125 ## ## $rate ## [1] 0.01051249 Here’s a more detailed look at the structure of their output. gamma_param &lt;- gamma_s_and_r_from_mode_sd(mode = 10, sd = 100) str(gamma_param) ## List of 2 ## $ shape: num 1.11 ## $ rate : num 0.0105 9.2.3 Doing it with JAGS brms. Unlike JAGS, the brms formula will not correspond as closely to Figure 9.7. You’ll see in just a bit. 9.2.4 Example: Therapeutic touch. Load the data from the TherapeuticTouchData.csv file. my_data &lt;- read_csv(&quot;data.R/TherapeuticTouchData.csv&quot;) glimpse(my_data) ## Observations: 280 ## Variables: 2 ## $ y &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ s &lt;chr&gt; &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;, &quot;S01&quot;,… Here are what the data look like: my_data %&gt;% mutate(y = y %&gt;% as.character()) %&gt;% ggplot(aes(x = y)) + geom_bar(aes(fill = stat(count))) + scale_y_continuous(breaks = seq(from = 0, to = 9, by = 3)) + scale_fill_viridis_c(option = &quot;A&quot;, end = .7) + coord_flip() + theme(legend.position = &quot;none&quot;, panel.grid = element_blank()) + facet_wrap(~s, ncol = 7) And here’s our Figure 9.9. my_data %&gt;% group_by(s) %&gt;% summarize(mean = mean(y)) %&gt;% ggplot(aes(x = mean)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .1) + coord_cartesian(xlim = 0:1) + labs(x = &quot;Proportion Correct&quot;, y = &quot;# Practitioners&quot;) + theme(panel.grid = element_blank()) Let’s open brms. library(brms) In applied statistics, the typical way to model a Bernoulli variable is with logistic regression. Instead of going through the pain of setting up a model in brms that mirrors the one in the text, I’m going to set up a hierarchical logistic regression model, instead. Note the family = bernoulli(link = logit) argument. In work-a-day regression with vanilla Gaussian variables, the prediction space is unbounded. But when we want to model the probability of a success for a Bernoulli variable (i.e., \\(\\theta\\)), we need to constrain the model to only produce predictions between 0 and 1. With logistic regression, we use a link function to do just that. The consequence is that instead of modeling the probability, \\(\\theta\\), we’re modeling the logit probability. In case you’re curious, the logit of \\(\\theta\\) follows the formula \\[\\operatorname{logit} (\\theta) = \\log \\big (\\theta/(1 - \\theta) \\big ).\\] But anyway, we’ll be doing logistic regression using the logit link. Kruschke will cover this in detail in Chapter 21. The next new part of our syntax is (1 | s). As in the popular frequentist lme4 package, you specify random effects or group-level parameters with the (|) syntax in brms. On the left side of the |, you tell brms what parameters you’d like to make random (i.e., vary by group). On the right side of the |, you tell brms what variable you want to group the parameters by. In our case, we want the intercepts to vary over the grouping variable s. fit1 &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) As it turns out, the \\(N(0, 1.5)\\) prior is flat in the probability space for the intercept in a logistic regression model. We’ll explore that a little further down. The \\(N(0, 1)\\) prior for the random effect is actually a half Normal. That’s because brms defaults to bound \\(SD\\) parameters to zero and above. The half Normal prior for a hierarchical \\(SD\\) parameter in a logistic regression model is weakly regularizing and is conservative in the sense that it presumes some pooling is preferable to no pooling. If you wanted to take a lighter approach, you might use something like a cauchy(0, 5), instead. See the prior wiki by the Stan team for more ideas on priors. Here are the trace plots and posterior densities of the main parameters. plot(fit1) The trace plots indicate no problems with convergence. We’ll need to extract the posterior samples and open the bayesplot package before we can examine the autocorrelations. post &lt;- posterior_samples(fit1, add_chain = T) library(bayesplot) One of the nice things about bayesplot is it returns ggplot2 objects. As such, we can amend their theme settings to be consistent with our other ggplot2 plots. The theme_set() function will make that particularly easy. And since I prefer to plot without gridlines, we’ll slip in a line on panel.grid to suppress them by default for the remainder of this chapter. theme_set(theme_grey() + theme(panel.grid = element_blank())) Now we’re ready for bayesplot::mcmc_acf(). mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sd_s__Intercept&quot;), lags = 10) It appears fit1 had very low autocorrelations. Here we’ll examine the \\(N_{eff}/N\\) ratio. neff_ratio(fit1) %&gt;% mcmc_neff() The \\(N_{eff}/N\\) ratio values for our model parameters were excellent. And if you really wanted them, you could get the parameter labels on the y-axis by tacking + yaxis_text() on at the end of the plot block. Here’s a numeric summary of the model. print(fit1) ## Family: bernoulli ## Links: mu = logit ## Formula: y ~ 1 + (1 | s) ## Data: my_data (Number of observations: 280) ## Samples: 4 chains, each with iter = 20000; warmup = 1000; thin = 10; ## total post-warmup samples = 7600 ## ## Group-Level Effects: ## ~s (Number of levels: 28) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.28 0.18 0.02 0.68 1.00 7438 7638 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -0.25 0.14 -0.53 0.02 1.00 7237 7507 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). We’ll need brms::inv_logit_scaled() to convert the model parameters to predict \\(\\theta\\) rather than \\(\\operatorname{logit} (\\theta)\\). After the conversions, we’ll be ready to make the histograms in the lower portion of Figure 9.10. library(tidybayes) post_small &lt;- post %&gt;% # convert the linter model to the probability space with `inv_logit_scaled()` mutate(`theta[1]` = (b_Intercept + `r_s[S01,Intercept]`) %&gt;% inv_logit_scaled(), `theta[14]` = (b_Intercept + `r_s[S14,Intercept]`) %&gt;% inv_logit_scaled(), `theta[28]` = (b_Intercept + `r_s[S28,Intercept]`) %&gt;% inv_logit_scaled()) %&gt;% # make the difference distributions mutate(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% select(starts_with(&quot;theta&quot;)) post_small %&gt;% gather() %&gt;% # this line is unnecessary, but will help order the plots mutate(key = factor(key, levels = c(&quot;theta[1]&quot;, &quot;theta[14]&quot;, &quot;theta[28]&quot;, &quot;theta[1] - theta[14]&quot;, &quot;theta[1] - theta[28]&quot;, &quot;theta[14] - theta[28]&quot;))) %&gt;% ggplot(aes(x = value)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + facet_wrap(~key, scales = &quot;free&quot;, ncol = 3) If you wanted the specific values of the posterior modes and 95% HDIs, you could execute this. post_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## Warning: unnest() has a new interface. See ?unnest for details. ## Try `df %&gt;% unnest(c(.lower, .upper))`, with `mutate()` if needed ## # A tibble: 6 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 theta[1] 0.431 0.206 0.521 0.95 mode hdi ## 2 theta[1] - theta[14] -0.003 -0.284 0.117 0.95 mode hdi ## 3 theta[1] - theta[28] -0.01 -0.414 0.068 0.95 mode hdi ## 4 theta[14] 0.426 0.282 0.578 0.95 mode hdi ## 5 theta[14] - theta[28] -0.004 -0.329 0.105 0.95 mode hdi ## 6 theta[28] 0.452 0.357 0.697 0.95 mode hdi And here are the Figure 9.10 scatter plots. library(gridExtra) p1 &lt;- post_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + coord_cartesian(xlim = 0:1, ylim = 0:1) p2 &lt;- post_small %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + coord_cartesian(xlim = 0:1, ylim = 0:1) p3 &lt;- post_small %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_abline(linetype = 1, color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + coord_cartesian(xlim = 0:1, ylim = 0:1) grid.arrange(p1, p2, p3, ncol = 3) This is posterior distribution for the population estimate for \\(\\theta\\), which roughly corresponds to the upper right histogram of \\(\\omega\\) in Figure 9.10. # this part makes it easier to set the break points in `scale_x_continuous()` labels &lt;- post %&gt;% transmute(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% mode_hdi() %&gt;% select(theta:.upper) %&gt;% gather() %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) %&gt;% slice(1:3) post %&gt;% mutate(theta = b_Intercept %&gt;% inv_logit_scaled()) %&gt;% ggplot(aes(x = theta)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_x_continuous(breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) I’m not aware there’s a straight conversion to get \\(\\sigma\\) in a probability metric. As far as I can tell, you have to first use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (p. 43 of the brms Reference manual, version 2.10.0). With the model coefficient draws in hand, you can index them by posterior iteration, group them by that index, compute the iteration-level \\(SD\\)s, and then plot the distribution of the \\(SD\\)s. # the tibble of the primary data sigmas &lt;- coef(fit1, summary = F)$s %&gt;% as_tibble() %&gt;% mutate(iter = 1:n()) %&gt;% group_by(iter) %&gt;% gather(key, value, -iter) %&gt;% mutate(theta = inv_logit_scaled(value)) %&gt;% summarise(sd = sd(theta)) # this, again, is just to customize `scale_x_continuous()` labels &lt;- sigmas %&gt;% mode_hdi(sd) %&gt;% select(sd:.upper) %&gt;% gather() %&gt;% mutate(label = value %&gt;% round(3) %&gt;% as.character) %&gt;% slice(1:3) # the plot sigmas %&gt;% ggplot(aes(x = sd)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30, boundary = 0) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_x_continuous(breaks = labels$value, labels = labels$label) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(sigma, &quot; of &quot;, theta, &quot; in a probability metric&quot;))) And now you have a sense of how to do all those by hand, bayesplot::mcmc_pairs() offers a fairly quick way to get a good portion of Figure 9.10. color_scheme_set(&quot;gray&quot;) coef(fit1, summary = F)$s %&gt;% inv_logit_scaled() %&gt;% as_tibble() %&gt;% rename(`theta[1]` = S01.Intercept, `theta[14]` = S14.Intercept, `theta[28]` = S28.Intercept) %&gt;% select(`theta[1]`, `theta[14]`, `theta[28]`) %&gt;% mcmc_pairs(off_diag_args = list(size = 1/8, alpha = 1/8)) Did you see how we slipped in that color_scheme_set(&quot;gray&quot;) line? When we used theme_set(), earlier, that changed the global theme settings for our ggplot2 plots. The color_scheme_set() function is specific to bayesplot plots and it sets the color palette within them. Setting the color palette “gray” changed the colors depicted in the dots and bars of the mcmc_pairs()-based scatter plots and histograms, respectively. Kruschke used a \\(\\operatorname{Beta} (1, 1)\\) prior for \\(\\omega\\). If you randomly draw from that prior and plot a histogram, you’ll see it was flat. set.seed(1) tibble(prior = rbeta(n = 1e5, 1, 1)) %&gt;% ggplot(aes(x = prior)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + xlab(expression(omega)) + theme(legend.position = &quot;none&quot;) You’ll note that plot corresponds to the upper right panel of Figure 9.11. Recall that we used a logistic regression model with a normal(0, 1.5) prior on the intercept. If you sample from normal(0, 1.5) and then convert the draws using brms::inv_logit_scaled(), you’ll discover that our normal(0, 1.5) prior was virtually flat on the probability scale. Here we’ll show the consequence of a variety of zero-mean Gaussian priors for the intercept of a logistic regression model: # define a function r_norm &lt;- function(i, n = 1e4) { set.seed(1) rnorm(n = n, mean = 0, sd = i) %&gt;% inv_logit_scaled() } # simulate and wrangle tibble(sd = seq(from = .25, to = 3, by = .25)) %&gt;% group_by(sd) %&gt;% mutate(prior = map(sd, r_norm)) %&gt;% unnest(prior) %&gt;% ungroup() %&gt;% mutate(sd = str_c(&quot;sd = &quot;, sd)) %&gt;% # plot! ggplot(aes(x = prior)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + facet_wrap(~sd) It appears that as \\(\\sigma\\) goes lower than 1.25, the prior becomes increasingly regularizing, pulling the estimate for \\(\\theta\\) to a neutral .5. However, as the prior’s \\(\\sigma\\) gets larger than 1.25, more and more of the probability mass ends up at extreme values. Next, Kruschke examined the prior distribution. There are a few ways to do this. The one we’ll explore involved adding the sample_prior = &quot;only&quot; argument to the brm() function. When you do so, the results of the model are just the prior. That is, brm() leaves out the likelihood. This returns a bunch of samples from the prior predictive distribution. fit1_prior &lt;- brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9, sample_prior = &quot;only&quot;) If we feed fit1_prior into the posterior_samples() function, we’ll get back a data frame of samples from the prior, but with the same parameter names we’d get from the posterior. prior &lt;- posterior_samples(fit1_prior) %&gt;% as_tibble() head(prior) ## # A tibble: 6 x 31 ## b_Intercept sd_s__Intercept `r_s[S01,Interc… `r_s[S02,Interc… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -2.15 1.36 1.18 -0.892 ## 2 -0.825 0.0112 -0.00256 -0.0330 ## 3 0.299 0.315 -0.00208 -0.535 ## 4 2.75 0.654 0.0715 0.821 ## 5 0.350 0.352 -0.127 0.0561 ## 6 -3.23 0.433 -0.680 0.0825 ## # … with 27 more variables: `r_s[S03,Intercept]` &lt;dbl&gt;, ## # `r_s[S04,Intercept]` &lt;dbl&gt;, `r_s[S05,Intercept]` &lt;dbl&gt;, ## # `r_s[S06,Intercept]` &lt;dbl&gt;, `r_s[S07,Intercept]` &lt;dbl&gt;, ## # `r_s[S08,Intercept]` &lt;dbl&gt;, `r_s[S09,Intercept]` &lt;dbl&gt;, ## # `r_s[S10,Intercept]` &lt;dbl&gt;, `r_s[S11,Intercept]` &lt;dbl&gt;, ## # `r_s[S12,Intercept]` &lt;dbl&gt;, `r_s[S13,Intercept]` &lt;dbl&gt;, ## # `r_s[S14,Intercept]` &lt;dbl&gt;, `r_s[S15,Intercept]` &lt;dbl&gt;, ## # `r_s[S16,Intercept]` &lt;dbl&gt;, `r_s[S17,Intercept]` &lt;dbl&gt;, ## # `r_s[S18,Intercept]` &lt;dbl&gt;, `r_s[S19,Intercept]` &lt;dbl&gt;, ## # `r_s[S20,Intercept]` &lt;dbl&gt;, `r_s[S21,Intercept]` &lt;dbl&gt;, ## # `r_s[S22,Intercept]` &lt;dbl&gt;, `r_s[S23,Intercept]` &lt;dbl&gt;, ## # `r_s[S24,Intercept]` &lt;dbl&gt;, `r_s[S25,Intercept]` &lt;dbl&gt;, ## # `r_s[S26,Intercept]` &lt;dbl&gt;, `r_s[S27,Intercept]` &lt;dbl&gt;, ## # `r_s[S28,Intercept]` &lt;dbl&gt;, lp__ &lt;dbl&gt; And here we’ll take a subset of the columns in prior, transform the results to the probability metric, and save the results as prior_samples. prior_samples &lt;- prior %&gt;% transmute(`theta[1]` = b_Intercept + `r_s[S01,Intercept]`, `theta[14]` = b_Intercept + `r_s[S14,Intercept]`, `theta[28]` = b_Intercept + `r_s[S28,Intercept]`) %&gt;% mutate_all(.funs = inv_logit_scaled) head(prior_samples) ## # A tibble: 6 x 3 ## `theta[1]` `theta[14]` `theta[28]` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.276 0.669 0.0393 ## 2 0.304 0.303 0.301 ## 3 0.574 0.527 0.565 ## 4 0.944 0.890 0.788 ## 5 0.556 0.711 0.505 ## 6 0.0197 0.0436 0.0305 Now we can use our prior_samples object to make the diagonal of the lower grid of Figure 9.11. prior_samples %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_x_continuous(labels = c(&quot;0&quot;, &quot;.25&quot;, &quot;.5&quot;, &quot;.75&quot;, &quot;1&quot;)) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:1) + facet_wrap(~key) With a little subtraction, we can reproduce the plots in the upper triangle. prior_samples %&gt;% mutate(`theta[1] - theta[14]` = `theta[1]` - `theta[14]`, `theta[1] - theta[28]` = `theta[1]` - `theta[28]`, `theta[14] - theta[28]` = `theta[14]` - `theta[28]`) %&gt;% select(contains(&quot;] - t&quot;)) %&gt;% gather() %&gt;% ggplot(aes(x = value)) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, binwidth = .05, boundary = 0) + scale_y_continuous(NULL, breaks = NULL) + facet_wrap(~key) Those plots clarify our hierarchical logistic regression model was a little more regularizing than Kruschke’s. The consequence of our priors was more aggressive regularization, greater shrinkage toward zero. The prose in the next section of the text clarifies this isn’t necessarily a bad thing. Finally, here are the plots for the lower triangle in Figure 9.11. p1 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[14]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + geom_abline(linetype = 1, color = &quot;white&quot;) + coord_cartesian(xlim = 0:1, ylim = 0:1) p2 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[1]`, y = `theta[28]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + geom_abline(linetype = 1, color = &quot;white&quot;) + coord_cartesian(xlim = 0:1, ylim = 0:1) p3 &lt;- prior_samples %&gt;% ggplot(aes(x = `theta[14]`, y = `theta[28]`)) + geom_point(color = &quot;grey50&quot;, size = 1/8, alpha = 1/8) + geom_abline(linetype = 1, color = &quot;white&quot;) + coord_cartesian(xlim = 0:1, ylim = 0:1) grid.arrange(p1, p2, p3, ncol = 3) In case you were curious, here are the Pearson’s correlation coefficients among the priors. cor(prior_samples) %&gt;% round(digits = 2) ## theta[1] theta[14] theta[28] ## theta[1] 1.00 0.73 0.73 ## theta[14] 0.73 1.00 0.72 ## theta[28] 0.73 0.72 1.00 9.3 Shrinkage in hierarchical models “In typical hierarchical models, the estimates of low-level parameters are pulled closer together than they would be if there were not a higher-level distribution. This pulling together is called shrinkage of the estimates” (p. 245, emphasis in the original) Further, shrinkage is a rational implication of hierarchical model structure, and is (usually) desired by the analyst because the shrunken parameter estimates are less affected by random sampling noise than estimates derived without hierarchical structure. Intuitively, shrinkage occurs because the estimate of each low-level parameter is influenced from two sources: (1) the subset of data that are directly dependent on the low-level parameter, and (2) the higher-level parameters on which the low-level parameter depends. The higher- level parameters are affected by all the data, and therefore the estimate of a low-level parameter is affected indirectly by all the data, via their influence on the higher-level parameters. (p. 247) Recall Formula 9.4 from page 223, \\[\\theta \\sim \\operatorname{dbeta} (\\omega(\\kappa - 2) + 1), (1 - \\omega)(\\kappa - 2) + 1).\\] With that formula, we can express dbeta()’s shape1 and shape2 in terms of \\(\\omega\\) and \\(\\kappa\\) and make the shapes in Figure 9.12. omega &lt;- 0.5 kappa1 &lt;- 2.1 kappa2 &lt;- 15.8 tibble(x = seq(from = 0, to = 1, by = .001)) %&gt;% mutate(`kappa = 2.1` = dbeta(x = x, shape1 = omega * (kappa1 - 2) + 1, shape2 = (1 - omega) * (kappa1 - 2) + 1), `kappa = 15.8` = dbeta(x = x, shape1 = omega * (kappa2 - 2) + 1, shape2 = (1 - omega) * (kappa2 - 2) + 1)) %&gt;% gather(key, value, - x) %&gt;% mutate(key = factor(key, levels = c(&quot;kappa = 2.1&quot;, &quot;kappa = 15.8&quot;))) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(paste(&quot;Data Proportion or &quot;, theta, &quot; value&quot;)), y = expression(paste(&quot;dbeta(&quot;, theta, &quot;|&quot;, omega, &quot;,&quot;, kappa, &quot;)&quot;))) + facet_wrap(~key) 9.4 Speeding up JAGS brms Here we’ll compare the time it takes to fit fit1 as either bernoulli(link = logit) or binomial(link = logit). # bernoulli start_time_bernoulli &lt;- proc.time() brm(data = my_data, family = bernoulli(link = logit), y ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_bernoulli &lt;- proc.time() # binomial start_time_binomial &lt;- proc.time() brm(data = my_data, family = binomial(link = logit), y | trials(1) ~ 1 + (1 | s), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 20000, warmup = 1000, thin = 10, chains = 4, cores = 4, seed = 9) stop_time_binomial &lt;- proc.time() See how we’re using proc.time() to record when we began and finished evaluating our brm() code? The last time we covered that was way back in Chapter 3. In Chapter 3 we also learned how subtracting the former from the latter yields the total elapsed time. stop_time_bernoulli - start_time_bernoulli ## user system elapsed ## 107.537 3.358 215.631 stop_time_binomial - start_time_binomial ## user system elapsed ## 152.644 3.528 238.123 If you wanted to be rigorous about this, you could do this multiple times in a mini simulation. As to the issue of parallel processing, we’ve been doing this all along. Note our chains = 4, cores = 4 code. 9.5 Extending the hierarchy: Subjects within categories Many data structures invite hierarchical descriptions that may have multiple levels. Software such as JAGS [brms] makes it easy to implement hierarchical models, and Bayesian inference makes it easy to interpret the parameter estimates, even for complex nonlinear hierarchical models. Here, we take a look at one type of extended hierarchical model. (p. 251) Here we depart a little from Kruschke, again. Though we will be fitting a hierarchical model with subjects \\(s\\) within categories \\(c\\), the higher-level parameters will not be \\(\\omega\\) and \\(\\kappa\\). As we’ll go over, below, we will use the binomial distribution within a more conventional hierarchical logistic regression paradigm. In this paradigm, we have an overall intercept, often called \\(\\alpha\\) or \\(\\beta_0\\), which will be our analogue to Kruschke’s overall \\(\\omega\\). For the two grouping categories, \\(s\\) and \\(c\\), we will have \\(\\sigma\\) estimates, which express the variability within those grouping. You’ll see when we get there. 9.5.1 Example: Baseball batting abilities by position. Here are the batting average data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Observations: 948 ## Variables: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dust… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, … ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 27… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, … To give a sense of the data, here are the number of occasions by primary position, PriPos, with their median at bat, AtBats, values. my_data %&gt;% group_by(PriPos) %&gt;% summarise(n = n(), median = median(AtBats)) %&gt;% arrange(desc(n)) ## # A tibble: 9 x 3 ## PriPos n median ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Pitcher 324 4 ## 2 Catcher 103 170 ## 3 Left Field 103 164 ## 4 1st Base 81 265 ## 5 3rd Base 75 267 ## 6 2nd Base 72 228. ## 7 Center Field 67 259 ## 8 Shortstop 63 205 ## 9 Right Field 60 340. As these data are aggregated, we’ll fit with an aggregated binomial model. This is still logistic regression. The Bernoulli distribution is a special case of the binomial distribution when the number of trials in each data point is 1 (see this vignette for details). Since our data are aggregated, the information encoded in Hits is a combination of multiple trials, which requires us to jump up to the more general binomial likelihood. Note the Hits | trials(AtBats) syntax. With that bit, we instructed brms that our criterion, Hits, is an aggregate of multiple trials and the number of trials is encoded in AtBats. Also note the (1 | PriPos) + (1 | PriPos:Player) syntax. In this model, we have two grouping factors, PriPos and Player. Thus we have two (|) arguments. But since players are themselves nested within positions, we have encoded that nesting with the (1 | PriPos:Player) syntax. For more on this style of syntax, see Kristoffer Magnusson’s handy post. Since brms syntax is based on that from the earlier nlme and lme4 packages, the basic syntax rules apply. Bürkner, of course, also covers these topics in the brmsformula subsection of his brms reference manual. fit2 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 9) The chains look good. color_scheme_set(&quot;blue&quot;) plot(fit2) We might examine the autocorrelations within the chains. post &lt;- posterior_samples(fit2, add_chain = T) mcmc_acf(post, pars = c(&quot;b_Intercept&quot;, &quot;sd_PriPos__Intercept&quot;, &quot;sd_PriPos:Player__Intercept&quot;), lags = 8) Here’s a histogram of the \\(N_{eff}/N\\) ratios. fit2 %&gt;% neff_ratio() %&gt;% mcmc_neff_hist(binwidth = .1) + yaxis_text() Happily, most have a very favorable ratio. Here’s a numeric summary of the primary model parameters. print(fit2) ## Family: binomial ## Links: mu = logit ## Formula: Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player) ## Data: my_data (Number of observations: 948) ## Samples: 3 chains, each with iter = 3500; warmup = 500; thin = 1; ## total post-warmup samples = 9000 ## ## Group-Level Effects: ## ~PriPos (Number of levels: 9) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.32 0.10 0.19 0.59 1.00 2660 4283 ## ## ~PriPos:Player (Number of levels: 948) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 0.14 0.01 0.12 0.15 1.00 3695 5911 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept -1.17 0.11 -1.40 -0.94 1.00 1507 2262 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). As far as I’m aware, brms offers three major ways to get the group-level parameters for a hierarchical model: using posterior_samples(), coef(), or fitted(). We’ll cover each, beginning with posterior_samples(). In order to look at the autocorrelation plots, above, we already saved the posterior_samples(fit2) output as post. Here we’ll look at its structure with head(). Before doing so we’ll convert post, which is currently saved as a data frame, into a tibble in order to keep the output from getting unwieldy. post &lt;- post %&gt;% as_tibble() head(post) ## # A tibble: 6 x 963 ## b_Intercept sd_PriPos__Inte… `sd_PriPos:Play… `r_PriPos[1st.B… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.213 0.145 -0.00504 ## 2 -1.13 0.262 0.147 0.101 ## 3 -1.18 0.351 0.149 0.169 ## 4 -1.18 0.279 0.130 0.141 ## 5 -1.16 0.408 0.137 0.0744 ## 6 -1.20 0.344 0.142 0.146 ## # … with 959 more variables: `r_PriPos[2nd.Base,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[3rd.Base,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Catcher,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Center.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Left.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Pitcher,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Right.Field,Intercept]` &lt;dbl&gt;, ## # `r_PriPos[Shortstop,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.Dunn,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.LaRoche,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adam.Lind,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Adrian.Gonzalez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Albert.Pujols,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Allen.Craig,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Anthony.Rizzo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Aubrey.Huff,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Billy.Butler,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Allen,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Belt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Moss,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brandon.Snyder,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brent.Lillibridge,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brett.Pill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Brett.Wallace,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Bryan.LaHair,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Carlos.Lee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Carlos.Pena,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Casey.Kotchman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Casey.McGehee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chad.Tracy,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Carter,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Davis,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Chris.Parmelee,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Corey.Hart,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Dan.Johnson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Daric.Barton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_David.Cooper,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_David.Ortiz,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Edwin.Encarnacion,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Eric.Hinske,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Eric.Hosmer,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Freddie.Freeman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Gaby.Sanchez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Garrett.Jones,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Hector.Luna,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ike.Davis,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_James.Loney,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jason.Giambi,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jeff.Clement,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Jim.Thome,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Joe.Mahoney,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Joey.Votto,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Juan.Rivera,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Justin.Morneau,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Justin.Smoak,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kendrys.Morales,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kila.Kaaihue,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Kyle.Blanks,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Lance.Berkman,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Luke.Scott,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Lyle.Overbay,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mark.Reynolds,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mark.Teixeira,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mat.Gamel,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Adams,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Carpenter,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Downs,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.Hague,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Matt.LaPorta,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mauro.Gomez,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Michael.Young,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Miguel.Cairo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Costanzo,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Jacobs,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mike.Olt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Mitch.Moreland,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Nick.Johnson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Paul.Goldschmidt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Paul.Konerko,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Prince.Fielder,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ryan.Howard,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Steven.Hill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Taylor.Green,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Todd.Helton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Travis.Ishikawa,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Ty.Wigginton,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Yan.Gomes,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Yonder.Alonso,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[1st.Base_Zach.Lutz,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Aaron.Hill,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Adam.Rosales,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Adrian.Cardenas,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Alexi.Amarista,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Alexi.Casilla,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Blake.DeWitt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brandon.Phillips,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brian.Roberts,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Brock.Holt,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Charlie.Culberson,Intercept]` &lt;dbl&gt;, ## # `r_PriPos:Player[2nd.Base_Chase.dArnaud,Intercept]` &lt;dbl&gt;, … In the text, Kruschke described the model as having 968 parameters. Our post tibble has one vector for each, with a couple others tacked onto the end. In the hierarchical logistic regression model, the group-specific parameters for the levels of PriPos are additive combinations of the global intercept vector, b_Intercept and each position-specific vector, r_PriPos[i.Base,Intercept], where i is a fill-in for the position of interest. And recall that since the linear model is of the logit of the criterion, we’ll need to use inv_logit_scaled() to convert that to the probability space. post_small &lt;- post %&gt;% transmute(`1st Base` = (b_Intercept + `r_PriPos[1st.Base,Intercept]`), Catcher = (b_Intercept + `r_PriPos[Catcher,Intercept]`), Pitcher = (b_Intercept + `r_PriPos[Pitcher,Intercept]`)) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we compute our difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(post_small) ## # A tibble: 6 x 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.246 0.237 0.135 -0.102 -0.00921 ## 2 0.264 0.251 0.134 -0.117 -0.0131 ## 3 0.267 0.242 0.122 -0.120 -0.0256 ## 4 0.262 0.242 0.126 -0.116 -0.0201 ## 5 0.252 0.241 0.133 -0.109 -0.0110 ## 6 0.259 0.243 0.137 -0.106 -0.0161 If you take a glance at Figures 9.14 through 9.16 in the text, we’ll be making a lot of histograms of the same basic structure. To streamline our code a bit, we can make a custom histogram plotting function. make_histogram &lt;- function(data, mapping, title, xlim, ...) { ggplot(data, mapping) + geom_histogram(fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + labs(title = title, x = expression(theta)) + coord_cartesian(xlim = xlim) + theme(legend.position = &quot;none&quot;) } We’ll do the same thing for the correlation plots. make_point &lt;- function(data, mapping, limits, ...) { ggplot(data, mapping) + geom_abline(color = &quot;white&quot;) + geom_point(color = &quot;grey50&quot;, size = 1/10, alpha = 1/20) + coord_cartesian(xlim = limits, ylim = limits) } To learn more about wrapping custom plots into custom functions, check out Chapter 12 of Wickham’s ggplot2, Elegant graphics for data analysis. Now we have our make_histogram() and make_point() functions, we’ll use grid.arrange() to paste together the left half of Figure 9.14. p1 &lt;- make_histogram(data = post_small, aes(x = Pitcher), title = &quot;Pitcher&quot;, xlim = c(.1, .25)) p2 &lt;- make_histogram(data = post_small, aes(x = `Pitcher - Catcher`), title = &quot;Pitcher - Catcher&quot;, xlim = c(-.15, 0)) p3 &lt;- make_point(data = post_small, aes(x = Pitcher, y = Catcher), limits = c(.12, .25)) p4 &lt;- make_histogram(data = post_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.1, .25)) grid.arrange(p1, p2, p3, p4, ncol = 2) We could follow the same procedure to make the right portion of Figure 9.14. But instead, let’s switch gears and explore the second way brms affords us for plotting group-level parameters. This time, we’ll use coef(). Up in section 9.2.4, we learned that we can use coef() to “extract [the] model coefficients, which are the sum of population-level effects and corresponding group-level effects” (p. 43 of the brms reference manual). The grouping level we’re interested in is PriPos, so we’ll use that to index the information returned by coef(). Since coef() returns a matrix, we’ll use as_tibble() to convert it to a tibble. coef_primary_position &lt;- coef(fit2, summary = F)$PriPos %&gt;% as_tibble() str(coef_primary_position) ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 9000 obs. of 9 variables: ## $ 1st Base.Intercept : num -1.12 -1.03 -1.01 -1.04 -1.09 ... ## $ 2nd Base.Intercept : num -1.059 -1.072 -1.067 -0.997 -1.169 ... ## $ 3rd Base.Intercept : num -1.01 -1.05 -1.04 -1.06 -1.07 ... ## $ Catcher.Intercept : num -1.17 -1.09 -1.14 -1.14 -1.15 ... ## $ Center Field.Intercept: num -1.05 -1.07 -1.06 -1.06 -1.03 ... ## $ Left Field.Intercept : num -1.08 -1.11 -1.06 -1.06 -1.12 ... ## $ Pitcher.Intercept : num -1.86 -1.87 -1.97 -1.94 -1.88 ... ## $ Right Field.Intercept : num -1.03 -1.05 -1.08 -1.09 -1.01 ... ## $ Shortstop.Intercept : num -1.16 -1.1 -1.14 -1.1 -1.12 ... Keep in mind that coef() returns the values in the logit scale when used for logistic regression models. So we’ll have to use brms::inv_logit_scaled() to convert the estimates to the probability metric. After we’re done converting the estimates, we’ll then make the difference distributions. coef_small &lt;- coef_primary_position %&gt;% select(`1st Base.Intercept`, Catcher.Intercept, Pitcher.Intercept) %&gt;% transmute(`1st Base` = `1st Base.Intercept`, Catcher = Catcher.Intercept, Pitcher = Pitcher.Intercept) %&gt;% mutate_all(inv_logit_scaled) %&gt;% # here we make the difference distributions mutate(`Pitcher - Catcher` = Pitcher - Catcher, `Catcher - 1st Base` = Catcher - `1st Base`) head(coef_small) ## # A tibble: 6 x 5 ## `1st Base` Catcher Pitcher `Pitcher - Catcher` `Catcher - 1st Base` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.246 0.237 0.135 -0.102 -0.00921 ## 2 0.264 0.251 0.134 -0.117 -0.0131 ## 3 0.267 0.242 0.122 -0.120 -0.0256 ## 4 0.262 0.242 0.126 -0.116 -0.0201 ## 5 0.252 0.241 0.133 -0.109 -0.0110 ## 6 0.259 0.243 0.137 -0.106 -0.0161 Now we’re ready for the right half of Figure 9.14. p1 &lt;- make_histogram(data = coef_small, aes(x = Catcher), title = &quot;Catcher&quot;, xlim = c(.22, .27)) p2 &lt;- make_histogram(data = coef_small, aes(x = `Catcher - 1st Base`), title = &quot;Catcher - 1st Base&quot;, xlim = c(-.04, .01)) p3 &lt;- make_point(data = coef_small, aes(x = Catcher, y = `1st Base`), limits = c(.22, .27)) p4 &lt;- make_histogram(data = coef_small, aes(x = `1st Base`), title = &quot;1st Base&quot;, xlim = c(.22, .27)) grid.arrange(p1, p2, p3, p4, ncol = 2) And if you wanted the posterior modes and HDIs, you’d use mode_hdi() after a little wrangling. coef_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.263 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.25 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.111 -0.124 -0.098 0.95 mode hdi While we’re at it, we should capitalize on the opportunity to show how these results are the same as those derived from our posterior_samples() approach, above. post_small %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 5 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1st Base 0.252 0.245 0.263 0.95 mode hdi ## 2 Catcher 0.241 0.233 0.25 0.95 mode hdi ## 3 Catcher - 1st Base -0.013 -0.024 -0.001 0.95 mode hdi ## 4 Pitcher 0.13 0.12 0.14 0.95 mode hdi ## 5 Pitcher - Catcher -0.111 -0.124 -0.098 0.95 mode hdi Success! For Figures 9.15 and 9.16, Kruschke drilled down further into the posterior. To drill along with him, we’ll take the opportunity to showcase fitted(), the third way brms affords us for plotting group-level parameters. # this will make life easier. just go with it name_list &lt;- c(&quot;Kyle Blanks&quot;, &quot;Bruce Chen&quot;, &quot;ShinSoo Choo&quot;, &quot;Ichiro Suzuki&quot;, &quot;Mike Leake&quot;, &quot;Wandy Rodriguez&quot;, &quot;Andrew McCutchen&quot;, &quot;Brett Jackson&quot;) # we&#39;ll define the data we&#39;d like to feed into `fitted()`, here nd &lt;- my_data %&gt;% filter(Player %in% name_list) %&gt;% # these last two lines aren&#39;t typically necessary, # but they allow us to arrange the rows in the same order we find the names in Figures 9.15 and 9.16 mutate(Player = factor(Player, levels = name_list)) %&gt;% arrange(Player) fitted_players &lt;- fitted(fit2, newdata = nd, scale = &quot;linear&quot;, summary = F) %&gt;% as_tibble() %&gt;% # rename the values as returned by `as_tibble()` set_names(name_list) %&gt;% # convert the values from the logit scale to the probability scale mutate_all(inv_logit_scaled) %&gt;% # in this last section, we make our difference distributions mutate(`Kyle Blanks - Bruce Chen` = `Kyle Blanks` - `Bruce Chen`, `ShinSoo Choo - Ichiro Suzuki` = `ShinSoo Choo` - `Ichiro Suzuki`, `Mike Leake - Wandy Rodriguez` = `Mike Leake` - `Wandy Rodriguez`, `Andrew McCutchen - Brett Jackson` = `Andrew McCutchen` - `Brett Jackson`) glimpse(fitted_players) ## Observations: 9,000 ## Variables: 12 ## $ `Kyle Blanks` &lt;dbl&gt; 0.2884479, 0.2444058, 0.30454… ## $ `Bruce Chen` &lt;dbl&gt; 0.1259121, 0.1468900, 0.11606… ## $ `ShinSoo Choo` &lt;dbl&gt; 0.2836534, 0.3041822, 0.26613… ## $ `Ichiro Suzuki` &lt;dbl&gt; 0.2631869, 0.2891847, 0.29991… ## $ `Mike Leake` &lt;dbl&gt; 0.1328278, 0.1611607, 0.17744… ## $ `Wandy Rodriguez` &lt;dbl&gt; 0.11374696, 0.12650915, 0.118… ## $ `Andrew McCutchen` &lt;dbl&gt; 0.3136231, 0.3139831, 0.29643… ## $ `Brett Jackson` &lt;dbl&gt; 0.2840384, 0.2368827, 0.20780… ## $ `Kyle Blanks - Bruce Chen` &lt;dbl&gt; 0.16253577, 0.09751580, 0.188… ## $ `ShinSoo Choo - Ichiro Suzuki` &lt;dbl&gt; 0.020466515, 0.014997563, -0.… ## $ `Mike Leake - Wandy Rodriguez` &lt;dbl&gt; 0.019080819, 0.034651509, 0.0… ## $ `Andrew McCutchen - Brett Jackson` &lt;dbl&gt; 0.02958472, 0.07710036, 0.088… Note our use of the scale = &quot;linear&quot; argument in the fitted() function. By default, fitted() returns predictions on the scale of the criterion. But we don’t want a list of successes and failures; we want player-level parameters. When you specify scale = &quot;linear&quot;, you request fitted() return the values in the parameter scale. Here’s the left portion of Figure 9.15. p1 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks`), title = &quot;Kyle Blanks (1st Base)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Kyle Blanks - Bruce Chen`), title = &quot;Kyle Blanks (1st Base) -\\nBruce Chen (Pitcher)&quot;, xlim = c(-.1, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Kyle Blanks`, y = `Bruce Chen`), limits = c(.09, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Bruce Chen`), title = &quot;Bruce Chen (Pitcher)&quot;, xlim = c(.05, .35)) grid.arrange(p1, p2, p3, p4, ncol = 2) Figure 9.15, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo`), title = &quot;ShinSoo Choo (Right Field)&quot;, xlim = c(.22, .34)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `ShinSoo Choo - Ichiro Suzuki`), title = &quot;ShinSoo Choo (Right Field) -\\nIchiro Suzuki (Right Field)&quot;, xlim = c(-.07, .07)) p3 &lt;- make_point(data = fitted_players, aes(x = `ShinSoo Choo`, y = `Ichiro Suzuki`), limits = c(.23, .32)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Ichiro Suzuki`), title = &quot;Ichiro Suzuki (Right Field)&quot;, xlim = c(.22, .34)) grid.arrange(p1, p2, p3, p4, ncol = 2) Figure 9.16, left: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake`), title = &quot;Mike Leake (Pitcher)&quot;, xlim = c(.05, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Mike Leake - Wandy Rodriguez`), title = &quot;Mike Leake (Pitcher) -\\nWandy Rodriguez (Pitcher)&quot;, xlim = c(-.05, .25)) p3 &lt;- make_point(data = fitted_players, aes(x = `Mike Leake`, y = `Wandy Rodriguez`), limits = c(.07, .25)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Wandy Rodriguez`), title = &quot;Wandy Rodriguez (Pitcher)&quot;, xlim = c(.05, .35)) grid.arrange(p1, p2, p3, p4, ncol = 2) Figure 9.16, right: p1 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen`), title = &quot;Andrew McCutchen (Center Field)&quot;, xlim = c(.15, .35)) p2 &lt;- make_histogram(data = fitted_players, aes(x = `Andrew McCutchen - Brett Jackson`), title = &quot;Andrew McCutchen (Center Field) -\\nBrett Jackson (Center Field)&quot;, xlim = c(0, .20)) p3 &lt;- make_point(data = fitted_players, aes(x = `Andrew McCutchen`, y = `Brett Jackson`), limits = c(.15, .35)) p4 &lt;- make_histogram(data = fitted_players, aes(x = `Brett Jackson`), title = &quot;Brett Jackson (Center Field)&quot;, xlim = c(.15, .35)) grid.arrange(p1, p2, p3, p4, ncol = 2) And if you wanted the posterior modes and HDIs, you’d use mode_hdi() after a little wrangling. fitted_players %&gt;% gather() %&gt;% group_by(key) %&gt;% mode_hdi(value) %&gt;% mutate_if(is.double, round, digits = 3) ## # A tibble: 12 x 7 ## key value .lower .upper .width .point .interval ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Andrew McCutchen 0.307 0.275 0.336 0.95 mode hdi ## 2 Andrew McCutchen - Brett Ja… 0.074 0.019 0.12 0.95 mode hdi ## 3 Brett Jackson 0.233 0.195 0.277 0.95 mode hdi ## 4 Bruce Chen 0.129 0.099 0.164 0.95 mode hdi ## 5 Ichiro Suzuki 0.275 0.246 0.306 0.95 mode hdi ## 6 Kyle Blanks 0.25 0.202 0.303 0.95 mode hdi ## 7 Kyle Blanks - Bruce Chen 0.117 0.059 0.179 0.95 mode hdi ## 8 Mike Leake 0.148 0.118 0.184 0.95 mode hdi ## 9 Mike Leake - Wandy Rodriguez 0.028 -0.015 0.069 0.95 mode hdi ## 10 ShinSoo Choo 0.275 0.244 0.304 0.95 mode hdi ## 11 ShinSoo Choo - Ichiro Suzuki 0 -0.042 0.043 0.95 mode hdi ## 12 Wandy Rodriguez 0.119 0.096 0.152 0.95 mode hdi Finally, we have only looked at a tiny fraction of the relations among the 968 parameters. We could investigate many more comparisons among parameters if we were specifically interested. In traditional statistical testing based on \\(p\\)-values (which will be discussed in Chapter 11), we would pay a penalty for even intending to make more comparisons. This is because a \\(p\\) value depends on the space of counter-factual possibilities created from the testing intentions. In a Bayesian analysis, however, decisions are based on the posterior distribution, which is based only on the data (and the prior), not on the testing intention. More discussion of multiple comparisons can be found in Section 11.4. (pp. 259–260) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] gridExtra_2.3 tidybayes_1.1.0 bayesplot_1.7.0 brms_2.10.3 ## [5] Rcpp_1.0.2 ggridges_0.5.1 forcats_0.4.0 stringr_1.4.0 ## [9] dplyr_0.8.3 purrr_0.3.2 readr_1.3.1 tidyr_1.0.0 ## [13] tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ellipsis_0.3.0 ## [3] rsconnect_0.8.15 ggstance_0.3.2 ## [5] markdown_1.1 base64enc_0.1-3 ## [7] rstudioapi_0.10 rstan_2.19.2 ## [9] svUnit_0.7-12 DT_0.9 ## [11] fansi_0.4.0 lubridate_1.7.4 ## [13] xml2_1.2.0 bridgesampling_0.7-2 ## [15] knitr_1.23 shinythemes_1.1.2 ## [17] zeallot_0.1.0 jsonlite_1.6 ## [19] broom_0.5.2 shiny_1.3.2 ## [21] compiler_3.6.0 httr_1.4.0 ## [23] backports_1.1.5 assertthat_0.2.1 ## [25] Matrix_1.2-17 lazyeval_0.2.2 ## [27] cli_1.1.0 later_1.0.0 ## [29] htmltools_0.4.0 prettyunits_1.0.2 ## [31] tools_3.6.0 igraph_1.2.4.1 ## [33] coda_0.19-3 gtable_0.3.0 ## [35] glue_1.3.1 reshape2_1.4.3 ## [37] cellranger_1.1.0 vctrs_0.2.0 ## [39] nlme_3.1-139 crosstalk_1.0.0 ## [41] xfun_0.10 ps_1.3.0 ## [43] rvest_0.3.4 mime_0.7 ## [45] miniUI_0.1.1.1 lifecycle_0.1.0 ## [47] gtools_3.8.1 zoo_1.8-6 ## [49] scales_1.0.0 colourpicker_1.0 ## [51] hms_0.4.2 promises_1.1.0 ## [53] Brobdingnag_1.2-6 parallel_3.6.0 ## [55] inline_0.3.15 shinystan_2.5.0 ## [57] yaml_2.2.0 loo_2.1.0 ## [59] StanHeaders_2.19.0 stringi_1.4.3 ## [61] dygraphs_1.1.1.6 pkgbuild_1.0.5 ## [63] rlang_0.4.0 pkgconfig_2.0.3 ## [65] matrixStats_0.55.0 HDInterval_0.2.0 ## [67] evaluate_0.14 lattice_0.20-38 ## [69] rstantools_2.0.0 htmlwidgets_1.5 ## [71] labeling_0.3 tidyselect_0.2.5 ## [73] processx_3.4.1 plyr_1.8.4 ## [75] magrittr_1.5 R6_2.4.0 ## [77] generics_0.0.2 pillar_1.4.2 ## [79] haven_2.1.0 withr_2.1.2 ## [81] xts_0.11-2 abind_1.4-5 ## [83] modelr_0.1.4 crayon_1.3.4 ## [85] arrayhelpers_1.0-20160527 utf8_1.1.4 ## [87] rmarkdown_1.13 grid_3.6.0 ## [89] readxl_1.3.1 callr_3.3.2 ## [91] threejs_0.3.1 digest_0.6.21 ## [93] xtable_1.8-4 httpuv_1.5.2 ## [95] stats4_3.6.0 munsell_0.5.0 ## [97] viridisLite_0.3.0 shinyjs_1.0 "],
["model-comparison-and-hierarchical-modeling.html", "10 Model Comparison and Hierarchical Modeling 10.1 General formula and the Bayes factor 10.2 Example: Two factories of coins 10.3 Solution by MCMC 10.4 Prediction: Model averaging 10.5 Model complexity naturally accounted for 10.6 Extreme sensitivity to the prior distribution 10.7 Bonus: There’s danger ahead Reference Session info", " 10 Model Comparison and Hierarchical Modeling There are situations in which different models compete to describe the same set of data… …Bayesian inference is reallocation of credibility over possibilities. In model comparison, the focal possibilities are the models, and Bayesian model comparison reallocates credibility across the models, given the data. In this chapter, we explore examples and methods of Bayesian inference about the relative credibilities of models. (pp. 265–266) In the text, the emphasis is on the Bayes Factor paradigm. While we will discuss that, we will also present the alternatives available with information criteria, model averaging, and model stacking. 10.1 General formula and the Bayes factor So far we have spoken of the data, denoted by \\(D\\) or \\(y\\); the model parameters, generically denoted by \\(\\theta\\); the likelihood function, denoted by \\(p(D | \\theta)\\); and the prior distribution, denoted by \\(p(\\theta)\\). Now we add to that \\(m\\), which is a model index with \\(m = 1\\) standing for the first model, \\(m = 2\\) standing for the second model, and so on. So when we have more than one model in play, we might refer to the likelihood as \\(p_m(y | \\theta_m, m)\\) and the prior as \\(p_m(\\theta_m | m)\\). It’s also the case, then, that each model can be given a prior probability \\(p(m)\\). “The Bayes factor (BF) is the ratio of the probabilities of the data in models 1 and 2” (p. 268). This can be expressed simply as \\[\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}.\\] Kruschke further explained that one convention for converting the magnitude of the BF to a discrete decision about the models is that there is “substantial” evidence for model \\(m = 1\\) when the BF exceeds 3.0 and, equivalently, “substantial” evidence for model \\(m = 2\\) when the BF is less than 1/3 (Jeffreys, 1961; Kass &amp; Raftery, 1995; Wetzels et al., 2011). However, as with \\(p\\)-values, effect sizes, and so on, BF values exist within continua and might should be evaluated in terms of degree more so than as ordered kinds. 10.2 Example: Two factories of coins Kruschke considered the coin bias of two factories, each described by the beta distribution. We can organize how to derive the \\(\\alpha\\) and \\(\\beta\\) parameters from \\(\\omega\\) and \\(\\kappa\\) with a tibble. library(tidyverse) d &lt;- tibble(factory = 1:2, omega = c(.25, .75), kappa = 12) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) d %&gt;% knitr::kable() factory omega kappa alpha beta 1 0.25 12 3.5 8.5 2 0.75 12 8.5 3.5 Thus given \\(\\omega_1 = .25\\), \\(\\omega_2 = .75\\) and \\(\\kappa = 12\\), we can describe the bias of the two coin factories as \\(\\text B_1 (3.5, 8.5)\\) and \\(\\text B_2 (8.5, 3.5)\\). With a little wrangling, we canuse our d tibble to make the densities of Figure 10.2. length &lt;- 101 d %&gt;% expand(nesting(factory, alpha, beta), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(label = str_c(&quot;factory &quot;, factory)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = dbeta(x = theta, shape1 = alpha, shape2 = beta))) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~label) We might recreate the top panel with geom_col(). tibble(Model = c(&quot;1&quot;, &quot;2&quot;), y = 1) %&gt;% ggplot(aes(x = Model, y = y)) + geom_col(width = .75, fill = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + ggtitle(expression(paste(italic(P)[italic(m)]))) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) Consider the Bernoulli bar plots in the bottom panels of Figure 10.2. The heights of the bars are arbitrary and just intended to give a sense of the Bernoulli distribution. If we wanted the heights to correspond to the Beta distributions above them, we might do so like this. tibble(factory = rep(str_c(&quot;factory &quot;, 1:2), each = 2), flip = rep(c(&quot;tails&quot;, &quot;heads&quot;), times = 2) %&gt;% factor(., levels = c(&quot;tails&quot;, &quot;heads&quot;)), prob = c(.75, .25, .25, .75)) %&gt;% ggplot(aes(x = flip, y = prob)) + geom_col(width = .75, fill = &quot;grey50&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(NULL) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) + facet_wrap(~factory) But now suppose we flip the coin nine times and get six heads. Given those data, what are the posterior probabilities of the coin coming from the head-biased or tail-biased factories? We will pursue the answer three ways: via formal analysis, grid approximation, and MCMC. (p. 270) 10.2.1 Solution by formal analysis. Here we rehearse if we have \\(\\operatorname{beta} (\\theta, a, b)\\) prior for \\(\\theta\\) of the Bernoulli likelihood function, then the analytic solution for the posterior is \\(\\operatorname{beta} (\\theta | z + a, N – z + b)\\). Within this paradigm, if you would like to compute \\(p(D | m)\\), don’t use the following function. If suffers from underflow with large values. p_d &lt;- function(z, n, a, b) { beta(z + a, n - z + b) / beta(a, b) } This version is more robust. p_d &lt;- function(z, n, a, b) { exp(lbeta(z + a, n - z + b) - lbeta(a, b)) } You’d use it like this to compute \\(p(D|m_1)\\). p_d(z = 6, n = 9, a = 3.5, b = 8.5) ## [1] 0.0004993439 So to compute our BF, \\(\\frac{p(D|m_1)}{p(D|m_2)}\\), you might use the p_d() function like this. p_d_1 &lt;- p_d(z = 6, n = 9, a = 3.5, b = 8.5) p_d_2 &lt;- p_d(z = 6, n = 9, a = 8.5, b = 3.5) p_d_1 / p_d_2 ## [1] 0.2135266 And if we computed the BF the other way, it’d look like this. p_d_2 / p_d_1 ## [1] 4.683258 Since the BF itself is only \\(\\text{BF} = \\frac{p(D | m = 1)}{p(D | m = 2)}\\), we’d need to bring in the priors for the models themselves to get the posterior probabilities, which follows the form \\[\\frac{p(m = 1 | D)}{p(m = 2 | D)} = \\Bigg (\\frac{p(D | m = 1)}{p(D | m = 2)} \\Bigg ) \\Bigg ( \\frac{p(m = 1)}{p(m = 2)} \\Bigg).\\] If for both our models \\(p(m) = .5\\), then the BF is (p_d_1 * .5) / (p_d_2 * .5) ## [1] 0.2135266 As Kruschke pointed out, because we’re working in the probability metric, the sum of \\(p(m = 1 | D )\\) and \\(p(m = 2 | D )\\) must be 1. By simple algebra then, \\[p(m = 2 | D ) = 1 - p(m = 1 | D ).\\] Therefore, it’s also the case that \\[\\frac{p(m = 1 | D)}{1 - p(m = 1 | D)} = 0.2135266.\\] Thus, 0.2135266 is in an odds metric. If you want to convert odds to a probability, you follow the formula \\[\\text{odds} = \\frac{\\text{probability}}{\\text{probability}}.\\] And with more algegraic manipulation, you can solve for the probability. \\[\\begin{align*} \\text{odds} &amp; = \\frac{\\text{probability}}{1 - \\text{probability}} \\\\ \\text{odds} - \\text{odds} \\cdot \\text{probability} &amp; = \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} + \\text{odds} \\cdot \\text{probability} \\\\ \\text{odds} &amp; = \\text{probability} (1 + \\text{odds}) \\\\ \\frac{\\text{odds}}{1 + \\text{odds}} &amp; = \\text{probability} \\end{align*}\\] Thus, the posterior probability for \\(m = 1\\) is \\[p(m = 1 | D) = \\frac{0.2135266}{1 + 0.2135266}.\\] We can express that in code like so. odds &lt;- (p_d_1 * .5) / (p_d_2 * .5) odds / (1 + odds) ## [1] 0.1759554 Relative to \\(m = 2\\), our posterior probability for \\(m = 1\\) is about .18. Therefore the posterior probability of \\(m = 2\\) is 1 minus that. 1 - (odds / (1 + odds)) ## [1] 0.8240446 Given the data, the two models and the prior assumption they were equally credible, we conclude \\(m = 2\\) is .82 probable. 10.2.2 Solution by grid approximation. We won’t be able to make the wireframe plots on the left of Figure 10.3, but we can do some of the others. Here’s the upper right panel. tibble(omega = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(m_p = ifelse(omega %in% c(.25, .75), 15, 0)) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = m_p)) + geom_ribbon(fill = &quot;grey67&quot;, color = &quot;grey67&quot;) + coord_flip(ylim = 0:25) + labs(subtitle = &quot;Remember, the scale on the x is arbitrary.&quot;, x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;)&quot;))) + theme(panel.grid = element_blank()) Building on that, here’s the upper middle panel of the “two [prior] dorsal fins” (p. 271). d &lt;- tibble(omega = seq(from = 0, to = 1, length.out = length)) %&gt;% expand(omega, theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(prior = ifelse(omega == .25, dbeta(theta, 3.5, 8.5), ifelse(omega == .75, dbeta(theta, 8.5, 3.5), 0))) d %&gt;% ggplot(aes(x = theta, y = omega, fill = prior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) This time we’ll separate \\(p_{m = 1}(\\theta)\\) and \\(p_{m = 2}(\\theta)\\) into the two short plots on the right of the next row down. p1 &lt;- d %&gt;% filter(omega == .75) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot; = .75)&quot;))) + theme(panel.grid = element_blank()) p2 &lt;- d %&gt;% filter(omega == .25) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = prior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|&quot;, omega, &quot; = .25)&quot;))) + theme(panel.grid = element_blank()) # we&#39;ll put them together with help from gridExtra library(gridExtra) grid.arrange(p1, p2) We can continue to build on those sensibilities for the middle panel of the same row. Here we’re literally adding \\(p_{m = 1}(\\theta)\\) to \\(p_{m = 2}(\\theta)\\) and taking their average. tibble(theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(d_75 = dbeta(x = theta, shape1 = 8.5, shape2 = 3.5), d_25 = dbeta(x = theta, shape1 = 3.5, shape2 = 8.5)) %&gt;% mutate(mean_prior = (d_75 + d_25) / 2) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = mean_prior)) + geom_ribbon(fill = &quot;grey67&quot;) + coord_cartesian(ylim = 0:3) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) We need the Bernoulli likelihood function for the next step. bernoulli_likelihood &lt;- function(theta, data) { n &lt;- length(data) z &lt;- sum(data) return(theta^z * (1 - theta)^(n - sum(data))) } Time to feed our data and the parameter space into bernoulli_likelihood(), which will allow us to make the 2-dimensional density plot at the heart of Figure 10.3. n &lt;- 9 z &lt;- 6 trial_data &lt;- rep(0:1, times = c(n - z, z)) d &lt;- d %&gt;% mutate(likelihood = bernoulli_likelihood(theta = theta, data = trial_data)) d %&gt;% ggplot(aes(x = theta, y = omega, fill = likelihood)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Now we just need the marginal likelihood, \\(p(D)\\), to compute the posterior. Our first depiction will be the middle panel of the second row from the bottom–the panel with the uneven dolphin fins. d &lt;- d %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) d %&gt;% ggplot(aes(x = theta, y = omega, fill = posterior)) + geom_raster(interpolate = T) + scale_fill_viridis_c(option = &quot;A&quot;) + labs(x = expression(theta), y = expression(omega)) + coord_equal() + theme(panel.grid = element_blank(), legend.position = &quot;none&quot;) Here, then, is a way to get the panel in on the right of the second row from the bottom. d %&gt;% mutate(marginal = (posterior / max(posterior)) * 25) %&gt;% ggplot(aes(x = omega, ymin = 0, ymax = marginal)) + geom_ribbon(fill = &quot;grey67&quot;, color = &quot;grey67&quot;) + coord_flip(ylim = 0:25) + labs(subtitle = &quot;Remember, the scale on the x is arbitrary.&quot;, x = expression(omega), y = expression(paste(&quot;Marginal p(&quot;, omega, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) To make the middle bottom panel of Figure 10.3, we have to average the posterior values of \\(\\theta\\) over the grid of \\(\\omega\\) values. That is, we have to marginalize. d %&gt;% group_by(theta) %&gt;% summarise(marginal_theta = mean(posterior)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = marginal_theta)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) For the lower right panel of Figure 10.3, we’ll filter() to our two focal values of \\(\\omega\\) and then facet by them. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = str_c(&quot;omega == &quot;, omega)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~omega, ncol = 1, scales = &quot;free&quot;, labeller = label_parsed) Do note the different scales on the \\(y\\). Here’s what they’d look like on the same scale. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% mutate(omega = str_c(&quot;omega == &quot;, omega)) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = posterior)) + geom_ribbon(fill = &quot;grey67&quot;) + labs(x = expression(theta), y = expression(paste(&quot;Marginal p(&quot;, theta, &quot;|&quot;, omega, &quot;)&quot;))) + theme(panel.grid = element_blank()) + facet_wrap(~omega, ncol = 1, labeller = label_parsed) Hopefully that helps build the intuition of what Kruschke meant when he wrote “visual inspection suggests that the ratio of the heights is about 5 to 1, which matches the Bayes factor of 4.68 that we computed exactly in the previous section” (p. 273, emphasis in the original). Using the grid, you might compute that BF like this. d %&gt;% filter(omega %in% c(.25, .75)) %&gt;% group_by(omega) %&gt;% summarise(sum_posterior = sum(posterior)) %&gt;% mutate(model = c(&quot;model_2&quot;, &quot;model_1&quot;)) %&gt;% select(-omega) %&gt;% spread(key = model, value = sum_posterior) %&gt;% summarise(BF = model_1 / model_2) ## # A tibble: 1 x 1 ## BF ## &lt;dbl&gt; ## 1 4.68 10.3 Solution by MCMC Kruschke started with: “For large, complex models, we cannot derive \\(p(D | m)\\) analytically or with grid approximation, and therefore we will approximate the posterior probabilities using MCMC methods” (p. 274). He’s not kidding. Welcome to modern Bayes. 10.3.1 Nonhierarchical MCMC computation of each model’s marginal likelihood. Before you get excited, Kruschke warned: “For complex models, this method might not be tractable. [But] for the simple application here, however, the method works well, as demonstrated in the next section” (p. 277). 10.3.1.1 Implementation with JAGS brms. Load brms. library(brms) Let’s save the trial_data as a tibble. trial_data &lt;- tibble(y = trial_data) Time to learn a new brms skill. When you want to enter variables into the parameters defining priors in brms::brm(), you need to specify them using the stanvar() function. Since we want to do this for two variables, we’ll use stanvar() twice and save the results as an object, conveniently named stanvars. omega &lt;- .75 kappa &lt;- 12 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Now we have our stanvars object, we are ready to fit the first model (i.e., the model for which \\(\\omega = .75\\)). fit1 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(my_alpha, my_beta), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, control = list(adapt_delta = .999)) Note how we fed our stanvars object into the stanvars function. Anyway, let’s inspect the chains. plot(fit1) They look great. Now we glance at the model summary. print(fit1) ## Family: bernoulli ## Links: mu = identity ## Formula: y ~ 1 ## Data: trial_data (Number of observations: 9) ## Samples: 4 chains, each with iter = 11000; warmup = 1000; thin = 1; ## total post-warmup samples = 40000 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 0.69 0.10 0.48 0.86 1.00 8007 9306 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). Next we’ll follow Kruschke and extract the posterior samples, saving them as theta. theta &lt;- posterior_samples(fit1) head(theta) ## b_Intercept lp__ ## 1 0.7263424 -4.691665 ## 2 0.7626307 -4.815941 ## 3 0.7222605 -4.686314 ## 4 0.8012281 -5.125448 ## 5 0.7373272 -4.714354 ## 6 0.6857306 -4.707360 The fixef() function will return the posterior summaries for the model intercept (i.e., \\(\\theta\\)). We can then index and save the desired summaries. fixef(fit1) ## Estimate Est.Error Q2.5 Q97.5 ## Intercept 0.691113 0.098457 0.4820692 0.863507 (mean_theta &lt;- fixef(fit1)[1]) ## [1] 0.691113 (sd_theta &lt;- fixef(fit1)[2]) ## [1] 0.098457 Now we’ll convert them to the \\(\\alpha\\) and \\(\\beta\\) parameters, a_post and b_post, respectively. a_post &lt;- mean_theta * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) Recall we’ve already defined several values. n &lt;- 9 z &lt;- 6 omega &lt;- .75 kappa &lt;- 12 The reason we’re saving all these values is we’re aiming to compute \\(p(D)\\), the probability of the data (i.e., the marginal likelihood), given the model. But our intermediary step will be computing its reciprocal, \\(\\frac{1}{p(D)}\\). Here we’ll express Kruschke’s oneOverPD as a function, one_over_pd(). one_over_pd &lt;- function(theta) { mean(dbeta(theta, a_post, b_post ) / (theta^z * (1 - theta)^(n - z) * dbeta(theta, omega * (kappa - 2) + 1, (1 - omega) * (kappa - 2) + 1 ))) } We’re ready to use one_over_pd() to help compute \\(p(D)\\). theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## pd ## 1 0.002338466 That matches up nicely with Kruschke’s value! Let’s rinse, wash, and repeat for \\(\\omega = .25\\). First, we’ll need to redefine omega and our stanvars. omega &lt;- .25 stanvars &lt;- stanvar( omega * (kappa - 2) + 1, name = &quot;my_alpha&quot;) + stanvar((1 - omega) * (kappa - 2) + 1, name = &quot;my_beta&quot;) Fit the model. fit2 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(my_alpha, my_beta), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10, stanvars = stanvars, control = list(adapt_delta = .999)) We’ll do the rest in bulk. theta &lt;- posterior_samples(fit2) mean_theta &lt;- fixef(fit2)[1] sd_theta &lt;- fixef(fit2)[2] a_post &lt;- mean_theta * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) b_post &lt;- (1 - mean_theta) * ( mean_theta * (1 - mean_theta) / sd_theta^2 - 1) theta %&gt;% summarise(pd = 1 / one_over_pd(theta = b_Intercept)) ## pd ## 1 0.0004992476 Boom! 10.3.2 Hierarchical MCMC computation of relative model probability is not available in brms: We’ll cover information criteria instead. I’m not aware of a way to specify a model “in which the top-level parameter is the index across models” in brms (p. 278). If you know of a way, share your code. However, we do have options. We can compare and weight models using information criteria, about which you can learn more here. In brms, the LOO and WAIC are two primary information criteria available. You can compute them for a given model with the loo() and waic() functions, respectively. Here’s a quick example of how to use the waic() function. waic(fit1) ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_waic -6.2 1.3 ## p_waic 0.5 0.1 ## waic 12.5 2.7 We’ll explain that output in a bit. Before we do, you should know the current recommended workflow for information criteria with brms models is to use the add_criterion() function, which will allow us to compute information-criterion-related output and save it to our brms fit objects. Here’s how to do that with both our fits. fit1 &lt;- add_criterion(fit1, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit2 &lt;- add_criterion(fit2, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) You can extract the same WAIC output for fit1 we saw above by executing fit1$waic. Here we look at the LOO summary for fit1, instead. fit1$loo ## ## Computed from 40000 by 9 log-likelihood matrix ## ## Estimate SE ## elpd_loo -6.2 1.3 ## p_loo 0.5 0.1 ## looic 12.5 2.7 ## ------ ## Monte Carlo SE of elpd_loo is 0.0. ## ## All Pareto k estimates are good (k &lt; 0.5). ## See help(&#39;pareto-k-diagnostic&#39;) for details. You get a wealth of output, more of which can be seen by executing str(fit1$loo). First, notice the message “All Pareto k estimates are good (k &lt; 0.5).” Pareto \\(k\\) values can be used for diagnostics. Each case in the data gets its own \\(k\\) value and we like it when those \\(k\\)s are low. The makers of the loo package get worried when \\(k\\) values exceed 0.7 and, as a result, we will get warning messages when they do. Happily, we have no such warning messages in this example. In the main section, we get estimates for the expected log predictive density (elpd_loo), the estimated effective number of parameters (p_loo), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; looic). Each estimate comes with a standard error (i.e., SE). Like other information criteria, the LOO values aren’t of interest in and of themselves. However, the estimate of one model’s LOO relative to that of another is of great interest. We generally prefer models with lower information criteria. With the loo_compare() function, we can compute a formal difference score between two models. loo_compare(fit1, fit2, criterion = &quot;loo&quot;) ## elpd_diff se_diff ## fit1 0.0 0.0 ## fit2 -0.8 1.7 The loo_compare() output rank orders the models such that the best fitting model appears on top. All models receive a difference score relative to the best model. Here the best fitting model is fit1 and since the LOO for fit1 minus itself is zero, the values in the top row are all zero. Each difference score also comes with a standard error. In this case, even though fit1 has the lower estimates, the standard error is twice the magnitude of the difference score. So the LOO difference score puts the two models on similar footing. You can do a similar analysis with the WAIC estimates. In addition to difference-score comparisons, you can also use the LOO or WAIC for AIC-type model weighting. In brms, you do this with the model_weights() function. (mw &lt;- model_weights(fit1, fit2)) ## fit1 fit2 ## 0.830191 0.169809 I don’t know that I’d call these weights probabilities, but they do sum to one. In this case, the analysis suggests we put about five times more weight to fit1 relative to fit2. mw[1] / mw[2] ## fit1 ## 4.888969 With brms::model_weights(), we have a variety of weighting schemes avaliable to us. Since we didn’t specify any in the weights argument, we used the default &quot;loo2&quot;, which is–perhaps confusingly given the name–the stacking method according to the paper by Yao, Vehtari, Simpson, and Gelman. Vehtari has written about the paper on Gelman’s blog, too. But anyway, the point is that different weighting schemes might not produce the same results. For example, here’s the result from weighting using the WAIC. model_weights(fit1, fit2, weights = &quot;waic&quot;) ## fit1 fit2 ## 0.6967995 0.3032005 The results are similar, for sure. But they’re not the same. The stacking method via the brms default weights = &quot;loo2&quot; is the current preferred method by the folks on the Stan team (e.g., the authors of the above linked paper). For more on stacking and other weighting schemes, see Vehtari and Gabry’s vignette Bayesian Stacking and Pseudo-BMA weights using the loo package or Vehtari’s modelselection_tutorial GitHub repository. But don’t worry. We will have more opportunities to practice with information criteria, model weights, and such later in this project. 10.3.2.1 Using [No need to use] pseudo-priors to reduce autocorrelation. Since we didn’t use Kruschke’s method from the last subsection, we don’t have the same worry about autocorrelation. For example, here are the autocorrelation plots for fit1. library(bayesplot) mcmc_acf(posterior_samples(fit1, add_chain = T), pars = &quot;b_Intercept&quot;, lags = 35) Our autocorrelations were a little high for HMC, but nowhere near pathological. The results for fit2 were similar. As you might imagine from the moderate autocorrelations, the \\(N_{eff}/N\\) ratio for b_Intercept wasn’t great. neff_ratio(fit1)[1] %&gt;% mcmc_neff() + yaxis_text(hjust = 0) But we specified a lot of post-warmup iterations, so we’re still in good shape. Plus, the \\(\\hat{R}\\) was fine. rhat(fit1)[1] ## b_Intercept ## 1.000613 10.3.3 Models with different “noise” distributions in JAGS brms. Probability distribution[s are] sometimes [called “noise”] distribution[s] because [they describe] the random variability of the data values around the underlying trend. In more general applications, different models can have different noise distributions. For example, one model might describe the data as log-normal distributed, while another model might describe the data as gamma distributed. (p. 288) If there are more than one plausible noise distributions for our data, we might want to compare the models. Kruschke then gave us a general trick in the form of this JAGS code: data { C &lt;- 10000 # JAGS does not warn if too small! for (i in 1:N) { ones[i] &lt;- 1 } } model { for (i in 1:N) { spy1[i] &lt;- pdf1(y[i], parameters1) / C # where pdf1 is a formula spy2[i] &lt;- pdf2(y[i], parameters2) / C # where pdf2 is a formula spy[i] &lt;- equals(m,1) * spy1[i] + equals(m, 2) * spy2[i] ones[i] ~ dbern(spy[i]) } parameters1 ~ dprior1... parameters2 ~ dprior2... m ~ dcat(mPriorProb[]) mPriorProb[1] &lt;- .5 mPriorProb[2] &lt;- .5 } I’m not aware that we can do this within the Stan/brms framework. If I’m in error and you know how, please share your code. However, we do have options. In anticipation of Chapter 16, let’s consider Gaussian-like data with thick tails. We might generate some like this: # how many draws would you like? n &lt;- 1e3 set.seed(10) (d &lt;- tibble(y = rt(n, df = 7))) ## # A tibble: 1,000 x 1 ## y ## &lt;dbl&gt; ## 1 0.0214 ## 2 -0.987 ## 3 0.646 ## 4 -0.237 ## 5 0.977 ## 6 -0.200 ## 7 0.781 ## 8 -1.09 ## 9 1.83 ## 10 -0.682 ## # … with 990 more rows The resulting data look like this. d %&gt;% ggplot(aes(x = y)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + scale_y_continuous(NULL, breaks = NULL) + theme(panel.grid = element_blank()) As you’d expect with a small-\\(\\nu\\) Student’s \\(t\\), some of our values are quite distinct from the central clump. If you don’t recall, Student’s \\(t\\)-distribution has three parameters: \\(\\nu\\), \\(\\mu\\), and \\(\\sigma\\). The Gaussian is a special case of Student’s \\(t\\) for which \\(\\nu = \\infty\\). When \\(\\nu\\) gets small, the consequence is the distribution allocates more mass in the tails. From a Gaussian perspective, the small-\\(\\nu\\) Student’s \\(t\\) expects more outliers–though it’s a little odd calling them outliers from a small-\\(\\nu\\) Student’s \\(t\\) perspective. Let’s see how well the Gaussian versus the Student’s \\(t\\) likelihoods handle the data. Here we’ll use fairly liberal priors. fit3 &lt;- brm(data = d, family = gaussian, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = sigma)), # by default, this has a lower bound of 0 chains = 4, cores = 4, seed = 10) fit4 &lt;- brm(data = d, family = student, y ~ 1, prior = c(prior(normal(0, 5), class = Intercept), prior(normal(0, 5), class = sigma), prior(gamma(2, 0.1), class = nu)), # this is the brms default prior for nu chains = 4, cores = 4, seed = 10) In case you were curious, here’s what that default gamma(2, 0.1) prior on nu looks like. tibble(x = seq(from = 0, to = 110, by = 1)) %&gt;% ggplot(aes(x = x, ymin = 0, ymax = dgamma(x, 2, 0.1))) + geom_ribbon(size = 0, fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + xlab(expression(paste(italic(p), (nu)))) + coord_cartesian(xlim = 0:100) + theme(panel.grid = element_blank()) That prior puts most of the probability mass below 50, but the right tail gently fades off into the triple digits, allowing for the possibility of larger estimates. We can use the posterior_summary() function to get a compact look at the model summaries. posterior_summary(fit3) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.03 0.04 -0.11 0.05 ## sigma 1.25 0.03 1.20 1.31 ## lp__ -1646.97 0.98 -1649.49 -1646.02 posterior_summary(fit4) %&gt;% round(digits = 2) ## Estimate Est.Error Q2.5 Q97.5 ## b_Intercept -0.01 0.04 -0.08 0.06 ## sigma 0.98 0.04 0.90 1.05 ## nu 5.76 1.02 4.12 8.06 ## lp__ -1590.50 1.26 -1593.76 -1589.07 Now we can compare the two approaches using information criteria. For kicks, we’ll use the WAIC. fit3 &lt;- add_criterion(fit3, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) fit4 &lt;- add_criterion(fit4, criterion = c(&quot;loo&quot;, &quot;waic&quot;)) loo_compare(fit3, fit4, criterion = &quot;waic&quot;) ## elpd_diff se_diff ## fit4 0.0 0.0 ## fit3 -60.3 40.1 Based on the WAIC difference, we hace some support for preferring the Student’s \\(t\\), but do notice how wide that SE was. We can also compare the models using model weights. Here we’ll use the default weighting scheme. model_weights(fit3, fit4) ## fit3 fit4 ## 0.03221235 0.96778765 Virtually all of the stacking weight was placed on the Student’s-\\(t\\) model, fit4. Remember what that \\(p(\\nu)\\) looked like? Here’s our posterior distribution for \\(\\nu\\). posterior_samples(fit4) %&gt;% ggplot(aes(x = nu)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:20) + labs(subtitle = expression(paste(&quot;Recall that for the Gaussian, &quot;, nu, &quot; = infinity.&quot;)), x = expression(paste(italic(p), &quot;(&quot;, nu, &quot;|&quot;, italic(D), &quot;)&quot;))) + theme(panel.grid = element_blank()) Even though our prior for \\(\\nu\\) was relatively weak, the posterior ended up concentrated on values in the middle-single-digit range. Recall the data-generating value was 7. We can also compare the models using posterior-predictive checks. There are a variety of ways we might do this, but the most convenient way is with brms::pp_check(), which is itself a wrapper for the family of ppc functions from the bayesplot package. pp_check(fit3) pp_check(fit4) The default pp_check() setting allows us to compare the density of the data \\(y\\) (i.e., the dark blue) with 10 density’s simulated from the posterior \\(y_\\text{rep}\\) (i.e., the light blue). We prefer model that produce \\(y_\\text{rep}\\) distributions that resemble \\(y\\). Though the results from both models were similar, the simulated distributions from fit4 mimicked the original data a little more convincingly. To learn more about this approach, check out Gabry’s vignette Graphical posterior predictive checks using the bayesplot package. 10.4 Prediction: Model averaging In many applications of model comparison, the analyst wants to identify the best model and then base predictions of future data on that single best model, denoted with index \\(b\\). In this case, predictions of future \\(\\hat{y}\\) are based exclusively on the likelihood function \\(p_b(\\hat{y} | \\theta_b, m = b)\\) and the posterior distribution \\(p_b(\\theta_b | D, m = b)\\) of the winning model: \\[p_b(\\hat y | D, m = b) = \\int \\text d \\theta_b p_b (\\hat{y} | \\theta_b, m = b) p_b(\\theta_b | D, m = b)\\] But the full model of the data is actually the complete hierarchical structure that spans all the models being compared, as indicated in Figure 10.1 (p. 267). Therefore, if the hierarchical structure really expresses our prior beliefs, then the most complete prediction of future data takes into account all the models, weighted by their posterior credibilities. In other words, we take a weighted average across the models, with the weights being the posterior probabilities of the models. Instead of conditionalizing on the winning model, we have \\[\\begin{align*} p (\\hat y | D) &amp; = \\sum_m p (\\hat y | D, m) p (m | D) \\\\ &amp; = \\sum_m \\int \\text d \\theta_m p_m (\\hat{y} | \\theta_m, m) p_m(\\theta_m | D, m) p (m | D) \\end{align*}\\] This is called model averaging. (p. 289) Okay, while the concept of model averaging is of great interest, we aren’t going to be able to follow this approach to it within the Stan/brms paradigm. This, recall, is because our paradigm doesn’t allow for a hierarchical organization of models in the same way JAGS does. However, we can still play the model averaging game with extensions of our model weighting paradigm, above. Before we get into the details, recall that there were two models of mints that created the coin, with one mint being tail-biased with mode \\(\\omega = 0.25\\) and one mint being head-biased with mode \\(\\omega = 0.75\\) The two subpanels in the lower-right illustrate the posterior distributions on \\(\\omega\\) within each model, \\(p(\\theta | D, \\omega = 0.25)\\) and \\(p(\\theta | D, \\omega = 0.75)\\) The winning model was \\(\\omega = 0.75\\), and therefore the predicted value of future data, based on the winning model alone, would use \\(p(\\theta | D, \\omega = 0.75)\\). (p. 289) That is, the posterior for fit1. library(tidybayes) posterior_samples(fit1) %&gt;% ggplot(aes(x = b_Intercept)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given fit1&quot;, x = expression(paste(italic(p), &quot;(&quot;, theta, &quot;|&quot;, italic(D), &quot;, &quot;, omega, &quot; = .75)&quot;))) + coord_cartesian(xlim = 0:1) + theme(panel.grid = element_blank()) But the overall model included \\(\\omega = 0.75\\), and if we use the overall model, then the predicted value of future data should be based on the complete posterior summed across values of \\(\\omega\\). The complete posterior distribution [is] \\(p(\\theta | D)\\) (p. 289). The cool thing about the model weighting stuff we learned about earlier is that you can use those model weights to average across models. Again, we’re not weighting the models by posterior probabilities the way Kruschke discussed in text. However, the spirit is similar. We can use the brms::pp_average() function to make posterior predictive prediction with mixtures of the models, weighted by our chosen weighting scheme. Here, we’ll go with the default stacking weights. nd &lt;- tibble(y = 1) pp_a &lt;- pp_average(fit1, fit2, newdata = nd, # this line is not necessary, but you should see how to choose weighing methods weights = &quot;loo2&quot;, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% set_names(&quot;theta&quot;) # what does this produce? head(pp_a) ## # A tibble: 6 x 1 ## theta ## &lt;dbl&gt; ## 1 0.717 ## 2 0.755 ## 3 0.908 ## 4 0.689 ## 5 0.666 ## 6 0.765 We can plot our model-averaged \\(\\theta\\) with a little help from good old tidybayes::stat_pointintervalh(). pp_a %&gt;% ggplot(aes(x = theta)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given the\\nweighted combination of fit1 and fit2&quot;, x = expression(paste(italic(p), &quot;(&quot;, theta, &quot;|&quot;, italic(D), &quot;)&quot;))) + coord_cartesian(xlim = 0:1) + theme(panel.grid = element_blank()) As Kruschke concluded, “you can see the contribution of \\(p(\\theta | D, \\omega = 0.25)\\) as the extended leftward tail” (p. 289). Interestingly enough, that looks a lot like the density we made with grid approximation in Figure 10.3, doesn’t it? 10.5 Model complexity naturally accounted for A complex model (usually) has an inherent advantage over a simpler model because the complex model can find some combination of its parameter values that match the data better than the simpler model. There are so many more parameter options in the complex model that one of those options is likely to fit the data better than any of the fewer options in the simpler model. The problem is that data are contaminated by random noise, and we do not want to always choose the more complex model merely because it can better fit noise. Without some way of accounting for model complexity, the presence of noise in data will tend to favor the complex model. Bayesian model comparison compensates for model complexity by the fact that each model must have a prior distribution over its parameters, and more complex models must dilute their prior distributions over larger parameter spaces than simpler models. Thus, even if a complex model has some particular combination of parameter values that fit the data well, the prior probability of that particular combination must be small because the prior is spread thinly over the broad parameter space. (pp. 289–290) Now our two models are: \\(p(\\theta | D, \\kappa = 2000)\\) (i.e., the “must-be-fair” model) and \\(p(\\theta | D, \\kappa = 2)\\) (i.e., the “anything’s-possible” model). They look like this. # how granular to you want the theta sequence? n &lt;- 1e3 # simulate the data tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% expand(nesting(omega, kappa, model), theta = seq(from = 0, to = 1, length.out = n)) %&gt;% mutate(density = dbeta(theta, shape1 = omega * (kappa - 2) + 1, shape2 = (1 - omega) * (kappa - 2) + 1)) %&gt;% # plot ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;Note that in this case, their y-axes are on the same scale.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~model) Here’s how you might compute the \\(\\alpha\\) and \\(\\beta\\) values for the corresponding Beta distributions. tibble(omega = .5, kappa = c(1000, 2), model = c(&quot;The must-be-fair model&quot;, &quot;The anything&#39;s-possible model&quot;)) %&gt;% mutate(alpha = omega * (kappa - 2) + 1, beta = (1 - omega) * (kappa - 2) + 1) ## # A tibble: 2 x 5 ## omega kappa model alpha beta ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.5 1000 The must-be-fair model 500 500 ## 2 0.5 2 The anything&#39;s-possible model 1 1 With those in hand, we can use our p_d() function to compute the Bayes factor based on flipping a coin \\(N = 20\\) times and observing \\(z = 15\\) heads. # the data summaries z &lt;- 15 n &lt;- 20 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.3229023 Let’s try again, this time supposing we observe \\(z = 15\\) heads out of \\(N = 20\\) coin flips. z &lt;- 11 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 3.337148 The anything’s-possible model loses because it pays the price of having a small prior probability on the values of \\(\\theta\\) near the data proportion, while the must-be-fair model has large prior probability on \\(\\theta\\) values sufficiently near the data proportion to be credible. Thus, in Bayesian model comparison, a simpler model can win if the data are consistent with it, even if the complex model fits just as well. The complex model pays the price of having small prior probability on parameter values that describe simple data. (p. 291) 10.5.1 Caveats regarding nested model comparison. A frequently encountered special case of comparing models of different complexity occurs when one model is “nested” within the other. Consider a model that implements all the meaningful parameters we can contemplate for the particular application. We call that the full model. We might consider various restrictions of those parameters, such as setting some of them to zero, or forcing some to be equal to each other. A model with such a restriction is said to be nested within the full model. (p. 291) Kruschke didn’t walk out the examples in this section. But for the sake of practice, let’s work through the first one. “Recall the hierarchical model of baseball batting abilities” from Chapter 9 (p. 291). Let’s reload those data. my_data &lt;- read_csv(&quot;data.R/BattingAverage.csv&quot;) glimpse(my_data) ## Observations: 948 ## Variables: 6 ## $ Player &lt;chr&gt; &quot;Fernando Abad&quot;, &quot;Bobby Abreu&quot;, &quot;Tony Abreu&quot;, &quot;Dust… ## $ PriPos &lt;chr&gt; &quot;Pitcher&quot;, &quot;Left Field&quot;, &quot;2nd Base&quot;, &quot;2nd Base&quot;, &quot;1… ## $ Hits &lt;dbl&gt; 1, 53, 18, 137, 21, 0, 0, 2, 150, 167, 0, 128, 66, … ## $ AtBats &lt;dbl&gt; 7, 219, 70, 607, 86, 1, 1, 20, 549, 576, 1, 525, 27… ## $ PlayerNumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, … ## $ PriPosNumber &lt;dbl&gt; 1, 7, 4, 4, 3, 1, 1, 3, 3, 4, 1, 5, 4, 2, 7, 4, 6, … “The full model has a distinct modal batting ability, \\(\\omega_c\\) , for each of the nine fielding positions. The full model also has distinct concentration parameters for each of the nine positions” (p. 291). Let’s fit that model again. fit5 &lt;- brm(data = my_data, family = binomial(link = logit), Hits | trials(AtBats) ~ 1 + (1 | PriPos) + (1 | PriPos:Player), prior = c(prior(normal(0, 1.5), class = Intercept), prior(normal(0, 1), class = sd)), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 10) Next we’ll consider a restricted version of fit5 “in which all infielders (first base, second base, etc.) are grouped together versus all outfielders (right field, center field, and left field). In this restricted model, we are forcing the modal batting abilities of all the outfielders to be the same, that is, \\(\\omega_\\text{left field} = \\omega_\\text{center field} = \\omega_\\text{right field}\\)” (p. 291). To fit that model, we’ll need to make a new variable PriPos_small which is identical to its parent variable PriPos except that it collapses those three positions into our new category Outfield. my_data &lt;- my_data %&gt;% mutate(PriPos_small = if_else(PriPos %in% c(&quot;Center Field&quot;, &quot;Left Field&quot;, &quot;Right Field&quot;), &quot;Outfield&quot;, PriPos)) Now use update() to fit the restricted model. fit6 &lt;- update(fit5, newdata = my_data, formula = Hits | trials(AtBats) ~ 1 + (1 | PriPos_small) + (1 | PriPos_small:Player), iter = 3500, warmup = 500, chains = 3, cores = 3, control = list(adapt_delta = .99), seed = 10) Unlike with what Kruschke alluded to in the prose, here we’ll compare the two models with the LOO information criteria. fit5 &lt;- add_criterion(fit5, criterion = &quot;loo&quot;) fit6 &lt;- add_criterion(fit6, criterion = &quot;loo&quot;) loo_compare(fit5, fit6) ## elpd_diff se_diff ## fit5 0.0 0.0 ## fit6 -2.2 2.1 Based on the LOO difference score, they’re near equivalent. Now let’s see how their model weights shake out. Here we’ll continue to use the default stacking method. model_weights(fit5, fit6) ## fit5 fit6 ## 0.997251197 0.002748803 Though most of the weight went to the parsimonious fit6, we should be skeptical. “Does that mean we should believe that [these positions] have literally identical batting abilities? Probably not” (p. 292). It’s good to be cautious of unnecessary model expansion. But we should also use good substantive reasoning, too. Just because you can restrict a model, that doesn’t necessarily mean it leads to better science. 10.6 Extreme sensitivity to the prior distribution In many realistic applications of Bayesian model comparison, the theoretical emphasis is on the difference between the models’ likelihood functions. For example, one theory predicts planetary motions based on elliptical orbits around the sun, and another theory predicts planetary motions based on circular cycles and epicycles around the earth. The two models involve very different parameters. In these sorts of models, the form of the prior distribution on the parameters is not a focus, and is often an afterthought. But, when doing Bayesian model comparison, the form of the prior is crucial because the Bayes factor integrates the likelihood function weighted by the prior distribution. (p. 292) However, “the sensitivity of Bayes factors to prior distributions is well known in the literature (e.g., Kass &amp; Raftery, 1995; Liu &amp; Aitkin, 2008; Vanpaemel, 2010),” and furthermore, when comparing Bayesian models using the methods Kruschke outlined in this chapter of the text, “different forms of vague priors can yield very different Bayes factors” (p. 293). In the two BFs to follow, we compare the must-be-fair model and the anything’s-possible models from 10.5 to new data: \\(z = 65, N = 100\\). z &lt;- 65 n &lt;- 100 p_d(z, n, a = 500, b = 500) / p_d(z, n, a = 1, b = 1) ## [1] 0.125287 The resulting 0.13 favored the anything’s-possible model. Another way to express the anything’s-possible model is with the Haldane prior, which sets the two parameters within the beta distribution to be a) equivalent and b) quite small (i.e., 0.01 in this case). p_d(z, n, a = 500, b = 500) / p_d(z, n, a = .01, b = .01) ## [1] 5.728066 Now we flipped to favoring the must-be-fair model. You might be asking, Wait, kind of distribution did that Haldane prior produce? Here we compare it to the Beta(1, 1). # how granular to you want the theta sequence? length &lt;- 1e3 # simulate the data tibble(alpha = c(1, .01), beta = c(1, .01), model = factor(c(&quot;Uninformative prior, Beta(1, 1)&quot;, &quot;Haldane prior, Beta(0.01, 0.01)&quot;), levels = c(&quot;Uninformative prior, Beta(1, 1)&quot;, &quot;Haldane prior, Beta(0.01, 0.01)&quot;))) %&gt;% expand(nesting(alpha, beta, model), theta = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(density = dbeta(theta, shape1 = alpha, shape2 = beta)) %&gt;% # plot ggplot(aes(x = theta, ymin = 0, ymax = density)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_y_continuous(NULL, breaks = NULL) + labs(title = &quot;We have two anything’s-possible models.&quot;, subtitle = &quot;These y-axes are on the same scale.&quot;, x = expression(theta)) + theme(panel.grid = element_blank()) + facet_wrap(~model) Before we can complete the analyses of this subsection, we’ll need to define our version of Kruschke’s HDIofICDF function(), hdi_of_icdf(). Like we’ve done in previous chapters, here we mildly reformat the function. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } And here we’ll make a custom variant to be more useful within the context of map2(). hdi_of_qbeta &lt;- function(shape1, shape2) { hdi_of_icdf(name = qbeta, shape1 = shape1, shape2 = shape2) %&gt;% data.frame() %&gt;% mutate(level = c(&quot;ll&quot;, &quot;ul&quot;)) %&gt;% spread(key = level, value = &quot;.&quot;) } Recall that when we combine a \\(\\text{Beta} (\\theta | \\alpha, \\beta)\\) prior with the results of a Bernoulli likelihood, we get a posterior defined by \\(\\text{Beta} (\\theta | z + \\alpha, N - z + \\beta)\\). d &lt;- tibble(model = c(&quot;Uniform&quot;, &quot;Haldane&quot;), prior_a = c(1, .01), prior_b = c(1, .01)) %&gt;% mutate(posterior_a = z + prior_a, posterior_b = n - z + prior_b) d ## # A tibble: 2 x 5 ## model prior_a prior_b posterior_a posterior_b ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 ## 2 Haldane 0.01 0.01 65.0 35.0 Now we’ll use our custom hdi_of_qbeta() to compute the HDIs. ( d &lt;- d %&gt;% mutate(levels = map2(posterior_a, posterior_b, hdi_of_qbeta)) %&gt;% unnest(levels) ) ## # A tibble: 2 x 7 ## model prior_a prior_b posterior_a posterior_b ll ul ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Uniform 1 1 66 36 0.554 0.738 ## 2 Haldane 0.01 0.01 65.0 35.0 0.556 0.742 Let’s compare those HDIs in a plot. d %&gt;% ggplot(aes(x = ll, xend = ul, y = model, yend = model)) + geom_segment(size = .75) + coord_cartesian(xlim = 0:1) + labs(subtitle = &quot;Those two sets of HDIs are quite similar.\\nIt almost seems silly their respective BFs\\nare so different.&quot;, x = expression(theta), y = NULL) + theme(panel.grid = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_text(hjust = 0)) “The HDIs are virtually identical. In particular, for either prior, the posterior distribution rules out \\(\\theta = 0.5\\), which is to say that the must-be-fair hypothesis is not among the credible values” (p. 294). 10.6.1 Priors of different models should be equally informed. “We have established that seemingly innocuous changes in the vagueness of a vague prior can dramatically change a model’s marginal likelihood, and hence its Bayes factor in comparison with other models. What can be done to ameliorate the problem” (p. 294)? Kruschke posed one method might be taking a small representative portion of the data in hand and use them to make an empirically-based prior for the remaining set of data. From our previous example, “suppose that the 10% subset has 6 heads in 10 flips, so the remaining 90% of the data has \\(z = 65 − 6\\) and \\(N = 100 − 10\\)” (p. 294). Here are the new Bayes factors based on that method. z &lt;- 65 - 6 n &lt;- 100 - 10 # Peaked vs Uniform p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = 1 + 6, b = 1 + 10 - 6) ## [1] 0.05570509 # Peaked vs Haldane p_d(z, n, a = 500 + 6, b = 500 + 10 - 6) / p_d(z, n, a = .01 + 6, b = .01 + 10 - 6) ## [1] 0.05748123 Now the two Bayes Factors are nearly the same. It’s not in the text, but let’s compare these three models using brms, information criteria, model weights, model averaging, and posterior predictive checks. First, we’ll save the \\(z\\) and \\(N\\) information as a tibble with a series of 0s and 1s. z &lt;- 65 n &lt;- 100 trial_data &lt;- tibble(y = rep(0:1, times = c(n - z, z))) glimpse(trial_data) ## Observations: 100 ## Variables: 1 ## $ y &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… Next, fit the three models with brms::brm(). fit7 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, prior(beta(500, 500), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10) fit8 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Uniform prior(beta(1, 1), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10) fit9 &lt;- brm(data = trial_data, family = bernoulli(link = identity), y ~ 1, # Haldane prior(beta(0.01, 0.01), class = Intercept), iter = 11000, warmup = 1000, chains = 4, cores = 4, seed = 10) Compare the models by the LOO. fit7 &lt;- add_criterion(fit7, criterion = &quot;loo&quot;) fit8 &lt;- add_criterion(fit8, criterion = &quot;loo&quot;) fit9 &lt;- add_criterion(fit9, criterion = &quot;loo&quot;) loo_compare(fit7, fit8, fit9) ## elpd_diff se_diff ## fit8 0.0 0.0 ## fit9 0.0 0.1 ## fit7 -2.9 2.7 Based on the LOO comparisons, none of the three models was a clear favorite. Although both versions of the anything’s-possible model (i.e., fit8 and fit9) had lower numeric estimates than the must-be-fair model (i.e., fit7), the standard errors on the difference scores were the same magnitude as the difference estimates themselves. As for comparing the two variants of the anything’s-possible model directly, their LOO estimates were almost indistinguishable. Now let’s see what happens when we compute their model weights. (mw &lt;- model_weights(fit7, fit8, fit9)) ## fit7 fit8 fit9 ## 0.1237299 0.3254725 0.5507977 If that’s hard to read, just round(). round(mw, digits = 2) ## fit7 fit8 fit9 ## 0.12 0.33 0.55 Here most of the stacking weight went to fit8, the model with the Beta(1, 1) prior. Like we did earlier with fit1 and fit2, we can use the pp_average() function to compute the stacking weighted posterior for \\(\\theta\\). pp_average(fit7, fit8, fit9, newdata = nd, weights = mw, method = &quot;fitted&quot;, summary = F) %&gt;% as_tibble() %&gt;% ggplot(aes(x = V1)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, size = .2, bins = 30) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = c(.95, .5)) + scale_y_continuous(NULL, breaks = NULL) + labs(subtitle = &quot;The posterior for the probability, given the\\nweighted combination of fit7, fit8, and fit9&quot;, x = expression(paste(italic(p), &quot;(&quot;, theta, &quot;|&quot;, italic(D), &quot;)&quot;))) + coord_cartesian(xlim = 0:1) + theme(panel.grid = element_blank()) Did you notice the weights = mw argument, there? From the pp_average section of the brms reference manual (version 2.10.0), we read: “Alternatively, weights can be a numeric vector of pre-specified weights.” Since we saved the results of model_weights() as an object mw, we were able to capitalize on that feature. If you leave out that argument, you’ll have to wait a bit for brms to compute those weights again from scratch. And just for the sake of practice, we can also compare the models with separate posterior predictive checks using pp_check(). pp_check(fit7, type = &quot;bars&quot;, nsamples = 1e3) + ylim(0, 80) pp_check(fit8, type = &quot;bars&quot;, nsamples = 1e3) + ylim(0, 80) pp_check(fit9, type = &quot;bars&quot;, nsamples = 1e3) + ylim(0, 80) Instead of the default 10, this time we used 1000 posterior simulations from each fit, which we summarized with dot and error bars. This method did a great job showing how little fit7 learned from the data. Another nice thing about this method is it reveals how similar the results are between fit8 and fit9, the two alternate versions of the anything’s-possible model. Also, did you notice how we tacked ylim(0, 80) onto the end of each plots’ code? Holding the scale of the axes constant makes it easier to compare results across plots. 10.7 Bonus: There’s danger ahead If you’re new to model comparison with Bayes factors, information criteria, model stacking and so on, you should know these methods are still subject to spirited debate amongst scholars. For a recent example, see Gronau and Wagenmakers’ (2019) Limitations of Bayesian leave-one-out cross-validation for model selection, which criticized the LOO. Their paper was commented on by Navarro (2019); Chandramouli and Shiffrin (2019); and Vehtari, Simpson, Yao, and Gelman (2019). You can find Gronau and Wagenmakers’ (2019) rejoinder here. And if you love those hot scholarly twitter discussions, these topics seem to spawn one every few months or so (e.g., here). Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 bayesplot_1.7.0 brms_2.10.3 Rcpp_1.0.2 ## [5] gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 ## [9] purrr_0.3.2 readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ## [13] ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] colorspace_1.4-1 ggridges_0.5.1 ## [3] rsconnect_0.8.15 ggstance_0.3.2 ## [5] markdown_1.1 base64enc_0.1-3 ## [7] rstudioapi_0.10 rstan_2.19.2 ## [9] svUnit_0.7-12 DT_0.9 ## [11] fansi_0.4.0 lubridate_1.7.4 ## [13] xml2_1.2.0 bridgesampling_0.7-2 ## [15] knitr_1.23 shinythemes_1.1.2 ## [17] zeallot_0.1.0 jsonlite_1.6 ## [19] broom_0.5.2 shiny_1.3.2 ## [21] compiler_3.6.0 httr_1.4.0 ## [23] backports_1.1.5 assertthat_0.2.1 ## [25] Matrix_1.2-17 lazyeval_0.2.2 ## [27] cli_1.1.0 later_1.0.0 ## [29] htmltools_0.4.0 prettyunits_1.0.2 ## [31] tools_3.6.0 igraph_1.2.4.1 ## [33] coda_0.19-3 gtable_0.3.0 ## [35] glue_1.3.1 reshape2_1.4.3 ## [37] cellranger_1.1.0 vctrs_0.2.0 ## [39] nlme_3.1-139 crosstalk_1.0.0 ## [41] xfun_0.10 ps_1.3.0 ## [43] rvest_0.3.4 mime_0.7 ## [45] miniUI_0.1.1.1 lifecycle_0.1.0 ## [47] gtools_3.8.1 zoo_1.8-6 ## [49] scales_1.0.0 colourpicker_1.0 ## [51] hms_0.4.2 promises_1.1.0 ## [53] Brobdingnag_1.2-6 parallel_3.6.0 ## [55] inline_0.3.15 shinystan_2.5.0 ## [57] yaml_2.2.0 loo_2.1.0 ## [59] StanHeaders_2.19.0 stringi_1.4.3 ## [61] highr_0.8 dygraphs_1.1.1.6 ## [63] pkgbuild_1.0.5 rlang_0.4.0 ## [65] pkgconfig_2.0.3 matrixStats_0.55.0 ## [67] HDInterval_0.2.0 evaluate_0.14 ## [69] lattice_0.20-38 rstantools_2.0.0 ## [71] htmlwidgets_1.5 labeling_0.3 ## [73] tidyselect_0.2.5 processx_3.4.1 ## [75] plyr_1.8.4 magrittr_1.5 ## [77] R6_2.4.0 generics_0.0.2 ## [79] pillar_1.4.2 haven_2.1.0 ## [81] withr_2.1.2 xts_0.11-2 ## [83] abind_1.4-5 modelr_0.1.4 ## [85] crayon_1.3.4 arrayhelpers_1.0-20160527 ## [87] utf8_1.1.4 rmarkdown_1.13 ## [89] grid_3.6.0 readxl_1.3.1 ## [91] callr_3.3.2 threejs_0.3.1 ## [93] digest_0.6.21 xtable_1.8-4 ## [95] httpuv_1.5.2 stats4_3.6.0 ## [97] munsell_0.5.0 viridisLite_0.3.0 ## [99] shinyjs_1.0 "]
]
