[
["index.html", "Doing Bayesian Data Analysis in brms and the tidyverse version 0.0.1 What and why Caution: Work in progress", " Doing Bayesian Data Analysis in brms and the tidyverse version 0.0.1 A Solomon Kurz 2019-10-19 What and why Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” In the same way, this project is designed to help those real people do Bayesian data analysis. My contribution is converting Kruschke’s JAGS code for use in Bürkner’s brms package, which makes it easier to fit Bayesian regression models in R using Hamiltonian Monte Carlo (HMC). I also prefer plotting and data wrangling with the packages from the tidyverse. So we’ll be using those methods, too. This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s Doing Bayesian Data Analysis. Please give the source material some love. Caution: Work in progress The first release of this project only contains chapters 1 through 5, the first section of the text. Most of the remaining chapters have completed drafts and just need another round of edits. I’ll add them, soon. More importantly, there are sections and chapters in this project I have yet to work through. In addition to checking in here, you can follow my GitHub progress log. In that log, I will point out figures or sections I’m having trouble with. Please feel free to offer insights and suggestions, there. "],
["whats-in-this-book-read-this-first.html", "1 What’s in This Book (Read This First!) 1.1 Real people can read this book 1.2 What’s in this book 1.3 What’s new in the second edition 1.4 Gimme feedback (be polite) 1.5 Thank you! Reference Session info", " 1 What’s in This Book (Read This First!) 1.1 Real people can read this book Kruschke began his text with “This book explains how to actually do Bayesian data analysis, by real people (like you), for realistic data (like yours).” Agreed. Similarly, this project is designed to help those real people do Bayesian data analysis. While I’m at it, I may as well explicate my assumptions about you. If you’re looking at this project, I’m guessing you’re a graduate student, a post-graduate academic or researcher of some sort. Which means I’m presuming you have at least a 101-level foundation in statistics. In his text, it seems like Kruschke presumed his readers would have a good foundation in calculus, too. I make no such presumption. But if your stats 101 chops are rusty, check out Legler and Roback’s free bookdown text, Broadening Your Statistical Horizons or Navarro’s free text, Learning statistics with R: A tutorial for psychology students and other beginners. I presume a basic working fluency in R and a vague idea about what the tidyverse is. Kruschke does some R warm-up in chapter 2, and I follow suit. But if you’re totally new to R, you might also consider starting with Peng’s R Programming for Data Science. The best introduction to the tidyvese-style of data analysis I’ve found is Grolemund and Wickham’s R for Data Science. 1.2 What’s in this book This project is not meant to stand alone. It’s a supplement to the second edition of Kruschke’s Doing Bayesian Data Analysis. I follow the structure of his text, chapter by chapter, translating his analyses into brms and tidyverse code. However, many of the sections in the text are composed entirely of equations and prose, leaving us nothing to translate. When we run into those sections, the corresponding sections in this project might be blank or even missing. Also beware the content herein will depart at times from the source material. Bayesian data analysis with HMC is an active area of development in terms of both statistical methods and software implementation. There will also be times when my thoughts and preferences on Bayesian data analysis diverge a bit from Kruschke’s. In those places of divergence, I will often provide references and explanations. In this project, I use a handful of formatting conventions gleaned from R4DS and R Markdown: The Definitive Guide. I put R and R packages (e.g., brms) in boldface. R code blocks and their output appear in a gray background. E.g., 2 + 2 ## [1] 4 Did you notice how there were two strips of gray background, there? The first one designated the actual code. The second one was the output of that code. The output of a code block often begins with ##. Functions are in a typewriter font and followed by parentheses, all atop a gray background (e.g., brm()). When I want to make explicit what packages a given function comes from, I insert the double-colon operator :: between the package name and the function (e.g., tidyr::pivot_longer()). R objects, such as data or function arguments, are in typewriter font atop a gray background (e.g., d or size = 2). Hyperlinks are denoted by their typical blue-colored font. 1.3 What’s new in the second edition This is my first attempt at this project. There’s nothing new from my end. 1.4 Gimme feedback (be polite) I am not a statistician and I have no formal background in computer science. I just finished my PhD in clinical psychology in 2018. During my graduate training I developed an unexpected interest in applied statistics and, more recently, programming. I became an R user in 2015 and started learning about Bayesian statistics around 2013. There is still so much to learn, so my apologies for when my code appears dated or inelegant. There will also be occasions in which I’m not yet sure how to reproduce models or plots in the text. Which is all to say, suggestions on how to improve my code are welcome. Id you’d like to learn more about me, you can find my website here. 1.5 Thank you! While in grad school, I benefitted tremendously from free online content. This project and others like it (e.g., here or here or here) are my attempts to pay it forward. As soon as you’ve gained a little proficiency, do consider doing to same. I addition to great texts like Kruschke’s, I’d like to point out a few other important resources that have allowed me to complete a project like this: Jenny Bryan’s Happy Git and GitHub for the useR is the reference that finally got me working on Github. Again and again, I return to Grolemund and Wickham’s R for Data Science to learn about the tidyverse way of coding. Yihui Xie’s bookdown: Authoring Books and Technical Documents with R Markdown is the primary source from which I learned how to make online books like this. If you haven’t already, bookmark these resources and share them with your friends. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info At the end of every chapter, I use the sessionInfo() function to help make my results more reproducible. sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] pacman_0.5.1 utf8_1.1.4 ## [3] ggstance_0.3.2 tidyselect_0.2.5 ## [5] htmlwidgets_1.5 grid_3.6.0 ## [7] munsell_0.5.0 codetools_0.2-16 ## [9] DT_0.9 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-6 ## [13] colorspace_1.4-1 knitr_1.23 ## [15] rstudioapi_0.10 stats4_3.6.0 ## [17] bayesplot_1.7.0 labeling_0.3 ## [19] rstan_2.19.2 bridgesampling_0.7-2 ## [21] coda_0.19-3 vctrs_0.2.0 ## [23] generics_0.0.2 xfun_0.10 ## [25] R6_2.4.0 markdown_1.1 ## [27] HDInterval_0.2.0 assertthat_0.2.1 ## [29] promises_1.1.0 scales_1.0.0 ## [31] gtable_0.3.0 processx_3.4.1 ## [33] rlang_0.4.0 zeallot_0.1.0 ## [35] lazyeval_0.2.2 broom_0.5.2 ## [37] inline_0.3.15 yaml_2.2.0 ## [39] reshape2_1.4.3 abind_1.4-5 ## [41] modelr_0.1.4 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.5 ## [45] httpuv_1.5.2 rsconnect_0.8.15 ## [47] tools_3.6.0 bookdown_0.12 ## [49] ggplot2_3.2.1 ellipsis_0.3.0 ## [51] ggridges_0.5.1 Rcpp_1.0.2 ## [53] plyr_1.8.4 base64enc_0.1-3 ## [55] purrr_0.3.2 ps_1.3.0 ## [57] prettyunits_1.0.2 openssl_1.4.1 ## [59] zoo_1.8-6 haven_2.1.0 ## [61] magrittr_1.5 colourpicker_1.0 ## [63] packrat_0.5.0 matrixStats_0.55.0 ## [65] hms_0.4.2 shinyjs_1.0 ## [67] mime_0.7 evaluate_0.14 ## [69] arrayhelpers_1.0-20160527 xtable_1.8-4 ## [71] shinystan_2.5.0 readxl_1.3.1 ## [73] gridExtra_2.3 rstantools_2.0.0 ## [75] compiler_3.6.0 tibble_2.1.3 ## [77] crayon_1.3.4 StanHeaders_2.19.0 ## [79] htmltools_0.4.0 later_1.0.0 ## [81] tidyr_1.0.0 lubridate_1.7.4 ## [83] MASS_7.3-51.4 Matrix_1.2-17 ## [85] readr_1.3.1 cli_1.1.0 ## [87] parallel_3.6.0 igraph_1.2.4.1 ## [89] forcats_0.4.0 pkgconfig_2.0.3 ## [91] xml2_1.2.0 svUnit_0.7-12 ## [93] dygraphs_1.1.1.6 rvest_0.3.4 ## [95] stringr_1.4.0 callr_3.3.2 ## [97] digest_0.6.21 rmarkdown_1.13 ## [99] cellranger_1.1.0 curl_4.2 ## [101] shiny_1.3.2 gtools_3.8.1 ## [103] lifecycle_0.1.0 nlme_3.1-139 ## [105] jsonlite_1.6 askpass_1.1 ## [107] fansi_0.4.0 pillar_1.4.2 ## [109] lattice_0.20-38 loo_2.1.0 ## [111] httr_1.4.0 pkgbuild_1.0.5 ## [113] glue_1.3.1 xts_0.11-2 ## [115] shinythemes_1.1.2 stringi_1.4.3 ## [117] dplyr_0.8.3 "],
["introduction-credibility-models-and-parameters.html", "2 Introduction: Credibility, Models, and Parameters 2.1 Bayesian inference is reallocation of credibility across possibilities 2.2 Possibilities are parameter values in descriptive models 2.3 The steps of Bayesian data analysis Reference Session info", " 2 Introduction: Credibility, Models, and Parameters The goal of this chapter is to introduce the conceptual framework of Bayesian data analysis. Bayesian data analysis has two foundational ideas. The first idea is that Bayesian inference is reallocation of credibility across possibilities. The second foundational idea is that the possibilities, over which we allocate credibility, are parameter values in meaningful mathematical models. (p. 15) 2.1 Bayesian inference is reallocation of credibility across possibilities The first step toward making Figure 2.1 is putting together a data object. And to help with that, we’ll open up the tidyverse. library(tidyverse) d &lt;- tibble(iteration = 1:3) %&gt;% expand(iteration, stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(nesting(iteration, stage), Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(.25, times = 4), 0, rep(1/3, times = 3), 0, rep(1/3, times = 3), rep(c(0, .5), each = 2), rep(c(0, .5), each = 2), rep(0, times = 3), 1)) When making data with man repetitions in the rows, it’s good to have the tidyr::expand() function up your sleeve. Go here to learn more. We can take a look at the top few rows of the data with head(). head(d) ## # A tibble: 6 x 4 ## iteration stage Possibilities Credibility ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Prior A 0.25 ## 2 1 Prior B 0.25 ## 3 1 Prior C 0.25 ## 4 1 Prior D 0.25 ## 5 1 Posterior A 0 ## 6 1 Posterior B 0.333 Before we attempt Figure 2.1, we’ll need two supplemental data frames. The first one, d_text, will supply the coordinates for the annotation in the plot. The second, d_arrow, will supply the coordinates for the arrows. d_text &lt;- tibble(Possibilities = &quot;B&quot;, Credibility = .75, label = str_c(LETTERS[1:3], &quot; is\\nimpossible&quot;), iteration = 1:3, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) d_arrow &lt;- tibble(Possibilities = LETTERS[1:3], iteration = 1:3) %&gt;% expand(nesting(Possibilities, iteration), Credibility = c(0.6, 0.01)) %&gt;% mutate(stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) Now we’re ready to code our version of Figure 2.1. d %&gt;% ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom row geom_text(data = d_text, aes(label = label)) + # arrows in the bottom row geom_line(data = d_arrow, arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;first&quot;, type = &quot;closed&quot;)) + facet_grid(stage ~ iteration) + theme(panel.grid = element_blank(), strip.text.x = element_blank(), axis.ticks.x = element_blank()) We will take a similar approach to make our version of Figure 2.2. But this time, we’ll define our supplemental data sets directly in geom_text() and geom_line(). It’s good to have both methods up your sleeve. Also notice how we simply fed our primary data set directly into ggplot() without saving it, either. # primary data tibble(stage = factor(c(&quot;Prior&quot;, &quot;Posterior&quot;), levels = c(&quot;Prior&quot;, &quot;Posterior&quot;))) %&gt;% expand(stage, Possibilities = LETTERS[1:4]) %&gt;% mutate(Credibility = c(rep(0.25, times = 4), rep(0, times = 3), 1)) %&gt;% # plot! ggplot(aes(x = Possibilities, y = Credibility)) + geom_col(color = &quot;grey30&quot;, fill = &quot;grey30&quot;) + # annotation in the bottom panel geom_text(data = tibble( Possibilities = &quot;B&quot;, Credibility = .8, label = &quot;D is\\nresponsible&quot;, stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), aes(label = label) ) + # the arrow geom_line(data = tibble( Possibilities = LETTERS[c(4, 4)], Credibility = c(.25, .99), stage = factor(&quot;Posterior&quot;, levels = c(&quot;Prior&quot;, &quot;Posterior&quot;)) ), arrow = arrow(length = unit(0.30, &quot;cm&quot;), ends = &quot;last&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + facet_wrap(~stage, ncol = 1) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) 2.1.1 Data are noisy and inferences are probabilistic. Now on to Figure 2.3. I’m pretty sure the curves in the plot are Gaussian, which we’ll make with the dnorm() function. After a little trial and error, their standard deviations look to be 1.2. However, it’s tricky placing those curves in along with the probabilities, because the probabilities for the four discrete sizes (i.e., 1 through 4) are in a different metric than the Gaussian density curves. Since the probability metric for the four discrete sizes are the primary metric of the plot, we need to rescale the curves using a little algebra. We do that in the data code, below. After that, the code for the plot is relatively simple. # data tibble(mu = 1:4, p = .25) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot(aes(x = x)) + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(y = density, group = mu)) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = 0:5, ylim = 0:1) + labs(title = &quot;Prior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) We can use the same basic method to make the bottom panel. The new consideration is choosing the relative probabilities for the different mu values–keeping in mind they have to sum to 1. I just eyeballed them. The only other notable change from the previous plot is our addition of a geom_point() section, in which we defined the data on the fly. tibble(mu = 1:4, p = c(.1, .58, .3, .02)) %&gt;% expand(nesting(mu, p), x = seq(from = -2, to = 6, by = .1)) %&gt;% mutate(density = dnorm(x, mean = mu, sd = 1.2)) %&gt;% mutate(d_max = max(density)) %&gt;% mutate(rescale = p / d_max) %&gt;% mutate(density = density * rescale) %&gt;% # plot! ggplot() + geom_col(data = . %&gt;% distinct(mu, p), aes(x = mu, y = p), fill = &quot;grey67&quot;, width = 1/3) + geom_line(aes(x = x, y = density, group = mu)) + geom_point(data = tibble(x = c(1.75, 2.25, 2.75), y = 0), aes(x = x, y = y), size = 3, color = &quot;grey33&quot;, alpha = 3/4) + scale_x_continuous(breaks = 1:4) + scale_y_continuous(breaks = 0:5 / 5) + coord_cartesian(xlim = 0:5, ylim = 0:1) + labs(title = &quot;Posterior&quot;, x = &quot;Possibilities&quot;, y = &quot;Credibility&quot;) + theme(panel.grid = element_blank(), axis.ticks.x = element_blank()) In summary, the essence of Bayesian inference is reallocation of credibility across possibilities. The distribution of credibility initially reflects prior knowledge about the possibilities, which can be quite vague. Then new data are observed, and the credibility is re-allocated. Possibilities that are consistent with the data garner more credibility, while possibilities that are not consistent with the data lose credibility. Bayesian analysis is the mathematics of re-allocating credibility in a logically coherent and precise way. (p. 22) 2.2 Possibilities are parameter values in descriptive models “A key step in Bayesian analysis is defining the set of possibilities over which credibility is allocated. This is not a trivial step, because there might always be possibilities beyond the ones we include in the initial set” (p. 22, emphasis added). In the last section, we used the dnorm() function to make curves following the Normal distribution. Here we’ll do that again, but also use the rnorm() function to simulate actual data from that same Normal distribution. Behold Figure 2.4.a. # set the seed to make the simulation reproducible set.seed(2) # simulate the data with `rnorm()` d &lt;- tibble(x = rnorm(2000, mean = 10, sd = 5)) # plot! ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/10) + geom_line(data = tibble(x = seq(from = -6, to = 26, by = .01)), aes(x = x, y = dnorm(x, mean = 10, sd = 5)), color = &quot;grey33&quot;) + coord_cartesian(xlim = -5:25) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 10 and SD of 5.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) Did you notice how we made the data for the density curve within geom_line()? That’s one way to do it. In our next plot, we’ll take a different and more elegant approach with stat_function(). Here’s our Figure 2.4.b. ggplot(data = d, aes(x = x)) + geom_histogram(aes(y = ..density..), binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + stat_function(fun = dnorm, n = 101, args = list(mean = 8, sd = 6), color = &quot;grey33&quot;, linetype = 2) + coord_cartesian(xlim = -5:25) + labs(subtitle = &quot;The candidate normal distribution\\nhas a mean of 8 and SD of 6.&quot;, x = &quot;Data Values&quot;, y = &quot;Data Probability&quot;) + theme(panel.grid = element_blank()) 2.3 The steps of Bayesian data analysis In general, Bayesian analysis of data follows these steps: Identify the data relevant to the research questions. What are the measurement scales of the data? Which data variables are to be predicted, and which data variables are supposed to act as predictors? Define a descriptive model for the relevant data. The mathematical form and its parameters should be meaningful and appropriate to the theoretical purposes of the analysis. Specify a prior distribution on the parameters. The prior must pass muster with the audience of the analysis, such as skeptical scientists. Use Bayesian inference to re-allocate credibility across parameter values. Interpret the posterior distribution with respect to theoretically meaningful issues (assuming that the model is a reasonable description of the data; see next step). Check that the posterior predictions mimic the data with reasonable accuracy (i.e., conduct a “posterior predictive check”). If not, then consider a different descriptive model. Perhaps the best way to explain these steps is with a realistic example of Bayesian data analysis. The discussion that follows is abbreviated for purposes of this introductory chapter, with many technical details suppressed. (p. 25) I will show you a few more details than Kruschke did in the text. But just has he did, we’ll cover this workflow in much more detail in the chapters to come. In order to recreate Figure 2.5, we need to generate the data and fit the model. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height/weight data of the kind in his text. Here is the code in full: HtWtDataGenerator &lt;- function(nSubj, rndsd = NULL, maleProb = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} datamatrix &lt;- matrix(0, nrow = nSubj, ncol = 3) colnames(datamatrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:nSubj) { # Flip coin to decide sex sex &lt;- sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(maleProb, 1 - maleProb)) if (sex == maleval) {datum = MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum = MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum = MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } datamatrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(datamatrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values to generate and how probable we want the values to be based on those from men. These are controlled by the nSubj and maleProb parameters. # set your seed to make the data generation reproducible set.seed(57) d &lt;- HtWtDataGenerator(nSubj = 57, maleProb = .5) %&gt;% as_tibble() d %&gt;% head() ## # A tibble: 6 x 3 ## male height weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 68.8 133. ## 2 1 70 187. ## 3 0 63.2 154 ## 4 0 61.4 145. ## 5 0 66.1 130. ## 6 1 71.5 271 We’re about ready for the model. We will fit it with HMC via the brms package. library(brms) The traditional use of diffuse and noninformative priors is discouraged with HMC, as is the uniform distribution for sigma. Instead, we’ll use weakly-regularizing priors for the intercept and slope and a half Cauchy with a fairly large scale parameter for \\(\\sigma\\). fit1 &lt;- brm(data = d, family = gaussian, weight ~ 1 + height, prior = c(prior(normal(0, 100), class = Intercept), prior(normal(0, 100), class = b), prior(cauchy(0, 10), class = sigma)), chains = 4, cores = 4, iter = 2000, warmup = 1000, seed = 2) If you wanted a quick model summary, you could execute print(fit1). We’ll walk through that and other diagnostics in greater detail starting in Chapter 8. But for now, here’s how we might make Figure 2.5.a. # extract the posterior draws post &lt;- posterior_samples(fit1) # this will streamline some of the code, below n_lines &lt;- 150 # plot! d %&gt;% ggplot(aes(x = height, y = weight)) + geom_abline(intercept = post[1:n_lines, 1], slope = post[1:n_lines, 2], color = &quot;grey50&quot;, size = 1/4, alpha = .3) + geom_point(alpha = 2/3) + coord_cartesian(xlim = 55:80, ylim = 50:250) + # the `eval(substitute(paste()))` trick came from: https://www.r-bloggers.com/value-of-an-r-object-in-an-expression/ labs(subtitle = eval(substitute(paste(&quot;Data with&quot;, n_lines, &quot;credible regression lines&quot;))), x = &quot;Height in inches&quot;, y = &quot;Weight in pounds&quot;) + theme(panel.grid = element_blank()) For Figure 2.5.b., we’ll use the handy stat_pointintervalh() function from the tidybayes package to mark off the mode and 95% HDIs. library(tidybayes) post %&gt;% ggplot(aes(x = b_height)) + geom_histogram(color = &quot;grey92&quot;, fill = &quot;grey67&quot;, binwidth = .2, size = .2) + stat_pointintervalh(aes(y = 0), point_interval = mode_hdi, .width = .95) + scale_y_continuous(NULL, breaks = NULL) + coord_cartesian(xlim = 0:8) + labs(title = &quot;The posterior distribution&quot;, subtitle = &quot;The mode and 95% HPD intervals are\\nthe dot and horizontal line at the bottom.&quot;, x = expression(paste(beta[1], &quot; (slope)&quot;))) + theme(panel.grid = element_blank()) Here’s Figure 2.6. We’ll go over the brms::predict() function later. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 20)) predict(fit1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_pointrange(aes(y = Estimate, ymin = Q2.5, ymax = Q97.5), color = &quot;grey67&quot;, shape = 20) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) The posterior predictions might be easier to depict with a ribbon and line, instead. nd &lt;- tibble(height = seq(from = 53, to = 81, length.out = 30)) predict(fit1, newdata = nd) %&gt;% as_tibble() %&gt;% bind_cols(nd) %&gt;% ggplot(aes(x = height)) + geom_ribbon(aes(ymin = Q2.5, ymax = Q97.5), fill = &quot;grey75&quot;) + geom_line(aes(y = Estimate), color = &quot;grey92&quot;) + geom_point(data = d, aes(y = weight), alpha = 2/3) + labs(subtitle = &quot;Data with the percentile-based 95% intervals and\\nthe means of the posterior predictions&quot;, x = &quot;Height in inches&quot;, y = &quot;Weight in inches&quot;) + theme(panel.grid = element_blank()) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 brms_2.10.3 Rcpp_1.0.2 forcats_0.4.0 ## [5] stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 readr_1.3.1 ## [9] tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.5.1 utf8_1.1.4 ## [3] ggstance_0.3.2 tidyselect_0.2.5 ## [5] htmlwidgets_1.5 grid_3.6.0 ## [7] munsell_0.5.0 codetools_0.2-16 ## [9] DT_0.9 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-6 ## [13] colorspace_1.4-1 knitr_1.23 ## [15] rstudioapi_0.10 stats4_3.6.0 ## [17] bayesplot_1.7.0 labeling_0.3 ## [19] rstan_2.19.2 bridgesampling_0.7-2 ## [21] coda_0.19-3 vctrs_0.2.0 ## [23] generics_0.0.2 xfun_0.10 ## [25] R6_2.4.0 markdown_1.1 ## [27] HDInterval_0.2.0 assertthat_0.2.1 ## [29] promises_1.1.0 scales_1.0.0 ## [31] gtable_0.3.0 processx_3.4.1 ## [33] rlang_0.4.0 zeallot_0.1.0 ## [35] lazyeval_0.2.2 broom_0.5.2 ## [37] inline_0.3.15 yaml_2.2.0 ## [39] reshape2_1.4.3 abind_1.4-5 ## [41] modelr_0.1.4 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.5 ## [45] httpuv_1.5.2 rsconnect_0.8.15 ## [47] tools_3.6.0 bookdown_0.12 ## [49] ellipsis_0.3.0 ggridges_0.5.1 ## [51] plyr_1.8.4 base64enc_0.1-3 ## [53] ps_1.3.0 prettyunits_1.0.2 ## [55] openssl_1.4.1 zoo_1.8-6 ## [57] haven_2.1.0 magrittr_1.5 ## [59] colourpicker_1.0 packrat_0.5.0 ## [61] matrixStats_0.55.0 hms_0.4.2 ## [63] shinyjs_1.0 mime_0.7 ## [65] evaluate_0.14 arrayhelpers_1.0-20160527 ## [67] xtable_1.8-4 shinystan_2.5.0 ## [69] readxl_1.3.1 gridExtra_2.3 ## [71] rstantools_2.0.0 compiler_3.6.0 ## [73] crayon_1.3.4 StanHeaders_2.19.0 ## [75] htmltools_0.4.0 later_1.0.0 ## [77] lubridate_1.7.4 MASS_7.3-51.4 ## [79] Matrix_1.2-17 cli_1.1.0 ## [81] parallel_3.6.0 igraph_1.2.4.1 ## [83] pkgconfig_2.0.3 xml2_1.2.0 ## [85] svUnit_0.7-12 dygraphs_1.1.1.6 ## [87] rvest_0.3.4 callr_3.3.2 ## [89] digest_0.6.21 rmarkdown_1.13 ## [91] cellranger_1.1.0 curl_4.2 ## [93] shiny_1.3.2 gtools_3.8.1 ## [95] lifecycle_0.1.0 nlme_3.1-139 ## [97] jsonlite_1.6 askpass_1.1 ## [99] fansi_0.4.0 pillar_1.4.2 ## [101] lattice_0.20-38 loo_2.1.0 ## [103] httr_1.4.0 pkgbuild_1.0.5 ## [105] glue_1.3.1 xts_0.11-2 ## [107] shinythemes_1.1.2 stringi_1.4.3 "],
["the-r-programming-language.html", "3 The R Programming Language 3.1 Get the software 3.2 A simple example of R in action 3.3 Basic commands and operators in R 3.4 Variable types 3.5 Loading and saving data 3.6 Some utility functions 3.7 Programming in R 3.8 Graphical plots: Opening and saving Reference Session info", " 3 The R Programming Language The material in this chapter is rather dull reading because it basically amounts to a list (although a carefully scaffolded list) of basic commands in R along with illustrative examples. After reading the first few pages and nodding off, you may be tempted to skip ahead, and I wouldn’t blame you. But much of the material in this chapter is crucial, and all of it will eventually be useful, so you should at least skim it all so you know where to return when the topics arise later. (p. 35) Most, but not all, of this part of my project will mirror what’s in the text. However, I do add tidyverse-oriented content, such as a small walk through of plotting in ggplot2. 3.1 Get the software In addition to R and RStudio, I make use of a variety of R packages in this project. You can get the heaviest hitters by executing this code block. install.packages(&quot;devtools&quot;) install.packages(&quot;tidyverse&quot;, dependencies = T) install.packages(&quot;brms&quot;, dependencies = T) install.packages(&quot;tidybayes&quot;, dependencies = T) 3.1.1 A look at RStudio. The R programming language comes with its own basic user interface that is adequate for modest applications. But larger applications become unwieldy in the basic R user interface, and therefore it helps to install a more sophisticated R-friendly editor. There are a number of useful editors available, many of which are free, and they are constantly evolving. At the time of this writing, I recommend RStudio, which can be obtained from http://www.rstudio.com/ (p. 35). I completely agree. R programing is easier with RStudio. 3.2 A simple example of R in action Basic arithmetic is straightforward in R. 2 + 3 ## [1] 5 Algebra is simple, too. x &lt;- 2 x + x ## [1] 4 Behold Figure 3.1. library(tidyverse) d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) If you’re new to the tidyverse and/or making figures with ggplot2, it’s worthwhile to walk that code out. With the first line, library(tidyverse), we opened up the core packages within the tidyverse, which are: ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats. With the next block d &lt;- tibble(x = seq(from = -2, to = 2, by = .1)) %&gt;% mutate(y = x^2) we made our tibble. In R, data frames are one of the primary types of data objects (see subsection 3.4.4., below). We’ll make extensive use of data frames in this project. Tibbles are a particular type of data frame, which you might learn more about here. With those first two lines, we determined what the name of our tibble would be, d, and made the first column, x. Note the %&gt;% operator at the end of the second line. In prose, we call that the pipe. As explained in chapter 5 of R4DS, “a good way to pronounce %&gt;% when reading code is ‘then.’” So in words, the those first two lines indicate “Make an object, d, which is a tibble with a variable, x, defined by the seq() function, then…” In the portion after then (i.e., the %&gt;%), we changed d. The mutate() function let us add another variable, y, which is a function of our first variable, x. With the next 4 lines of code, we made our plot. When plotting with ggplot2, the first line is always with the ggplot() function. This is where you typically tell ggplot2 what data object you’re using–which must be a data frame or tibble–and what variables you want on your axes. The interesting thing about ggplot2 is that the code is modular. So if we only coded the ggplot() portion, we’d get: ggplot(data = d, aes(x = x, y = y)) Although ggplot2 knows which variables to put on which axes, it has no idea how we’d like to express the data. The result is an empty coordinate system. The next line of code is the main event. With geom_line() we told ggplot2 to connect the data points with a line. With the color argument, we made that line skyblue. [Here’s a great list of the named colors available in ggplot2.] Also, notice the + operator at the end of the ggplot() function. With ggplot2, you add functions by placing the + operator on the right of the end of one function, which will then append the next function. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) Personally, I’m not a fan of gridlines. They occasionally have their place and I do use them from time to time. But on the while, I prefer to omit them from my plots. The final theme() function allowed me to do so. ggplot(data = d, aes(x = x, y = y)) + geom_line(color = &quot;skyblue&quot;) + theme(panel.grid = element_blank()) Chapter 3 of R4DS is a great introduction to plotting with ggplot2. If you want to dive deeper, see the references at the bottom of this page. 3.2.1 Get the programs used with this book. This subtitle has a double meaning, here. Yes, you should probably get Kruschke’s scripts from the book’s website. You may have noticed this already, but unlike in Kruschke’s text, I will usually show all my code. Indeed, the purpose of my project is to make coding these kinds of models and visualizations easier. But if you’re ever curious, you can always find my script files in their naked form, here. Later in this subsection, Kruschke mentioned working directories. If you don’t know what your current working directory is, just execute getwd(). I’ll have more to say on this topic later on when I make my pitch for RStudio projects. 3.3 Basic commands and operators in R In addition to the resource link Kruschke provided in the text, Grolemund and Wickham’s R4DS is an excellent general introduction to the kinds of R functions you’ll want to succeed with your data analysis. Other than that, I’ve learned the most when I had a specific data problem to solve and then sought out the specific code/techniques required to solve it. If already have your own data or can get your hands on some sexy data, learn these techniques by playing around with them. This isn’t the time to worry about rigor, preregistration, or all of that. This is time to play. 3.3.1 Getting help in R. As with plot() you can learn more about the ggplot() function with ?. ?ggplot help.start() can be nice, too. help.start() ??geom_line() can help us learn more about the geom_line() function. ??geom_line() 3.3.2 Arithmetic and logical operators. With arithmetic, the order of operations is: power first, then multiplication, then addition. 1 + 2 * 3^2 ## [1] 19 With parentheses, you can force addition before multiplication. (1 + 2) * 3^2 ## [1] 27 Operations inside parentheses get done before power operations. (1 + 2 * 3)^2 ## [1] 49 One can nest parentheses. ((1 + 2) * 3)^2 ## [1] 81 ?Syntax We can use R to perform a variety of logical tests, such as negation. !TRUE ## [1] FALSE We can do conjunction. TRUE &amp; FALSE ## [1] FALSE And we can do disjunction. TRUE | FALSE ## [1] TRUE Conjunction has precedence over disjunction. TRUE | TRUE &amp; FALSE ## [1] TRUE However, with parentheses we can force disjunction first. (TRUE | TRUE) &amp; FALSE ## [1] FALSE 3.3.3 Assignment, relational operators, and tests of equality. In contrast to Kruschke’s preference, I will use the arrow operator, &lt;-, to assign values to named variables. x = 1 x &lt;- 1 Yep, this ain’t normal math. (x = 1) ## [1] 1 (x = x + 1) ## [1] 2 Here we use == to test for equality. (x = 2) ## [1] 2 x == 2 ## [1] TRUE Using !=, we can check whether the value of x is NOT equal to 3. x != 3 ## [1] TRUE We can use &lt; to check whether the value of x is less than 3. x &lt; 3 ## [1] TRUE Similarly, we can use &gt; to check whether the value of x is greater than 3. x &gt; 3 ## [1] FALSE This normal use of the &lt;- operator x &lt;- 3 is not the same as x &lt; - 3 ## [1] FALSE The limited precision of a computer’s memory can lead to odd results. x &lt;- 0.5 - 0.3 y &lt;- 0.3 - 0.1 Although mathematically TRUE, this is FALSE for limited precision. x == y ## [1] FALSE However, they are equal up to the precision of a computer. all.equal(x, y) ## [1] TRUE 3.4 Variable types If you’d like to learn more about the differences among vectors, matrices, lists, data frames and so on, you might check out Roger Peng’s R Programming for Data Science, chapter 4. 3.4.1 Vector. 3.4.1.1 The combine function. The combine function is c(). c(2.718, 3.14, 1.414) ## [1] 2.718 3.140 1.414 x &lt;- c(2.718, 3.14, 1.414) You’ll note the equivalence. x == c(2.718, 3.14, 1.414) ## [1] TRUE TRUE TRUE This leads to the next subsection. 3.4.1.2 Component-by-component vector operations. We can multiply two vectors, component by component. c(1, 2, 3) * c(7, 6, 5) ## [1] 7 12 15 If you have a sole number, a scaler, you can multiply an entire vector by it like: 2 * c(1, 2, 3) ## [1] 2 4 6 which is a more compact way to perform this. c(2, 2, 2) * c(1, 2, 3) ## [1] 2 4 6 The same sensibilities hold for other operations, such as addition. 2 + c(1, 2, 3) ## [1] 3 4 5 3.4.1.3 The colon operator and sequence function. The colon operator has precedence over addition. 2 + 3:6 ## [1] 5 6 7 8 Parentheses override default precedence. (2 + 3):6 ## [1] 5 6 The power operator has precedence over the colon operator. 1:3^2 ## [1] 1 2 3 4 5 6 7 8 9 And parentheses override default precedence. (1:3)^2 ## [1] 1 4 9 The seq() function is quite handy. If you don’t specify the length of the output, it will figure that out the logical consequence of the other arguments. seq(from = 0, to = 3, by = 0.5) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This sequence won’t exceed to = 3. seq(from = 0, to = 3, by = 0.5001) ## [1] 0.0000 0.5001 1.0002 1.5003 2.0004 2.5005 In each of the following examples, we’ll omit one of the core seq() arguments: from, to, by, and length.out. Here we do not define the end point. seq(from = 0, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 This time we fail to define the increment. seq(from = 0, to = 3, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 And this time we omit a starting point. seq(to = 3, by = 0.5, length.out = 7) ## [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.4.1.4 The replicate function. We’ll define our pre-replication vector with the &lt;- operator. abc &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) With times, we repeat the vector as a unit. rep(abc, times = 2) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; But if we mix times with c(), we can repeat individual components of abc differently. rep(abc, times = c(4, 2, 1)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; With the each argument, we repeat the individual components of abc one at a time. rep(abc, each = 2) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; And you can even combine each and length, repeating each element until the length requirement has been fulfilled. rep(abc, each = 2, length = 10) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; You can also combine each and times. rep(abc, each = 2, times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; ## [18] &quot;C&quot; I tend to do things like the above as two separate steps. One way to do so is by nesting one rep() function within another. rep(rep(abc, each = 2), times = 3) ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; ## [18] &quot;C&quot; As Kruschke points out, this can look confusing. rep(abc, each = 2, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; But breaking the results up into two steps might be easier to understand, rep(rep(abc, each = 2), times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; And especially earlier in my R career, it helped quite a bit to break operation sequences like this up by saving and assessing the intermediary steps. step_1 &lt;- rep(abc, each = 2) step_1 ## [1] &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; rep(step_1, times = c(1, 2, 3, 1, 2, 3)) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; 3.4.1.5 Getting at elements of a vector. Behold our exemplar vector, x. x &lt;- c(2.718, 3.14, 1.414, 47405) The straightforward way to extract the second and fourth elements is x[c(2, 4)] ## [1] 3.14 47405.00 Or you might use reverse logic and omit the first and third elements. x[c(-1, -3 )] ## [1] 3.14 47405.00 It’s handy to know that T is a stand in for TRUE and F is a stand in for FALSE. You’ll probably notice I tend to use the abbreviations most of the time. x[c(F, T, F, T)] ## [1] 3.14 47405.00 The names() function makes it easy to name the components of a vector. names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) x ## e pi sqrt2 zipcode ## 2.718 3.140 1.414 47405.000 Now we can call the components with their names. x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 Here’s Kruschke’s review: # define a vector x &lt;- c(2.718, 3.14, 1.414, 47405) # name the components names(x) &lt;- c(&quot;e&quot;, &quot;pi&quot;, &quot;sqrt2&quot;, &quot;zipcode&quot;) # you can indicate which elements you&#39;d like to include x[c(2, 4)] ## pi zipcode ## 3.14 47405.00 # you can decide which to exclude x[c(-1, -3)] ## pi zipcode ## 3.14 47405.00 # or you can use logical tests x[c(F, T, F, T)] ## pi zipcode ## 3.14 47405.00 # and you can use the names themselves x[c(&quot;pi&quot;, &quot;zipcode&quot;)] ## pi zipcode ## 3.14 47405.00 3.4.2 Factor. Here are our five-person SES status data. x &lt;- c(&quot;high&quot;, &quot;medium&quot;, &quot;low&quot;, &quot;high&quot;, &quot;medium&quot;) x ## [1] &quot;high&quot; &quot;medium&quot; &quot;low&quot; &quot;high&quot; &quot;medium&quot; The factor() function turns them into a factor, which will return the levels when called. xf &lt;- factor(x) xf ## [1] high medium low high medium ## Levels: high low medium Here are the factor levels as numerals. as.numeric(xf) ## [1] 1 3 2 1 3 With the levels and ordered arguments, we can order the factor elements. xfo &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T) xfo ## [1] high medium low high medium ## Levels: low &lt; medium &lt; high Now “high” is a larger integer. as.numeric(xfo) ## [1] 3 2 1 3 2 We’ve already specified xf. xf ## [1] high medium low high medium ## Levels: high low medium And we know how it’s been coded numerically. as.numeric(xf) ## [1] 1 3 2 1 3 We can have levels and labels. xfol &lt;- factor(x, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), ordered = T, labels = c(&quot;Bottom SES&quot;, &quot;Middle SES&quot;, &quot;Top SES&quot;)) xfol ## [1] Top SES Middle SES Bottom SES Top SES Middle SES ## Levels: Bottom SES &lt; Middle SES &lt; Top SES 3.4.3 Matrix and array. Kruschke uses these more often than I do. I’m more of a vector and data frame kinda guy. Even so, here’s an example of a matrix. matrix(1:6, ncol = 3) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 We can get the same thing using nrow. matrix(1:6, nrow = 2) ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 Note how the numbers got ordered by rows within each column? We can specify them to be ordered across columns, first. matrix(1:6, nrow = 2, byrow = T) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 We can name the dimensions. I’m not completely consistent, but I’ve been moving in the direction of following The Tidyverse Style Guide for naming my R objects and their elements. From the guide, we read Variable and function names should use only lowercase letters, numbers, and _. Use underscores (_) (so called snake case) to separate words within a name. By those sensibilities, we’ll name our rows and columns as matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) ## TheColDimName ## TheRowDimName col_1_name col_2_name col_3_name ## row_1_name 1 3 5 ## row_2_name 2 4 6 You’ve also probably noticed that I “always put a space after a comma, never before, just like in regular English,” as well as “put a space before and after = when naming arguments in function calls.” IMO, this makes code easier to read. You do you. We’ll name our matrix x. x &lt;- matrix(1:6, nrow = 2, dimnames = list(TheRowDimName = c(&quot;row_1_name&quot;, &quot;row_2_name&quot;), TheColDimName = c(&quot;col_1_name&quot;, &quot;col_2_name&quot;, &quot;col_3_name&quot;))) Since there are 2 dimensions, we’ll subset with two dimensions. Numerical indices work. x[2, 3] ## [1] 6 Row and column names work, too. Just make sure to use quotation marks, &quot;&quot;, for those. x[&quot;row_2_name&quot;, &quot;col_3_name&quot;] ## [1] 6 Here we specify the range of columns to include. x[2, 1:3] ## col_1_name col_2_name col_3_name ## 2 4 6 Leaving that argument blank returns them all. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 And leaving the row index blank returns all row values within the specified column(s). x[, 3] ## row_1_name row_2_name ## 5 6 Mind your commas! This produces the second row, returned as a vector. x[2, ] ## col_1_name col_2_name col_3_name ## 2 4 6 This returns both rows of the 2nd column. x[, 2] ## row_1_name row_2_name ## 3 4 Leaving out the comma will return the numbered element. x[2] ## [1] 2 It’ll be important in your brms career to have a sense of 3-dimensional arrays. Several brms convenience functions often return them (e.g., ranef() in multilevel models). a &lt;- array(1:24, dim = c(3, 4, 2), # 3 rows, 4 columns, 2 layers dimnames = list(RowDimName = c(&quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;), ColDimName = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;), LayDimName = c(&quot;l1&quot;, &quot;l2&quot;))) a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 Since these have 3 dimensions, you have to use 3-dimensional indexing. As with 2-dimensional objects, leaving the indices for a dimension blank will return all elements within that dimension. For example, this code returns all columns of r3 and l2, as a vector. a[&quot;r3&quot;, , &quot;l2&quot;] ## c1 c2 c3 c4 ## 15 18 21 24 And this code returns all layers of r3 and c4, as a vector. a[&quot;r3&quot;, &quot;c4&quot;, ] ## l1 l2 ## 12 24 3.4.4 List and data frame. Here’s my_list. my_list &lt;- list(&quot;a&quot; = 1:3, &quot;b&quot; = matrix(1:6, nrow = 2), &quot;c&quot; = &quot;Hello, world.&quot;) my_list ## $a ## [1] 1 2 3 ## ## $b ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## $c ## [1] &quot;Hello, world.&quot; To return the contents of the a portion of my_list, just execute this. my_list$a ## [1] 1 2 3 We can index further within a. my_list$a[2] ## [1] 2 To return the contents of the first item in our list with the double bracket, [[]], do: my_list[[1]] ## [1] 1 2 3 You can index further to return only the second element of the first list item. my_list[[1]][2] ## [1] 2 But double brackets, [][], are no good, here. my_list[1][2] ## $&lt;NA&gt; ## NULL To learn more, Jenny Bryan has a great talk discussing the role of lists within data wrangling. But here’s a data frame. d &lt;- data.frame(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) d ## integers number_names ## 1 1 one ## 2 2 two ## 3 3 three With data frames, we can continue indexing with the $ operator. d$number_names ## [1] one two three ## Levels: one three two We can also use the double bracket. d[[2]] ## [1] one two three ## Levels: one three two Notice how the single bracket with no comma indexes columns rather than rows. d[2] ## number_names ## 1 one ## 2 two ## 3 three But adding the comma returns the factor-level information when indexing columns. d[, 2] ## [1] one two three ## Levels: one three two It works a touch differently when indexing by row. d[2, ] ## integers number_names ## 2 2 two Let’s try with a tibble, instead. t &lt;- tibble(integers = 1:3, number_names = c(&quot;one&quot;, &quot;two&quot;, &quot;three&quot;)) t ## # A tibble: 3 x 2 ## integers number_names ## &lt;int&gt; &lt;chr&gt; ## 1 1 one ## 2 2 two ## 3 3 three One difference is that tibbles default to assigning text columns as character strings rather than factors. Another difference occurs when printing large data frames versus large tibbles. Tibbles yield more compact glimpses. For more, check out R4DS Chapter 10. It’s also worthwhile pointing out that within the tidyverse, you can pull out a specific column with the select() function. Here we select number_names. t %&gt;% select(number_names) ## # A tibble: 3 x 1 ## number_names ## &lt;chr&gt; ## 1 one ## 2 two ## 3 three Go here learn more about select(). 3.5 Loading and saving data 3.5.1 The read.csv read_csv() and read.table read_table() functions. Although read.csv() is the default CSV reader in R, the read_csv() function from the readr package (i.e., one of the core tidyverse packages) is a new alternative. In comparison to base R’s read.csv(), readr::read_csv() is faster and returns tibbles (as opposed to data frames with read.csv()). The same general points hold for base R’s read.table() versus readr::read_table(). Using Kruschke’s HGN.csv example, we’d load the CSV with read_csv() like this: hgn &lt;- read_csv(&quot;data.R/HGN.csv&quot;) Note again that read_csv() defaults to returning columns with character information as characters, not factors. hgn$Hair ## [1] &quot;black&quot; &quot;brown&quot; &quot;blond&quot; &quot;black&quot; &quot;black&quot; &quot;red&quot; &quot;brown&quot; See? As a character variable, Hair no longer has factor level information. But if you knew you wanted to treat Hair as a factor, you could easily convert it with dplyr::mutate(). hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair)) hgn$Hair ## [1] black brown blond black black red brown ## Levels: black blond brown red And here’s a tidyverse way to reorder the levels for the Hair factor. hgn &lt;- hgn %&gt;% mutate(Hair = factor(Hair, levels = c(&quot;red&quot;, &quot;blond&quot;, &quot;brown&quot;, &quot;black&quot;))) hgn$Hair ## [1] black brown blond black black red brown ## Levels: red blond brown black as.numeric(hgn$Hair) ## [1] 4 3 2 4 4 1 3 Since we imported hgn with read_csv(), the Name column is already a character vector, which we can verify with the str() function. hgn$Name %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; Note how using as.vector() did nothing in our case. Name was already a character vector. hgn$Name %&gt;% as.vector() %&gt;% str() ## chr [1:7] &quot;Alex&quot; &quot;Betty&quot; &quot;Carla&quot; &quot;Diane&quot; &quot;Edward&quot; &quot;Frank&quot; &quot;Gabrielle&quot; The Group column was imported as composed of integers. hgn$Group %&gt;% str() ## num [1:7] 1 1 1 2 2 2 2 Switching Group to a factor is easy enough. hgn &lt;- hgn %&gt;% mutate(Group = factor(Group)) hgn$Group ## [1] 1 1 1 2 2 2 2 ## Levels: 1 2 3.5.2 Saving data from R. Yeah you guessed, readr has a write_csv() function, too. The arguments are as follows: write_csv(x, path, na = &quot;NA&quot;, append = FALSE, col_names = !append). Saving hgn in your working directory is as easy as: write_csv(hgn, &quot;hgn.csv&quot;) You could also use save(). save(hgn, file = &quot;hgn.Rdata&quot; ) Once we start fitting Bayesian models, this method will be an important way to save the results of those models. The load() function is simple. load(&quot;hgn.Rdata&quot; ) The ls() function works very much the same way as the more verbosely-named objects() function. ls() ## [1] &quot;a&quot; &quot;abc&quot; &quot;d&quot; ## [4] &quot;d_arrow&quot; &quot;d_text&quot; &quot;fit1&quot; ## [7] &quot;hgn&quot; &quot;HtWtDataGenerator&quot; &quot;my_list&quot; ## [10] &quot;n_lines&quot; &quot;nd&quot; &quot;post&quot; ## [13] &quot;step_1&quot; &quot;t&quot; &quot;x&quot; ## [16] &quot;xf&quot; &quot;xfo&quot; &quot;xfol&quot; ## [19] &quot;y&quot; 3.6 Some utility functions # this is a more compact way to replicate 100 1&#39;s, 200 2&#39;s, and 300 3&#39;s x &lt;- rep(1:3, times = c(100, 200, 300)) summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 2.500 2.333 3.000 3.000 We can use the pipe to convert and then summarize x. x %&gt;% factor() %&gt;% summary() ## 1 2 3 ## 100 200 300 head() and tail() are quite useful. head(x) ## [1] 1 1 1 1 1 1 tail(x) ## [1] 3 3 3 3 3 3 Within the tidyverse, the slice() function serves a similar role. In order to use slice(), we’ll want to convert x, which is just a vector of integers, into a data frame. Then we’ll use slice() to return a subset of the rows. x &lt;- x %&gt;% as_tibble() x %&gt;% slice(1:6) ## # A tibble: 6 x 1 ## value ## &lt;int&gt; ## 1 1 ## 2 1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 So that was analogous to what we accomplished with head(). Here’s the analogue to tail(). x %&gt;% slice(595:600) ## # A tibble: 6 x 1 ## value ## &lt;int&gt; ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 The downside of that code was we had to do the math to determine that \\(600 - 6 = 595\\) in order to get the last six rows, as returned by tail(). A more general approach is to use n(), which will return the total number of rows in the tibble. x %&gt;% slice((n() - 6):n()) ## # A tibble: 7 x 1 ## value ## &lt;int&gt; ## 1 3 ## 2 3 ## 3 3 ## 4 3 ## 5 3 ## 6 3 ## 7 3 To unpack (n() - 6):n(), because n() = 600, (n() - 6) = 600 - 6 = 595. Therefore (n() - 6):n() was equivalent to having coded 595:600. Instead of having to do the math ourselves, n() did it for us. It’s often easier to just go with head() or tail(). But the advantage of this more general approach is that it allows one take more complicated slices of the data, such as returning the first three and last three rows. x %&gt;% slice(c(1:3, (n() - 3):n())) ## # A tibble: 7 x 1 ## value ## &lt;int&gt; ## 1 1 ## 2 1 ## 3 1 ## 4 3 ## 5 3 ## 6 3 ## 7 3 We’ve already used the handy str() function a bit. It’s also nice to know that tidyverse::glimpse() performs a similar function. x %&gt;% str() ## Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 600 obs. of 1 variable: ## $ value: int 1 1 1 1 1 1 1 1 1 1 ... x %&gt;% glimpse() ## Observations: 600 ## Variables: 1 ## $ value &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… Within the tidyverse, we’d use group_by() and then summarize() as alternatives to the aggregate() function. With group_by() we group the observations first by Hair and then by Gender within Hair. After that, we summarize the groups by taking the median() values of their Number. hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(median = median(Number)) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender median ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 red M 7 ## 2 blond F 3 ## 3 brown F 7 ## 4 black F 7 ## 5 black M 1.5 One of the nice things about this workflow is that the code reads somewhat like how we’d explain what we were doing. We, in effect, told R to Take hgn, then group the data by Hair and Gender within Hair, and then summarize() those groups by their median() Number values. There’s also the nice quality that we don’t have to continually tell R where the data are coming from the way the aggregate() function required Kruschke to prefix each of his variables with HGNdf$. We also didn’t have to explicitly rename the output columns the way Kruschke had to. I’m not aware that our group_by() %&gt;% summarize() workflow has a formula format the way aggregate() does. To count how many levels we had in a grouping factor, we’d use the n() function in summarize(). hgn %&gt;% group_by(Hair, Gender) %&gt;% summarize(n = n()) ## # A tibble: 5 x 3 ## # Groups: Hair [4] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 Alternatively, we could switch out the summary(n = n()) line with count(). hgn %&gt;% group_by(Hair, Gender) %&gt;% count() ## # A tibble: 5 x 3 ## # Groups: Hair, Gender [5] ## Hair Gender n ## &lt;fct&gt; &lt;chr&gt; &lt;int&gt; ## 1 red M 1 ## 2 blond F 1 ## 3 brown F 2 ## 4 black F 1 ## 5 black M 2 We could then use spread() to convert that output to a format similar to Kruschke’s table of counts. hgn %&gt;% group_by(Hair, Gender) %&gt;% count() %&gt;% spread(key = Hair, value = n) ## # A tibble: 2 x 5 ## # Groups: Gender [2] ## Gender red blond brown black ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 F NA 1 2 1 ## 2 M 1 NA NA 2 With this method, the NAs are stand-ins for 0s. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 apply() is part of a family of functions that offer a wide array of uses. You can learn more about the apply() family here or here. apply(a, MARGIN = c(2, 3), FUN = sum) ## LayDimName ## ColDimName l1 l2 ## c1 6 42 ## c2 15 51 ## c3 24 60 ## c4 33 69 Here’s a. a ## , , LayDimName = l1 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 1 4 7 10 ## r2 2 5 8 11 ## r3 3 6 9 12 ## ## , , LayDimName = l2 ## ## ColDimName ## RowDimName c1 c2 c3 c4 ## r1 13 16 19 22 ## r2 14 17 20 23 ## r3 15 18 21 24 The reshape2 package is a precursor to the tidyr package (i.e., one of the core tidyverse packages). The reshape2::melt() function is a quick way to transform the 3-dimensional a matrix into a tidy data frame. a %&gt;% reshape2::melt() ## RowDimName ColDimName LayDimName value ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## 11 r2 c4 l1 11 ## 12 r3 c4 l1 12 ## 13 r1 c1 l2 13 ## 14 r2 c1 l2 14 ## 15 r3 c1 l2 15 ## 16 r1 c2 l2 16 ## 17 r2 c2 l2 17 ## 18 r3 c2 l2 18 ## 19 r1 c3 l2 19 ## 20 r2 c3 l2 20 ## 21 r3 c3 l2 21 ## 22 r1 c4 l2 22 ## 23 r2 c4 l2 23 ## 24 r3 c4 l2 24 We have an alternative if you wanted to stay within the tidyverse. To my knowledge, the fastest way to make the transformation is to first use as.tbl_cube() and follow that up with as_tibble(). The as.tbl_cube() function will convert the a matrix into a tbl_cube. We will use the met_name argument to determine the name of the measure assessed in the data. Since the default is for as.tbl_cube() to name the measure name as ., it seemed value was a more descriptive choice. We’ll then use the as_tibble() function to convert our tbl_cube object into a tidy tibble. a %&gt;% as.tbl_cube(met_name = &quot;value&quot;) %&gt;% as_tibble() ## # A tibble: 24 x 4 ## RowDimName ColDimName LayDimName value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 r1 c1 l1 1 ## 2 r2 c1 l1 2 ## 3 r3 c1 l1 3 ## 4 r1 c2 l1 4 ## 5 r2 c2 l1 5 ## 6 r3 c2 l1 6 ## 7 r1 c3 l1 7 ## 8 r2 c3 l1 8 ## 9 r3 c3 l1 9 ## 10 r1 c4 l1 10 ## # … with 14 more rows Notice how the first three columns are returned as characters instead of factors. If you really wanted those to be factors, you could always follow up the code with mutate_if(is.character, as.factor). 3.7 Programming in R It’s worthy to note that this project was done with R Markdown, which is an alternative to an R script. As Grolemund and Wickham point out R Markdown integrates a number of R packages and external tools. This means that help is, by-and-large, not available through ?. Instead, as you work through this chapter, and use R Markdown in the future, keep these resources close to hand: R Markdown Cheat Sheet: Help &gt; Cheatsheets &gt; R Markdown Cheat Sheet, R Markdown Reference Guide: Help &gt; Cheatsheets &gt; R Markdown Reference Guide. Both cheatsheets are also available at http://rstudio.com/cheatsheets. I also strongly recommend checking out R Notebooks, which is a kind of R Markdown document but with a few bells a whistles that make it more useful for working scientists. You can learn more about it here and here. And for a more comprehensive overview, check out Xie, Allaire, and Grolemund’s R Markdown: The Definitive Guide. 3.7.1 Variable names in R. Kruschke prefers to use camelBack notation for his variable and function names. Though I initially hated it, I’ve been moving in the direction of snake_case. It seems easier to read_prose_in_snake_case than it is to readProseInCamelBack. To each their own. 3.7.2 Running a program. See R4DS Chapter 8 for a nice overview on working directories within the context of an RStudio project. 3.7.3 Programming a function. Here’s our simple a_sq_plus_b function. a_sq_plus_b &lt;- function(a, b = 1) { c &lt;- a^2 + b return(c) } If you explicitly denote your arguments, everything works fine. a_sq_plus_b(a = 3, b = 2) ## [1] 11 Keep things explicit and you can switch up the order of the arguments. a_sq_plus_b(b = 2, a = 3) ## [1] 11 But here’s what happens when you are less explicit. # this a_sq_plus_b(3, 2) ## [1] 11 # is not the same as this a_sq_plus_b(2, 3) ## [1] 7 Since we gave b a default value, we can be really lazy. a_sq_plus_b(a = 2) ## [1] 5 But we can’t be lazy with a. This a_sq_plus_b(b = 1) yielded this warning on my computer: “Error in a_sq_plus_b(b = 1) : argument”a&quot; is missing, with no default&quot;. If we’re completely lazy, a_sq_plus_b() presumes our sole input value is for the a argument and it uses the default value of 1 for b. a_sq_plus_b(2) ## [1] 5 The lesson is important because it’s good practice to familiarize yourself with the defaults of the functions you use in statistics and data analysis, more generally. 3.7.4 Conditions and loops. Here’s our starting point for if() and else(). if(x &lt;= 3){ # if x is less than or equal to 3 show(&quot;small&quot;) # display the word &quot;small&quot; } else { # otherwise show(&quot;big&quot;) # display the word &quot;big&quot; } # end of ’else’ clause ## Warning in if (x &lt;= 3) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;small&quot; Yep, this is no good. if (x &lt;= 3) {show(&quot;small&quot;)} else {show(&quot;big&quot;)} On my computer, it returned this message: “the condition has length &gt; 1 and only the first element will be used[1]”small&quot; Error: unexpected ‘else’ in “else”&quot;. Here we use the loop. for (count_down in 5:1) { show(count_down) } ## [1] 5 ## [1] 4 ## [1] 3 ## [1] 2 ## [1] 1 for (note in c(&quot;do&quot;, &quot;re&quot;, &quot;mi&quot;)) { show(note) } ## [1] &quot;do&quot; ## [1] &quot;re&quot; ## [1] &quot;mi&quot; It’s also useful to understand how to use the ifelse() function within the context of a data frame. Recall hos x is a data frame. x &lt;- tibble(x = 1:5) x ## # A tibble: 5 x 1 ## x ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 ## 4 4 ## 5 5 We can use the mutate() function to make a new variable, size, which is itself a function of the original variable, x. We’ll use the ifelse() function to return “small” if x &lt;= 3, but to return “big” otherwise. x %&gt;% mutate(size = ifelse(x &lt;= 3, &quot;small&quot;, &quot;big&quot;)) ## # A tibble: 5 x 2 ## x size ## &lt;int&gt; &lt;chr&gt; ## 1 1 small ## 2 2 small ## 3 3 small ## 4 4 big ## 5 5 big You should also know there’s a dplyr alternative, called if_else(). It works quite similarily, but is stricter about type consistency. If you ever get into a situation where you need to do many ifelse() statements or a many-layered ifelse() statement, you might check out dplyr::case_when(). 3.7.5 Measuring processing time. This will be nontrivial to consider in your Bayesian career. Here’s the loop. start_time &lt;- proc.time() y &lt;- vector(mode = &quot;numeric&quot;, length = 1.0E6) for (i in 1:1.0E6) {y[i] &lt;- log(i)} stop_time &lt;- proc.time() elapsed_time_loop &lt;- stop_time - start_time show(elapsed_time_loop) ## user system elapsed ## 1.144 0.046 1.262 Now we use a vector. start_time &lt;- proc.time() y &lt;- log(1:1.0E6) stop_time &lt;- proc.time() elapsed_time_vector &lt;- stop_time - start_time show(elapsed_time_vector) ## user system elapsed ## 0.022 0.004 0.028 Here we compare the two times. elapsed_time_vector[1] / elapsed_time_loop[1] ## user.self ## 0.01923077 For my computer, the vectorized approach took about 1.9% the time the loop approach did. When using R, avoid loops for vectorized approaches whenever possible. As an alternative, when I’m doing analyses like these, I tend to just use Sys.time(). I’m not going to walk them out, here. But as we go along, you might notice I sometimes use functions from the purrr::map() family in places where Kruschke used loops. I think they’re pretty great. 3.7.6 Debugging. This should be no surprise, by now, but in addition to Kruschke’s good advice, I also recommend checking out R4DS. I reference it often. 3.8 Graphical plots: Opening and saving For making and saving plots with ggplot2, I recommend reviewing R4DS Chapters 3 and 28. Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.5.1 utf8_1.1.4 ## [3] ggstance_0.3.2 tidyselect_0.2.5 ## [5] htmlwidgets_1.5 grid_3.6.0 ## [7] munsell_0.5.0 codetools_0.2-16 ## [9] DT_0.9 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-6 ## [13] colorspace_1.4-1 knitr_1.23 ## [15] rstudioapi_0.10 stats4_3.6.0 ## [17] bayesplot_1.7.0 labeling_0.3 ## [19] rstan_2.19.2 bridgesampling_0.7-2 ## [21] coda_0.19-3 vctrs_0.2.0 ## [23] generics_0.0.2 xfun_0.10 ## [25] R6_2.4.0 markdown_1.1 ## [27] HDInterval_0.2.0 assertthat_0.2.1 ## [29] promises_1.1.0 scales_1.0.0 ## [31] gtable_0.3.0 processx_3.4.1 ## [33] rlang_0.4.0 zeallot_0.1.0 ## [35] lazyeval_0.2.2 broom_0.5.2 ## [37] inline_0.3.15 yaml_2.2.0 ## [39] reshape2_1.4.3 abind_1.4-5 ## [41] modelr_0.1.4 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.5 ## [45] httpuv_1.5.2 rsconnect_0.8.15 ## [47] tools_3.6.0 bookdown_0.12 ## [49] ellipsis_0.3.0 ggridges_0.5.1 ## [51] Rcpp_1.0.2 plyr_1.8.4 ## [53] base64enc_0.1-3 ps_1.3.0 ## [55] prettyunits_1.0.2 openssl_1.4.1 ## [57] zoo_1.8-6 haven_2.1.0 ## [59] magrittr_1.5 colourpicker_1.0 ## [61] packrat_0.5.0 matrixStats_0.55.0 ## [63] hms_0.4.2 shinyjs_1.0 ## [65] mime_0.7 evaluate_0.14 ## [67] arrayhelpers_1.0-20160527 xtable_1.8-4 ## [69] shinystan_2.5.0 readxl_1.3.1 ## [71] gridExtra_2.3 rstantools_2.0.0 ## [73] compiler_3.6.0 crayon_1.3.4 ## [75] StanHeaders_2.19.0 htmltools_0.4.0 ## [77] later_1.0.0 lubridate_1.7.4 ## [79] MASS_7.3-51.4 Matrix_1.2-17 ## [81] cli_1.1.0 parallel_3.6.0 ## [83] igraph_1.2.4.1 pkgconfig_2.0.3 ## [85] xml2_1.2.0 svUnit_0.7-12 ## [87] dygraphs_1.1.1.6 rvest_0.3.4 ## [89] callr_3.3.2 digest_0.6.21 ## [91] rmarkdown_1.13 cellranger_1.1.0 ## [93] curl_4.2 shiny_1.3.2 ## [95] gtools_3.8.1 lifecycle_0.1.0 ## [97] nlme_3.1-139 jsonlite_1.6 ## [99] askpass_1.1 fansi_0.4.0 ## [101] pillar_1.4.2 lattice_0.20-38 ## [103] loo_2.1.0 httr_1.4.0 ## [105] pkgbuild_1.0.5 glue_1.3.1 ## [107] xts_0.11-2 shinythemes_1.1.2 ## [109] stringi_1.4.3 "],
["what-is-this-stuff-called-probability.html", "4 What is This Stuff Called Probability? 4.1 The set of all possible events 4.2 Probability: Outside or inside the head 4.3 Probability distributions 4.4 Two-way distributions Reference Session info", " 4 What is This Stuff Called Probability? Inferential statistical techniques assign precise measures to our uncertainty about possibilities. Uncertainty is measured in terms of probability, and therefore we must establish the properties of probability before we can make inferences about it. This chapter introduces the basic ideas of probability. (p. 71, emphasis in the original) 4.1 The set of all possible events This snip from page 72 is important (emphasis in the original): Whenever we ask about how likely an outcome is, we always ask with a set of possible outcomes in mind. This set exhausts all possible outcomes, and the outcomes are all mutually exclusive. This set is called the sample space. 4.2 Probability: Outside or inside the head It’s worthwhile to quote this section in full. Sometimes we talk about probabilities of outcomes that are “out there” in the world. The face of a flipped coin is such an outcome: We can observe the flip, and the probability of coming up heads can be estimated by observing several flips. But sometimes we talk about probabilities of things that are not so clearly “out there,” and instead are just possible beliefs “inside the head.” Our belief about the fairness of a coin is an example of something inside the head. The coin may have an intrinsic physical bias, but now I am referring to our belief about the bias. Our beliefs refer to a space of mutually exclusive and exhaustive possibilities. It might be strange to say that we randomly sample from our beliefs, like we randomly sample from a sack of coins. Nevertheless, the mathematical properties of probabilities outside the head and beliefs inside the head are the same in their essentials, as we will see. (pp. 73–74, emphasis in the original) 4.2.1 Outside the head: Long-run relative frequency. For events outside the head, it’s intuitive to think of probability as being the long-run relative frequency of each possible outcome… We can determine the long-run relative frequency by two different ways. One way is to approximate it by actually sampling from the space many times and tallying the number of times each event happens. A second way is by deriving it mathematically. These two methods are now explored in turn. (p. 74) 4.2.1.1 Simulating a long-run relative frequency. Before we try coding the simulation, we’ll first load the tidyverse. library(tidyverse) Now run the simulation. n &lt;- 500 # specify the total number of flips p_heads &lt;- 0.5 # specify underlying probability of heads # Kruschke reported this was the seed he used at the top of page 94 set.seed(47405) # here we use that seed to flip a coin n times and compute the running proportion of heads at each flip. # we generate a random sample of n flips (heads = 1, tails = 0) d &lt;- tibble(flip_sequence = sample(x = c(0, 1), prob = c(1 - p_heads, p_heads), size = n, replace = T), r = cumsum(flip_sequence), n = 1:n) %&gt;% mutate(run_prop = r / n) end_prop &lt;- d %&gt;% select(run_prop) %&gt;% slice(n()) %&gt;% round(digits = 3) %&gt;% pull() Now we’re ready to make Figure 4.1. d %&gt;% filter(n &lt; 1000) %&gt;% # this step cuts down on the time it takes to make the plot ggplot(aes(x = n, y = run_prop)) + geom_hline(yintercept = .5, color = &quot;white&quot;) + geom_line(color = &quot;grey50&quot;) + geom_point(color = &quot;grey50&quot;, alpha = 1/4) + scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 200, 500)) + coord_cartesian(xlim = 1:500, ylim = 0:1) + labs(title = &quot;Running proportion of heads&quot;, subtitle = paste(&quot;Our end proportion =&quot;, end_prop), x = &quot;Flip number&quot;, y = &quot;Proportion of heads&quot;) + theme(panel.grid = element_blank()) 4.2.1.2 Deriving a long-run relative frequency. Sometimes, when the situation is simple enough mathematically, we can derive the exact long-run relative frequency. The case of the fair coin is one such simple situation. The sample space of the coin consists of two possible outcomes, head and tail. By the assumption of fairness, we know that each outcome is equally likely. Therefore, the long-run relative frequency of heads should be exactly one out of two, i.e., 1/2, and the long-run relative frequency of tails should also be exactly 1/2. (p. 76) 4.2.2 Inside the head: Subjective belief. To specify our subjective beliefs, we have to specify how likely we think each possible outcome is. It can be hard to pin down mushy intuitive beliefs. In the next section, we explore one way to “calibrate” subjective beliefs, and in the subsequent section we discuss ways to mathematically describe degrees of belief. (p. 76) 4.2.3 Probabilities assign numbers to possibilities. In general, a probability, whether it’s outside the head or inside the head, is just a way of assigning numbers to a set of mutually exclusive possibilities. The numbers, called “probabilities,” merely need to satisfy three properties (Kolmogorov, 1956): A probability value must be nonnegative (i.e., zero or positive). The sum of the probabilities across all events in the entire sample space must be 1.0 (i.e., one of the events in the space must happen, otherwise the space does not exhaust all possibilities). For any two mutually exclusive events, the probability that one or the other occurs is the sum of their individual probabilities. For example, the probability that a fair six-sided die comes up 3-dots or 4-dots is 1/6 + 1/6 = 2/6. Any assignment of numbers to events that respects those three properties will also have all the properties of probabilities that we will discuss below. (pp. 77–78, emphasis in the original) 4.3 Probability distributions “A probability distribution is simply a list of all possible outcomes and their corresponding probabilities” (p. 78, emphasis in the original) 4.3.1 Discrete distributions: Probability mass. When the sample space consists of discrete outcomes, then we can talk about the probability of each distinct outcome. For example, the sample space of a flipped coin has two discrete outcomes, and we talk about the probability of head or tail… For continuous outcome spaces, we can discretize the space into a finite set of mutually exclusive and exhaustive “bins.” (p. 78, emphasis in the original) In order to recreate Figure 4.2, we need to generate the heights data. In his HtWtDataDenerator.R script, Kruschke provided the code for a function that will generate height data of the kind in his text. Here is the code: HtWtDataGenerator &lt;- function(n_subj, rndsd = NULL, male_prob = 0.50) { # Random height, weight generator for males and females. Uses parameters from # Brainard, J. &amp; Burmaster, D. E. (1992). Bivariate distributions for height and # weight of men and women in the United States. Risk Analysis, 12(2), 267-275. # Kruschke, J. K. (2011). Doing Bayesian data analysis: # A Tutorial with R and BUGS. Academic Press / Elsevier. # Kruschke, J. K. (2014). Doing Bayesian data analysis, 2nd Edition: # A Tutorial with R, JAGS and Stan. Academic Press / Elsevier. # require(MASS) # Specify parameters of multivariate normal (MVN) distributions. # Men: HtMmu &lt;- 69.18 HtMsd &lt;- 2.87 lnWtMmu &lt;- 5.14 lnWtMsd &lt;- 0.17 Mrho &lt;- 0.42 Mmean &lt;- c(HtMmu, lnWtMmu) Msigma &lt;- matrix(c(HtMsd^2, Mrho * HtMsd * lnWtMsd, Mrho * HtMsd * lnWtMsd, lnWtMsd^2), nrow = 2) # Women cluster 1: HtFmu1 &lt;- 63.11 HtFsd1 &lt;- 2.76 lnWtFmu1 &lt;- 5.06 lnWtFsd1 &lt;- 0.24 Frho1 &lt;- 0.41 prop1 &lt;- 0.46 Fmean1 &lt;- c(HtFmu1, lnWtFmu1) Fsigma1 &lt;- matrix(c(HtFsd1^2, Frho1 * HtFsd1 * lnWtFsd1, Frho1 * HtFsd1 * lnWtFsd1, lnWtFsd1^2), nrow = 2) # Women cluster 2: HtFmu2 &lt;- 64.36 HtFsd2 &lt;- 2.49 lnWtFmu2 &lt;- 4.86 lnWtFsd2 &lt;- 0.14 Frho2 &lt;- 0.44 prop2 &lt;- 1 - prop1 Fmean2 &lt;- c(HtFmu2, lnWtFmu2) Fsigma2 &lt;- matrix(c(HtFsd2^2, Frho2 * HtFsd2 * lnWtFsd2, Frho2 * HtFsd2 * lnWtFsd2, lnWtFsd2^2), nrow = 2) # Randomly generate data values from those MVN distributions. if (!is.null(rndsd)) {set.seed(rndsd)} data_matrix &lt;- matrix(0, nrow = n_subj, ncol = 3) colnames(data_matrix) &lt;- c(&quot;male&quot;, &quot;height&quot;, &quot;weight&quot;) maleval &lt;- 1; femaleval &lt;- 0 # arbitrary coding values for (i in 1:n_subj) { # Flip coin to decide sex sex = sample(c(maleval, femaleval), size = 1, replace = TRUE, prob = c(male_prob, 1 - male_prob)) if (sex == maleval) {datum &lt;- MASS::mvrnorm(n = 1, mu = Mmean, Sigma = Msigma)} if (sex == femaleval) { Fclust = sample(c(1, 2), size = 1, replace = TRUE, prob = c(prop1, prop2)) if (Fclust == 1) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean1, Sigma = Fsigma1)} if (Fclust == 2) {datum &lt;- MASS::mvrnorm(n = 1, mu = Fmean2, Sigma = Fsigma2)} } data_matrix[i, ] = c(sex, round(c(datum[1], exp(datum[2])), 1)) } return(data_matrix) } # end function Now we have the HtWtDataGenerator() function, all we need to do is determine how many values are generated and how probable we want the values to be based on those from men. These are controlled by the n_subj and male_prob parameters. set.seed(4) d &lt;- HtWtDataGenerator(n_subj = 10000, male_prob = .5) %&gt;% as_tibble() %&gt;% mutate(person = 1:n()) d %&gt;% head() ## # A tibble: 6 x 4 ## male height weight person ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 76 222. 1 ## 2 0 59.5 190 2 ## 3 0 60.2 118. 3 ## 4 1 64.1 138. 4 ## 5 1 69.3 148. 5 ## 6 1 69.1 166. 6 For Figure 4.2, we’ll make extensive use of the case_when() syntax, which you can learn more about from hrbrmstr’s Making a Case for case_when. d_bin &lt;- d %&gt;% mutate(bin = case_when( height &lt; 51 ~ 51, between(height, 51, 53) ~ 53, between(height, 53, 55) ~ 55, between(height, 55, 57) ~ 57, between(height, 57, 59) ~ 59, between(height, 59, 61) ~ 61, between(height, 61, 63) ~ 63, between(height, 63, 65) ~ 65, between(height, 65, 67) ~ 67, between(height, 67, 69) ~ 69, between(height, 69, 71) ~ 71, between(height, 71, 73) ~ 73, between(height, 73, 75) ~ 75, between(height, 75, 77) ~ 77, between(height, 77, 79) ~ 79, between(height, 79, 81) ~ 71, between(height, 81, 83) ~ 83, height &gt; 83 ~ 85) ) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 1) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 2), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) Because we’re simulating and we don’t know what seed number Kruschke used for his plot, ours will differ a little from his. But the overall pattern is the same. It’s a little less work to make Figure 4.2.b. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), binwidth = 2, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = c(0, .04, .08)) + coord_cartesian(xlim = 51:83) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) Our data binning approach for Figure 4.2.c will be a little different than what we did, above. Here we’ll make our bins with the round() function. d_bin &lt;- d %&gt;% mutate(bin = round(height, digits = 0)) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - 0.5) d %&gt;% ggplot(aes(x = height, y = person)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 51, to = 83, by = 1), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Person #&quot;) + theme(panel.grid = element_blank()) However, our method for Figure 4.2.d will be like what we did, before. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = 1, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = c(0, .04, .08)) + coord_cartesian(xlim = 51:83) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) The probability of a discrete outcome, such as the probability of falling into an interval on a continuous scale, is referred to as a probability mass. Loosely speaking, the term “mass” refers the amount of stuff in an object. When the stuff is probability and the object is an interval of a scale, then the mass is the proportion of the outcomes in the interval. (p. 80, emphasis in the original) 4.3.2 Continuous distributions: Rendezvous with density. If you think carefully about a continuous outcome space, you realize that it becomes problematic to talk about the probability of a specific value on the continuum, as opposed to an interval on the continuum… Therefore, what we will do is make the intervals infinitesimally narrow, and instead of talking about the infinitesimal probability mass of each infinitesimal interval, we will talk about the ratio of the probability mass to the interval width. That ratio is called the probability density. Loosely speaking, density is the amount of stuff per unit of space it takes up. Because we are measuring amount of stuff by its mass, then density is the mass divided by the amount space it occupies. (p. 80, emphasis in the original) To make Figure 4.3, we’ll need new data. set.seed(4) d &lt;- tibble(height = rnorm(1e4, mean = 84, sd = .1)) %&gt;% mutate(door = 1:n()) d %&gt;% head() ## # A tibble: 6 x 2 ## height door ## &lt;dbl&gt; &lt;int&gt; ## 1 84.0 1 ## 2 83.9 2 ## 3 84.1 3 ## 4 84.1 4 ## 5 84.2 5 ## 6 84.1 6 To make the bins for our version of Figure 4.3.a, we could use the case_when() approach from above. However, that would require some tedious code. Happily, we have an alternative in the santoku package. We can use the santoku::chop() function to discretize our height values. Here we’ll walk through the first part. # devtools::install_github(&quot;hughjonesd/santoku&quot;) library(santoku) d_bin &lt;- d %&gt;% mutate(bin = chop(height, breaks = seq(from = 83.6, to = 84.4, length.out = 31), labels = seq(from = 83.6, to = 84.4, length.out = 31)[-1])) head(d_bin) ## # A tibble: 6 x 3 ## height door bin ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 84.0 1 84.0266666666667 ## 2 83.9 2 83.9466666666667 ## 3 84.1 3 84.1066666666667 ## 4 84.1 4 84.08 ## 5 84.2 5 84.1866666666667 ## 6 84.1 6 84.08 We’ve labeled our bin levels by their upper bounds. Note how they are saved as factors. To make use of those values in our plot, we’ll need to convert them to numerals. Here we make that conversion and complete the data wrangling. d_bin &lt;- d_bin %&gt;% mutate(bin = as.character(bin) %&gt;% as.double()) %&gt;% group_by(bin) %&gt;% summarise(n = n()) %&gt;% mutate(height = bin - (83.62667 - 83.6) / 2) head(d_bin) ## # A tibble: 6 x 3 ## bin n height ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 83.7 5 83.6 ## 2 83.7 5 83.7 ## 3 83.7 7 83.7 ## 4 83.7 24 83.7 ## 5 83.8 37 83.7 ## 6 83.8 86 83.8 Now we plot. d %&gt;% ggplot(aes(x = height, y = door)) + geom_point(size = 3/4, color = &quot;grey67&quot;, alpha = 1/2) + geom_vline(xintercept = seq(from = 83.6, to = 84.4, length.out = 31), linetype = 3, color = &quot;grey33&quot;) + geom_text(data = d_bin, aes(y = 5000, label = n, angle = 90), size = 3.25) + scale_y_continuous(breaks = c(0, 5000, 10000)) + labs(title = &quot;Total N = 10,000&quot;, x = &quot;Height (inches)&quot;, y = &quot;Door #&quot;) + theme(panel.grid = element_blank()) Figure 4.3.b is a breeze. d %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = stat(density)), boundary = 0, binwidth = .025, fill = &quot;grey67&quot;, color = &quot;grey92&quot;, size = 1/8) + scale_y_continuous(breaks = 0:4) + coord_cartesian(xlim = c(83.6, 84.4)) + labs(x = &quot;Height (inches)&quot;, y = &quot;Probability density&quot;) + theme(panel.grid = element_blank()) 4.3.2.1 Properties of probability density functions. 4.3.2.2 The normal probability density function. We’ll use dnorm() again to make our version of Figure 4.4. tibble(x = seq(from = -.8, to = .8, by = .02)) %&gt;% mutate(p = dnorm(x, mean = 0, sd = .2)) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = p), color = &quot;grey50&quot;, size = 1.25) + geom_linerange(aes(ymin = 0, ymax = p), size = 1/3) + coord_cartesian(xlim = c(-.61, .61)) + labs(title = &quot;Normal probability density&quot;, subtitle = expression(paste(mu, &quot; = 0 and &quot;, sigma, &quot; = 0.2&quot;)), y = &quot;p(x)&quot;) + theme(panel.grid = element_blank()) The equation for the normal probability density follows the form \\[ p(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\Bigg ( - \\frac{1}{2} \\bigg [ \\frac{x - \\mu}{\\sigma}^2 \\bigg ] \\Bigg ), \\] where \\(\\mu\\) governs the mean and \\(\\sigma\\) governs the standard deviation. 4.3.3 Mean and variance of a distribution. The mean of a probability distribution is also called the expected value, which follow the form \\[E[x] = \\sum_x p(x) x\\] when \\(x\\) is discrete. For continuous \\(x\\) values, the formula is \\[E[x] = \\int \\text d x \\; p(x) x.\\] The variance is defined as the mean squared deviation from the mean, \\[\\text{var}_x = \\int \\text d x \\; p(x) (x - E[x])^2.\\] If you take the square root of the variance, you get the standard deviation. 4.3.4 Highest density interval (HDI). The HDI indicates which points of a distribution are most credible, and which cover most of the distribution. Thus, the HDI summarizes the distribution by specifying an interval that spans most of the distribution, say 95% of it, such that every point inside the interval has higher credibility than any point outside the interval. (p. 87) In chapter 10 (p. 294), Kruschke briefly mentions his HDIofICDF() function, the code for which you can find in his DBDA2E-utilities.R file. It’s a handy function which we’ll put to use from time to time. Here’s a mild reworking of his code. hdi_of_icdf &lt;- function(name, width = .95, tol = 1e-8, ... ) { # Arguments: # `name` is R&#39;s name for the inverse cumulative density function # of the distribution. # `width` is the desired mass of the HDI region. # `tol` is passed to R&#39;s optimize function. # Return value: # Highest density iterval (HDI) limits in a vector. # Example of use: For determining HDI of a beta(30, 12) distribution, type # `hdi_of_icdf(qbeta, shape1 = 30, shape2 = 12)` # Notice that the parameters of the `name` must be explicitly stated; # e.g., `hdi_of_icdf(qbeta, 30, 12)` does not work. # Adapted and corrected from Greg Snow&#39;s TeachingDemos package. incredible_mass &lt;- 1.0 - width interval_width &lt;- function(low_tail_prob, name, width, ...) { name(width + low_tail_prob, ...) - name(low_tail_prob, ...) } opt_info &lt;- optimize(interval_width, c(0, incredible_mass), name = name, width = width, tol = tol, ...) hdi_lower_tail_prob &lt;- opt_info$minimum return(c(name(hdi_lower_tail_prob, ...), name(width + hdi_lower_tail_prob, ...))) } Now we already know from the text, and perhaps from prior experience, what the 95% HDIs for the unit normal. But it’s nice to be able to confirm that with a function. h &lt;- hdi_of_icdf(name = qnorm, mean = 0, sd = 1) h ## [1] -1.959964 1.959964 Now we’ve saved those values in h, we can use then to make our version of Figure 4.5.a. tibble(x = seq(from = -3.5, to = 3.5, by = .05)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;=h[1] &amp; x &lt;= h[2]), aes(ymin = 0, ymax = dnorm(x, mean = 0, sd = 1)), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .02, h[2] - .02), y = c(.059, .059)), aes(y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 0, y = .09, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(-3.1, 3.1) + labs(y = &quot;p(x)&quot;) + theme(panel.grid = element_blank()) As far as I could tell, Figure 4.5.b is of a beta distribution, which Kruschke covers in greater detail starting in chapter 6. I got the shape1 and shape2 values from playing around. If you have a more principled approach, do share. But anyway, we can use our hdi_of_icdf() funciton to ge the correct values. h &lt;- hdi_of_icdf(name = qbeta, shape1 = 15, shape2 = 4) h ## [1] 0.6103498 0.9507510 Let’s put those h values to work. tibble(x = seq(from = 0, to = 1, by = .01)) %&gt;% ggplot(aes(x = x)) + geom_ribbon(aes(ymin = 0, ymax = dbeta(x, shape1 = 15, shape2 = 4)), fill = &quot;grey75&quot;) + geom_ribbon(data = . %&gt;% filter(x &gt;=h[1] &amp; x &lt;= h[2]), aes(ymin = 0, ymax = dbeta(x, shape1 = 15, shape2 = 4)), fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1] + .01, h[2] - .002), y = c(.75, .75)), aes(x = x, y = y), arrow = arrow(length = unit(.2, &quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = .8, y = 1.1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + xlim(.4, 1) + ylab(&quot;p(x)&quot;) + theme(panel.grid = element_blank()) Figure 4.5.c was also a lot of trial and error. It seemed the easiest way to reproduce the shape was to mash two Gaussians together. After playing around with rnorm(), I ended up with this. set.seed(4) d &lt;- tibble(x = c(rnorm(6e5, mean = 1.50, sd = .5), rnorm(4e5, mean = 4.75, sd = .5))) glimpse(d) ## Observations: 1,000,000 ## Variables: 1 ## $ x &lt;dbl&gt; 1.6083774, 1.2287537, 1.9455723, 1.7979903, 2.3178090, 1.84463… As you’ll see, it’s not exactly right. But it’s close enough to give you a sense of what’s going on. But anyway, since we’re working with simulated data rather than an analytic solution, we’ll want to use a powerful convenience function from the tidybayes package. library(tidybayes) Kay’s tidybayes package provides a family of functions for generating point summaries and intervals from draws in a tidy format. These functions follow the naming scheme [median|mean|mode]_[qi|hdi], for example, median_qi, mean_qi, mode_hdi, and so on. The first name (before the _) indicates the type of point summary, and the second name indicates the type of interval. qi yields a quantile interval (a.k.a. equi-tailed interval, central interval, or percentile interval) and hdi yields a highest (posterior) density interval. (from here) Here we’ll use mode_hdi() to compute the HDIs and put them in a tibble. We’ll be using a lot of mode_hdi() in this project. h &lt;- d %&gt;% mode_hdi() h ## # A tibble: 2 x 6 ## x .lower .upper .width .point .interval ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.50 0.458 2.55 0.95 mode hdi ## 2 1.50 3.82 5.68 0.95 mode hdi Usually, mode_hdi() will return a tibble with just one row. But in this case, since we had a bimodal distribution, it returned two rows—one for each of the two distinct regions. Oh, and in case it wasn’t clear, that first column x is the measure of central tendency—the mode, in this case. Though I acknowledge, it’s a little odd to speak of central tendency in a bimodal distribution. Again, this won’t happen much. In order to fill the bimodal density with the split HDIs, you need to use the density() function to transform the d data to a tibble with the values for the x-axis in an x vector and the corresponding density values in a y vector. dens &lt;- d$x %&gt;% density() %&gt;% with(tibble(x, y)) head(dens) ## # A tibble: 6 x 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.11 0.0000000503 ## 2 -1.09 0.0000000822 ## 3 -1.08 0.000000131 ## 4 -1.06 0.000000201 ## 5 -1.04 0.000000304 ## 6 -1.03 0.000000449 We’re finally ready to plot. Forgive me. It’s a monster. ggplot(data = dens, aes(x = x, y = y)) + geom_ribbon(aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey75&quot;) + # note the use of `pull()`, which extracts the values, rather than return a tibble geom_ribbon(data = dens %&gt;% filter(x &gt; h[1, 2] %&gt;% pull() &amp; x &lt; h[1, 3] %&gt;% pull()), aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey50&quot;) + geom_ribbon(data = dens %&gt;% filter(x &gt; h[2, 2] %&gt;% pull() &amp; x &lt; h[2, 3] %&gt;% pull()), aes(ymin = 0, ymax = y), size = 0, fill = &quot;grey50&quot;) + geom_line(data = tibble(x = c(h[1, 2] %&gt;% pull(), h[1, 3] %&gt;% pull()), y = c(.06, .06)), aes(x = x, y = y), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + geom_line(data = tibble(x = c(h[2, 2] %&gt;% pull(), h[2, 3] %&gt;% pull()), y = c(.06, .06)), aes(x = x, y = y), arrow = arrow(length = unit(.2,&quot;cm&quot;), ends = &quot;both&quot;, type = &quot;closed&quot;), color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 1.5, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + annotate(&quot;text&quot;, x = 4.75, y = .1, label = &quot;95% HDI&quot;, color = &quot;grey92&quot;) + scale_x_continuous(breaks = 0:6, limits = c(0, 6.3)) + scale_y_continuous(&quot;p(x)&quot;, breaks = c(0, .1, .2, .3, .4, .5)) + theme(panel.grid = element_blank()) 4.4 Two-way distributions Here’s a way to compute the marginals from the inner cells of Table 4.1. d1 &lt;- tibble(eye_color = c(&quot;brown&quot;, &quot;blue&quot;, &quot;hazel&quot;, &quot;green&quot;), black = c(.11, .03, .03, .01), brunette = c(.2, .14, .09, .05), red = c(.04, .03, .02, .02), blond = c(.01, .16, .02, .03)) %&gt;% mutate(marginal_eye_color = black + brunette + red + blond) d2 &lt;- d1 %&gt;% summarise_if(is.double, sum) %&gt;% mutate(eye_color = &quot;marginal_hair_color&quot;) %&gt;% select(eye_color, everything()) bind_rows(d1, d2) ## # A tibble: 5 x 6 ## eye_color black brunette red blond marginal_eye_color ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 brown 0.11 0.2 0.04 0.01 0.36 ## 2 blue 0.03 0.14 0.03 0.16 0.36 ## 3 hazel 0.03 0.09 0.02 0.02 0.160 ## 4 green 0.01 0.05 0.02 0.03 0.11 ## 5 marginal_hair_color 0.18 0.48 0.11 0.22 0.99 Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] tidybayes_1.1.0 santoku_0.2.0.9000 forcats_0.4.0 ## [4] stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [7] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ## [10] ggplot2_3.2.1 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.5.1 utf8_1.1.4 ## [3] ggstance_0.3.2 tidyselect_0.2.5 ## [5] htmlwidgets_1.5 grid_3.6.0 ## [7] munsell_0.5.0 codetools_0.2-16 ## [9] DT_0.9 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-6 ## [13] colorspace_1.4-1 knitr_1.23 ## [15] rstudioapi_0.10 stats4_3.6.0 ## [17] bayesplot_1.7.0 labeling_0.3 ## [19] rstan_2.19.2 bridgesampling_0.7-2 ## [21] coda_0.19-3 vctrs_0.2.0 ## [23] generics_0.0.2 xfun_0.10 ## [25] R6_2.4.0 markdown_1.1 ## [27] HDInterval_0.2.0 assertthat_0.2.1 ## [29] promises_1.1.0 scales_1.0.0 ## [31] gtable_0.3.0 processx_3.4.1 ## [33] rlang_0.4.0 zeallot_0.1.0 ## [35] lazyeval_0.2.2 broom_0.5.2 ## [37] inline_0.3.15 yaml_2.2.0 ## [39] reshape2_1.4.3 abind_1.4-5 ## [41] modelr_0.1.4 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.5 ## [45] httpuv_1.5.2 rsconnect_0.8.15 ## [47] tools_3.6.0 bookdown_0.12 ## [49] ellipsis_0.3.0 ggridges_0.5.1 ## [51] Rcpp_1.0.2 plyr_1.8.4 ## [53] base64enc_0.1-3 ps_1.3.0 ## [55] prettyunits_1.0.2 openssl_1.4.1 ## [57] zoo_1.8-6 haven_2.1.0 ## [59] magrittr_1.5 colourpicker_1.0 ## [61] packrat_0.5.0 matrixStats_0.55.0 ## [63] hms_0.4.2 shinyjs_1.0 ## [65] mime_0.7 evaluate_0.14 ## [67] arrayhelpers_1.0-20160527 xtable_1.8-4 ## [69] shinystan_2.5.0 readxl_1.3.1 ## [71] gridExtra_2.3 rstantools_2.0.0 ## [73] compiler_3.6.0 crayon_1.3.4 ## [75] StanHeaders_2.19.0 htmltools_0.4.0 ## [77] later_1.0.0 lubridate_1.7.4 ## [79] MASS_7.3-51.4 Matrix_1.2-17 ## [81] cli_1.1.0 parallel_3.6.0 ## [83] igraph_1.2.4.1 pkgconfig_2.0.3 ## [85] xml2_1.2.0 svUnit_0.7-12 ## [87] dygraphs_1.1.1.6 rvest_0.3.4 ## [89] callr_3.3.2 digest_0.6.21 ## [91] rmarkdown_1.13 cellranger_1.1.0 ## [93] curl_4.2 shiny_1.3.2 ## [95] gtools_3.8.1 lifecycle_0.1.0 ## [97] nlme_3.1-139 jsonlite_1.6 ## [99] askpass_1.1 fansi_0.4.0 ## [101] pillar_1.4.2 lattice_0.20-38 ## [103] loo_2.1.0 httr_1.4.0 ## [105] pkgbuild_1.0.5 glue_1.3.1 ## [107] xts_0.11-2 shinythemes_1.1.2 ## [109] stringi_1.4.3 "],
["bayes-rule.html", "5 Bayes’ Rule 5.1 Bayes’ rule 5.2 Applied to parameters and data 5.3 Complete examples: Estimating bias in a coin 5.4 5.4. Why Bayesian inference can be difficult Reference Session info", " 5 Bayes’ Rule “Bayes’ rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data” (pp. 99–100). 5.1 Bayes’ rule Thomas Bayes (1702-1761) was a mathematician and Presbyterian minister in England. His famous theorem was published posthumously in 1763, thanks to the extensive editorial efforts of his friend, Richard Price (Bayes &amp; Price, 1763). The simple rule has vast ramifications for statistical inference, and therefore as long as his name is attached to the rule, we’ll continue to see his name in textbooks. But Bayes himself probably was not fully aware of these ramifications, and many historians argue that it is Bayes’ successor, Pierre-Simon Laplace (1749-1827), whose name should really label this type of analysis, because it was Laplace who independently rediscovered and extensively developed the methods (e.g., Dale, 1999; McGrayne, 2011). (p. 100) I do recommend checking out McGrayne’s book It’s an easy and entertaining read. For a sneak preview, why not listen to her discuss the main themes she covered in her book? 5.1.1 Derived from definitions of conditional probability. With equations 5.5 and 5.6, Kruschke gave us Bayes’ rule in terms of \\(c\\) and \\(r\\). Equation 5.5 was \\[p(c|r) = \\frac{p(r|c)p(c)}{p(r)}.\\] Since \\(p(r) = \\sum_{c^*}p(r|c^*)p(c^*)\\), we can re-express that as Equation 5.6: \\[p(c|r) = \\frac{p(r|c)p(c)}{\\sum_{c^*}p(r|c^*)p(c^*)},\\] where \\(c^*\\) “in the denominator is a variable that takes on all possible values” of \\(c\\) (p. 101). 5.2 Applied to parameters and data Here we get those equations re-expressed in the terms data analysts tend to think with, parameters (i.e., \\(\\theta\\)) and data (i.e., \\(D\\)). \\[ \\begin{align*} p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\;\\; \\text{and since} \\\\ p(D) &amp; = \\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*) \\;\\; \\text{it&#39;s also the case that} \\\\ p(\\theta|D) &amp; = \\frac{p(D|\\theta)p(\\theta)}{\\sum\\limits_{\\theta^*}p(D|\\theta^*)p(\\theta^*)}. \\end{align*} \\] As in the previous section where we spoke in terms of \\(r\\) and \\(c\\), our updated \\(\\theta^*\\) notation is meant to indicate all possible values of \\(\\theta\\). For practice, it’s worth repeating how Kruschke broke this down with Equation 5.7, \\[ \\underbrace{p(\\theta|D)}_\\text{posterior} \\; = \\; \\underbrace{p(D|\\theta)}_\\text{likelihood} \\;\\; \\underbrace{p(\\theta)}_\\text{prior} \\; / \\; \\underbrace{p(D)}_\\text{evidence}. \\] The “prior,” \\(p(\\theta)\\), is the credibility of the \\(\\theta\\) values without the data \\(D\\). The “posterior,” \\(p(\\theta|D)\\), is the credibility of \\(\\theta\\) values with the data \\(D\\) taken into account. The “likelihood,” \\(p(D|\\theta)\\), is the probability that the data could be generated by the model with parameter value \\(\\theta\\). The “evidence” for the model, \\(p(D)\\), is the overall probability of the data according to the model, determined by averaging across all possible parameter values weighted by the strength of belief in those parameter values. (pp. 106–107) And don’t forget, “evidence” is short “marginal likelihood,” which is the term we’ll use in some of our code, below. 5.3 Complete examples: Estimating bias in a coin Here’s a way to make Figure 5.1.a. library(tidyverse) tibble(theta = seq(from = 0, to = 1, by = .1), prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% ggplot(aes(x = theta, y = prior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Prior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) If you were curious, it is indeed the case that those prior values sum to 1. tibble(prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% summarise(s = sum(prior)) ## # A tibble: 1 x 1 ## s ## &lt;dbl&gt; ## 1 1 If we follow Kruschke’s equation 5.10 (i.e., the Bernoulli function) closely, we can express it as a function in R. bernoulli &lt;- function(theta, y){ return(theta^y * (1 - theta)^(1 - y)) } To get a sense of how it works, consider a single coin flip of heads when heads is considered a successful trial. We’ll call the single successful trial y = 1. We can use our custom bernoulli() function to compute the likelihood of different values of \\(\\theta\\). We’ll look at 11 candidate \\(\\theta\\) values, which we’ll call theta_sequence. theta_sequence &lt;- seq(from = 0, to = 1, by = .1) bernoulli(theta = theta_sequence, y = 1) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Notice how our theta_sequence corresponds nicely with the sequence of \\(\\theta\\) values on the x-axes of Figure 5.1. We can combine theta_sequence and our bernoulli() function to make the middle panel of Figure 5.1 tibble(x = theta_sequence) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% ggplot(aes(x = x, y = likelihood)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Likelihood&quot;, x = expression(theta), y = expression(paste(&quot;p(D|&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) In order to compute \\(p(D)\\) (i.e., the evidence or the marginal likelihood), we’ll need to multiply our respective prior and likelihood values for each point in our theta sequence and then sum all that up. That sum will be our marginal likelihood. With cleared up, we can make Figure 5.1.c. tibble(theta = theta_sequence, prior = c(seq(from = 0, to = .2, length.out = 6), seq(from = .16, to = 0, length.out = 5))) %&gt;% mutate(likelihood = bernoulli(theta = theta_sequence, y = 1)) %&gt;% mutate(marginal_likelihood = sum(prior * likelihood)) %&gt;% mutate(posterior = (prior * likelihood) / marginal_likelihood) %&gt;% ggplot(aes(x = theta, y = posterior)) + geom_col(width = .025, color = &quot;grey50&quot;, fill = &quot;grey50&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(title = &quot;Posterior&quot;, x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;|D)&quot;))) + theme(panel.grid = element_blank()) 5.3.1 Influence of sample size on the posterior. In order to follow along with this section, we’re going to have to update our Bernoulli likelihood function so it can accommodate more than a single trial. We’ll anticipate chapter 6 and call our more general function the bernoulli_likelihood(). bernoulli_likelihood &lt;- function(theta, data) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) n &lt;- length(data) return(theta^sum(data) * (1 - theta)^(n - sum(data))) } Here’s the work required to make our version of the left portion of Figure 5.2. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) We’ll follow the same procedure to make the right portion of Figure 5.2. The only difference is how we switched from small_data to large_data. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) With just an \\(N = 40\\), the likelihood already dominated the posterior. But this is also a function of our fairly gentle prior. 5.3.2 Influence of prior on the posterior. It’s not immediately obvious how Kruschke made his prior distributions for Figure 5.3. However, hidden away in his BernGridExample.R file he indicated that to get the distribution for the left side of Figure 5.3, you simply raise the prior from the left of Figure 5.2 to the 0.1 power. small_data &lt;- rep(0:1, times = c(3, 1)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^0.1) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = small_data)) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) The trick is similar for the right half of Figure 5.3. large_data &lt;- rep(0:1, times = c(30, 10)) tibble(theta = seq(from = 0, to = 1, by = .001), Prior = c(seq(from = 0, to = 1, length.out = 501), seq(from = 0.998, to = 0, length.out = 500))) %&gt;% mutate(Prior = Prior / sum(Prior), Likelihood = bernoulli_likelihood(theta = theta, data = large_data)) %&gt;% # here&#39;s the important line of code mutate(Prior = Prior^10) %&gt;% mutate(marginal_likelihood = sum(Prior * Likelihood)) %&gt;% mutate(Posterior = (Prior * Likelihood) / marginal_likelihood) %&gt;% select(theta, Prior, Likelihood, Posterior) %&gt;% gather(key, value, -theta) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;))) %&gt;% ggplot(aes(x = theta, ymin = 0, ymax = value)) + geom_ribbon(fill = &quot;grey67&quot;) + scale_x_continuous(breaks = seq(from = 0, to = 1, by = .2)) + labs(x = expression(theta), y = &quot;probability density&quot;) + theme(panel.grid = element_blank()) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) Bayesian inference is intuitively rational: With a strongly informed prior that uses a lot of previous data to put high credibility over a narrow range of parameter values, it takes a lot of novel contrary data to budge beliefs away from the prior. But with a weakly informed prior that spreads credibility over a wide range of parameter values, it takes relatively little data to shift the peak of the posterior distribution toward the data (although the posterior will be relatively wide and uncertain). (p. 114) 5.4 5.4. Why Bayesian inference can be difficult Determining the posterior distribution directly from Bayes, rule involves computing the evidence (a.k.a. marginal likelihood) in Equations 5.8 and 5.9. In the usual case of continuous parameters, the integral in Equation 5.9 can be impossible to solve analytically. Historically, the difficulty of the integration was addressed by restricting models to relatively simple likelihood functions with corresponding formulas for prior distributions, called conjugate priors, that “played nice” with the likelihood function to produce a tractable integral. (p. 115, emphasis in the original) However, the simple model + conjugate prior approach has its limitations. As we’ll see, we often want to fit complex models without shackling ourselves with conjugate priors—which can be quite a pain to work with. Happily, another kind of approximation involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution. In recent decades, many such algorithms have been developed, generally referred to as Markov chain Monte Carlo (MCMC) methods. What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models without computing the integral in Bayes’ rule. It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. (pp. 115–116, emphasis in the original) Reference Kruschke, J. K. (2015). Doing Bayesian data analysis, Second Edition: A tutorial with R, JAGS, and Stan. Burlington, MA: Academic Press/Elsevier. Session info sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS High Sierra 10.13.6 ## ## Matrix products: default ## BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.6/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] forcats_0.4.0 stringr_1.4.0 dplyr_0.8.3 purrr_0.3.2 ## [5] readr_1.3.1 tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## [9] tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] pacman_0.5.1 utf8_1.1.4 ## [3] ggstance_0.3.2 tidyselect_0.2.5 ## [5] htmlwidgets_1.5 grid_3.6.0 ## [7] munsell_0.5.0 codetools_0.2-16 ## [9] DT_0.9 miniUI_0.1.1.1 ## [11] withr_2.1.2 Brobdingnag_1.2-6 ## [13] colorspace_1.4-1 knitr_1.23 ## [15] rstudioapi_0.10 stats4_3.6.0 ## [17] bayesplot_1.7.0 labeling_0.3 ## [19] rstan_2.19.2 bridgesampling_0.7-2 ## [21] coda_0.19-3 vctrs_0.2.0 ## [23] generics_0.0.2 xfun_0.10 ## [25] R6_2.4.0 markdown_1.1 ## [27] HDInterval_0.2.0 assertthat_0.2.1 ## [29] promises_1.1.0 scales_1.0.0 ## [31] gtable_0.3.0 processx_3.4.1 ## [33] rlang_0.4.0 zeallot_0.1.0 ## [35] lazyeval_0.2.2 broom_0.5.2 ## [37] inline_0.3.15 yaml_2.2.0 ## [39] reshape2_1.4.3 abind_1.4-5 ## [41] modelr_0.1.4 threejs_0.3.1 ## [43] crosstalk_1.0.0 backports_1.1.5 ## [45] httpuv_1.5.2 rsconnect_0.8.15 ## [47] tools_3.6.0 bookdown_0.12 ## [49] ellipsis_0.3.0 ggridges_0.5.1 ## [51] Rcpp_1.0.2 plyr_1.8.4 ## [53] base64enc_0.1-3 ps_1.3.0 ## [55] prettyunits_1.0.2 openssl_1.4.1 ## [57] zoo_1.8-6 haven_2.1.0 ## [59] magrittr_1.5 colourpicker_1.0 ## [61] packrat_0.5.0 matrixStats_0.55.0 ## [63] hms_0.4.2 shinyjs_1.0 ## [65] mime_0.7 evaluate_0.14 ## [67] arrayhelpers_1.0-20160527 xtable_1.8-4 ## [69] shinystan_2.5.0 readxl_1.3.1 ## [71] gridExtra_2.3 rstantools_2.0.0 ## [73] compiler_3.6.0 crayon_1.3.4 ## [75] StanHeaders_2.19.0 htmltools_0.4.0 ## [77] later_1.0.0 lubridate_1.7.4 ## [79] MASS_7.3-51.4 Matrix_1.2-17 ## [81] cli_1.1.0 parallel_3.6.0 ## [83] igraph_1.2.4.1 pkgconfig_2.0.3 ## [85] xml2_1.2.0 svUnit_0.7-12 ## [87] dygraphs_1.1.1.6 rvest_0.3.4 ## [89] callr_3.3.2 digest_0.6.21 ## [91] rmarkdown_1.13 cellranger_1.1.0 ## [93] curl_4.2 shiny_1.3.2 ## [95] gtools_3.8.1 lifecycle_0.1.0 ## [97] nlme_3.1-139 jsonlite_1.6 ## [99] askpass_1.1 fansi_0.4.0 ## [101] pillar_1.4.2 lattice_0.20-38 ## [103] loo_2.1.0 httr_1.4.0 ## [105] pkgbuild_1.0.5 glue_1.3.1 ## [107] xts_0.11-2 shinythemes_1.1.2 ## [109] stringi_1.4.3 "]
]
